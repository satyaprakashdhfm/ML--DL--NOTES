{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values:\n",
    "Detection\n",
    " * How much missing % (by heatmap)\n",
    " * Like massive missing not co-occurrence plot\n",
    "Numeric Data:\n",
    " * Deletion\n",
    " * Imputation\n",
    "   * Arbitrary value\n",
    "   * Mean\n",
    "   * Median\n",
    "   * Random sample\n",
    " * Using algorithms like KNN, KNN can handle missing values\n",
    " * Making it as dependent variable\n",
    "Categorical Data :\n",
    " * Deletion\n",
    " * Most frequent\n",
    " * Using deep learning library - Datawig (Imputation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Noise and Outliers\n",
    "\n",
    "\n",
    "- **Noise**: Unwanted or wrong data.\n",
    "- **Outliers**: Data points that are out of range or significantly different from other observations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| **Category**      | **Robust Methods**                                     | **Sensitive Methods**                               |\n",
    "|-------------------|--------------------------------------------------------|-----------------------------------------------------|\n",
    "| **Naive Bayes**   | ✔                                                      |                                                     |\n",
    "| **SVM**           | ✔                                                      |                                                     |\n",
    "| **Decision Tree** | ✔                                                      |                                                     |\n",
    "| **Ensemble Methods** | Random Forest (RF), XGBoost (XGB), Gradient Boosting (GB) |                                                     |\n",
    "| **KNN**           | ✔                                                      |                                                     |\n",
    "| **Linear Regression** |                                                        | ✔                                                   |\n",
    "| **Logistic Regression** |                                                        | ✔                                                   |\n",
    "| **k-Means Clustering** |                                                        | ✔                                                   |\n",
    "| **Hierarchical Clustering** |                                                        | ✔                                                   |\n",
    "| **PCA**           |                                                        | ✔                                                   |\n",
    "| **Neural Networks** |                                                        | ✔                                                   |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Detection Methods for Outliers**\n",
    "- **Z-score**\n",
    "- **Standard Deviation & Interquartile Range (IQR)**\n",
    "- **Boxplot**\n",
    "- **DBSCAN Clustering**: Identifies points that don’t belong to any cluster.\n",
    "- **Isolation Forest**: Effective for high-dimensional data.\n",
    "- **Robust Random Cut Forest**\n",
    "\n",
    "\n",
    "\n",
    "### **Treating Outliers**\n",
    "- **Transformation**: Use log or square root transformations to reduce the impact of outliers.\n",
    "- **Feature Transformation Techniques**:\n",
    "  - **Standardization**\n",
    "  - **Robust Scaler**: Based on interquartile range.\n",
    "- **Winsorization**: Replace extreme values with the nearest acceptable percentile range.\n",
    "- **Trying Different Models**: E.g., Random Forest, XGBoost.\n",
    "- **Imputation**: Replace outliers with mean or median values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Encoding Categorical Variables\n",
    "\n",
    "1. **Algorithms:**\n",
    "   - **Naive Bayes:** Works well with categorical data.\n",
    "   - **Random Forest:** Handles both categorical and numerical data effectively.\n",
    "\n",
    "2. **Nominal Encoding:**\n",
    "   - **One-Hot Encoding (OHE):** Creates binary columns for each category.\n",
    "   - **OHE-Many Categorical:** Applies OHE to datasets with many categories.\n",
    "   - **Top-10 Names:** Encodes only the top 10 most frequent categories.\n",
    "\n",
    "3. **Ordinal Encoding:**\n",
    "   - **Label Encoding:** Assigns numerical values based on category order (e.g., Low = 0, Medium = 1).\n",
    "   - **Count (or) Frequency Encoding:** Encodes categories based on their frequency.\n",
    "   - **Cardinality Encoding:** Encodes categories based on the number of unique values.\n",
    "   - **Target Guided Encoding:** Encodes categories based on their relationship with the target variable.\n",
    "\n",
    "4. **Additional Methods:**\n",
    "   - **Frequency Encoding:** Encodes categories by their occurrence frequency.\n",
    "   - **Binary Encoding:** Converts categories to binary format, reducing dimensionality.\n",
    "   - **Hashing Encoding:** Uses a hash function to map categories to a fixed number of buckets.\n",
    "   - **Mean Encoding:** Encodes categories with the mean of the target variable.\n",
    "   - **Leave-One-Out Encoding:** Uses mean target value excluding the current observation.\n",
    "   - **Count Encoding:** Encodes categories by their count in the dataset.\n",
    "   - **Polynomial Encoding:** Creates polynomial features for interactions between categories.\n",
    "   - **Embeddings:** Uses learned vector representations for categories, often from neural networks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "#### 1. **Data Cleaning**\n",
    "   - **Dropping:** \n",
    "     - **Description:** Removing unnecessary or irrelevant data to improve the quality of the dataset.\n",
    "\n",
    "#### 2. **Feature Scaling**\n",
    "   - **Normalization and Standardization:**\n",
    "     - **Normalization:**\n",
    "       - **Min-Max Scaling:**\n",
    "         - **Description:** Scales data to a specific range (e.g., 0 to 1 or -1 to 1).\n",
    "         - **Usage:** Suitable for algorithms that are sensitive to the scale of data, such as neural networks.\n",
    "     - **Standardization:**\n",
    "       - **Z-score Normalization:**\n",
    "         - **Description:** Scales data to have a mean of 0 and a standard deviation of 1.\n",
    "         - **Usage:** Required for distance-based algorithms and when data should follow a normal distribution.\n",
    "\n",
    "#### 3. **Feature Transformation**\n",
    "   - **Transforming Features:**\n",
    "     - **Description:** Converting features from one domain to another to meet the assumptions of the model.\n",
    "     - **Assumption for Linear and Logistic Regression:** Data should follow a normal (Gaussian) distribution.\n",
    "   \n",
    "   - **Detection of Normal Distribution:**\n",
    "     - **Histogram:** Visualizes the distribution of data.\n",
    "     - **Density Plot:** Shows the data's probability density function.\n",
    "     - **Boxplot:** Highlights data spread and outliers.\n",
    "     - **Q-Q Plot:** Compares data distribution to a normal distribution.\n",
    "     - **Shapiro-Wilk Test:** Statistical test for normality.\n",
    "     - **Many Other Methods:** Various other statistical or graphical techniques.\n",
    "\n",
    "   - **Feature Transformation Methods:**\n",
    "     - **Gaussian Transformation:** Converts data to follow a Gaussian distribution.\n",
    "     - **Logarithmic Transformation:** Useful for skewed distributions.\n",
    "     - **Reciprocal Transformation:** Inverts values to reduce skew.\n",
    "     - **Square Root Transformation:** Reduces the impact of outliers.\n",
    "     - **Exponential Transformation:** Increases the impact of large values.\n",
    "     - **Box-Cox Transformation:** Finds the optimal power transformation.\n",
    "     - **Robust Scalar:** Handles outliers more effectively than standard scaling methods.\n",
    "\n",
    "#### 4. **Algorithms and Their Requirements**\n",
    "   - **Z-score Normalization:**\n",
    "     - **Requires:** Distance-based algorithms.\n",
    "     - **Scales and Distributions:** Assumes data is normally distributed (Gaussian).\n",
    "   - **Min-Max Scaling:**\n",
    "     - **Requires:** Tree-based algorithms.\n",
    "     - **Scales and Distributions:** No specific distribution requirement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "#### 1. **Feature Selection**\n",
    "   - **Lasso Regression:**\n",
    "     - **Description:** A regularization technique that selects features by shrinking some coefficients to zero, thus performing automatic feature selection.\n",
    "   - **Dropping Constant Features:**\n",
    "     - **Description:** Removing features that have the same value across all observations, as they do not contribute to the model.\n",
    "   - **Correlation Checking:**\n",
    "     - **Description:** Identifying and removing features that are highly correlated with each other to avoid multicollinearity.\n",
    "   - **Chi-Square Test:**\n",
    "     - **Description:** A statistical test used for selecting features in categorical data.\n",
    "   - **ANOVA (Analysis of Variance):**\n",
    "     - **Description:** A statistical test used for selecting features in numerical data by assessing the variance between groups.\n",
    "   - **Recursive Feature Elimination (RFE):**\n",
    "     - **Description:** A greedy algorithm that recursively removes features with the least impact on model performance.\n",
    "   - **Feature Importance:**\n",
    "     - **Description:** Using built-in functions in libraries like scikit-learn to determine the importance of features based on model performance.\n",
    "   - **Variance Inflation Factor (VIF):**\n",
    "     - **Description:** Measures the degree of multicollinearity among features, identifying features that are highly correlated.\n",
    "\n",
    "#### 2. **Dimensionality Reduction**\n",
    "   - **Purpose:**\n",
    "     - **Description:** Reducing the number of features while preserving the essential information in the data.\n",
    "   - **Techniques:**\n",
    "     - **PCA (Principal Component Analysis):**\n",
    "       - **Description:** Transforms features into a set of orthogonal components that capture the maximum variance.\n",
    "     - **LDA (Linear Discriminant Analysis):**\n",
    "       - **Description:** A supervised method that focuses on maximizing class separation.\n",
    "     - **t-SNE (t-Distributed Stochastic Neighbor Embedding):**\n",
    "       - **Description:** A technique for visualizing high-dimensional data by mapping it to a lower-dimensional space.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "#### 1. **Covariance, Co-linearity, and Multi-colinearity**\n",
    "   - **Covariance:**\n",
    "     - **Description:** Measures the relationship between two variables; indicates how much two variables change together.\n",
    "   - **Co-linearity:**\n",
    "     - **Description:** Occurs when two or more independent variables are highly correlated.\n",
    "   - **Multi-colinearity:**\n",
    "     - **Description:** Occurs when more than two independent variables are highly correlated.\n",
    "\n",
    "   - **Detection:**\n",
    "     - **Pearson Correlation:**\n",
    "       - **Description:** Uses a heatmap to visualize the correlation between variables.\n",
    "     - **Variable Inflation Factor (VIF):**\n",
    "       - **Description:** Measures the degree of multicollinearity in regression models; higher VIF indicates more multicollinearity.\n",
    "\n",
    "   - **Fixing:**\n",
    "     - **Dropping One of the Features:**\n",
    "       - **Description:** Removes one of the correlated features to reduce multicollinearity.\n",
    "     - **PCA (Principal Component Analysis):**\n",
    "       - **Description:** Creates new, uncorrelated features (principal components) to address multicollinearity.\n",
    "\n",
    "#### 2. **Dimensionality Reduction**\n",
    "   - **Purpose:**\n",
    "     - **Description:** Reduces the number of features while preserving essential information.\n",
    "     - **Benefits:**\n",
    "       - **Reduces Computational Cost:** Less data to process.\n",
    "       - **Improves Model Performance:** Can enhance accuracy by reducing noise.\n",
    "       - **Reduces Overfitting:** Simplifies the model to avoid fitting noise.\n",
    "\n",
    "   - **Methods:**\n",
    "     - **PCA (Principal Component Analysis):**\n",
    "       - **Description:** Creates new, uncorrelated features called principal components.\n",
    "       - **Visualization:** Represents clusters of co-variant features in a 2D graph.\n",
    "     - **Kernel PCA:**\n",
    "       - **Description:** A non-linear extension of PCA, useful for capturing non-linear relationships.\n",
    "     - **Linear Discriminant Analysis (LDA):**\n",
    "       - **Description:** A supervised method that focuses on maximizing class separation.\n",
    "     - **Autoencoders:**\n",
    "       - **Description:** Neural networks that learn to compress and reconstruct data, effectively reducing dimensionality.\n",
    "     - **Feature Selection:**\n",
    "       - **Description:** Choosing a subset of features based on their relevance to the target variable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Datasets\n",
    "\n",
    "#### 1. **Characteristics of Imbalanced Datasets**\n",
    "   - **Description:** Datasets where one class significantly outnumbers the other.\n",
    "   - **Algorithm Suitability:**\n",
    "     - **Tree-Based Algorithms:** Generally handle imbalance better.\n",
    "     - **Decision Trees and Random Forests:** Often not well-suited for imbalanced datasets.\n",
    "     - **Boosting Algorithms:** AdaBoost, XGBoost, and Gradient Boosting Machines can handle imbalance effectively.\n",
    "\n",
    "#### 2. **Fixing Imbalance**\n",
    "   - **Over-sampling:**\n",
    "     - **Description:** Increasing the number of instances in the minority class.\n",
    "     - **Methods:**\n",
    "       - **Duplicating Examples:** Replicating existing minority class samples.\n",
    "       - **Generating Synthetic Examples:** Creating new samples using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "   - **Under-sampling:**\n",
    "     - **Description:** Reducing the number of instances in the majority class.\n",
    "     - **Methods:**\n",
    "       - **Random Removal:** Removing a subset of majority class samples to balance the dataset.\n",
    "\n",
    "   - **Data Augmentation:**\n",
    "     - **Description:** Creating new features or transformations to better represent the minority class and improve understanding.\n",
    "\n",
    "   - **Anomaly Detection:**\n",
    "     - **Description:** Treating the minority class as anomalies and applying anomaly detection techniques to identify and classify them.\n",
    "\n",
    "   - **Cost-sensitive Learning:**\n",
    "     - **Description:** Assigning different costs to misclassification errors based on class, to penalize errors on the minority class more heavily.\n",
    "\n",
    "#### 3. **Evaluation Metrics**\n",
    "   - **Precision, Recall, and F1-score:**\n",
    "     - **Description:** Metrics more appropriate for evaluating performance on imbalanced datasets than accuracy.\n",
    "       - **Precision:** The ratio of true positives to the sum of true positives and false positives.\n",
    "       - **Recall:** The ratio of true positives to the sum of true positives and false negatives.\n",
    "       - **F1-score:** The harmonic mean of precision and recall.\n",
    "   - **AUC (Area Under the ROC Curve):**\n",
    "     - **Description:** Measures the overall performance of a model, providing a summary of the model's ability to discriminate between classes.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Testing, and Validation Splits in Datasets\n",
    "\n",
    "#### 1. **Purpose of Splits**\n",
    "   - **Training Set:**\n",
    "     - **Description:** Used to train the model by adjusting its parameters based on the data.\n",
    "   - **Validation Set:**\n",
    "     - **Description:** Used to tune hyperparameters and make decisions about model architecture.\n",
    "   - **Testing Set:**\n",
    "     - **Description:** Used to assess the final performance of the model after training and validation.\n",
    "\n",
    "#### 2. **Common Splitting Methods**\n",
    "\n",
    "   - **Simple Split:**\n",
    "     - **Description:** Fixed proportions are used to divide the data into training, validation, and test sets.\n",
    "     - **Process:**\n",
    "       - **Shuffle Data:** Randomly shuffle the data.\n",
    "       - **Split Data:** Allocate data into training (e.g., 70%), validation (e.g., 15%), and testing sets (e.g., 15%).\n",
    "\n",
    "   - **k-Fold Cross-Validation:**\n",
    "     - **Description:** Data is divided into \\( k \\) folds. The model is trained \\( k \\) times, using \\( k-1 \\) folds for training and 1 fold for validation.\n",
    "     - **Process:**\n",
    "       - **Divide Data:** Split data into \\( k \\) folds.\n",
    "       - **Train and Validate:** Train on \\( k-1 \\) folds and validate on the remaining fold.\n",
    "       - **Average Results:** Compute average performance metrics across all \\( k \\) folds.\n",
    "\n",
    "   - **Stratified Split:**\n",
    "     - **Description:** Maintains the proportion of classes in each split, useful for imbalanced datasets.\n",
    "     - **Process:**\n",
    "       - **Divide Data:** Split data while preserving class proportions in training, validation, and test sets.\n",
    "\n",
    "   - **Time Series Split:**\n",
    "     - **Description:** Splits data based on time, ensuring the order of data is respected.\n",
    "     - **Process:**\n",
    "       - **Chronological Split:** Use past data for training and future data for validation/testing.\n",
    "\n",
    "   - **Leave-One-Out Cross-Validation (LOOCV):**\n",
    "     - **Description:** Each sample is used as a validation set once, while the remaining samples are used for training.\n",
    "     - **Process:**\n",
    "       - **Train and Validate:** Train on all but one sample and validate on the single sample.\n",
    "\n",
    "   - **Holdout Method:**\n",
    "     - **Description:** Randomly splits data into training and test sets, optionally performing further validation on the training set.\n",
    "     - **Process:**\n",
    "       - **Shuffle and Split:** Split data into training and test sets, with optional additional validation.\n",
    "\n",
    "### Summary\n",
    "- **Training Set:** For fitting the model's parameters.\n",
    "- **Validation Set:** For tuning model parameters and selecting the best model.\n",
    "- **Testing Set:** For evaluating final model performance.\n",
    "\n",
    "**Splitting Methods:**\n",
    "- **Simple Split:** Fixed proportions for training, validation, and test sets.\n",
    "- **k-Fold Cross-Validation:** Multiple training and validation rounds with \\( k \\) folds.\n",
    "- **Stratified Split:** Preserves class proportions in each split.\n",
    "- **Time Series Split:** Ensures data is split chronologically.\n",
    "- **LOOCV:** Uses each sample as a validation set once.\n",
    "- **Holdout Method:** Simple train-test split with optional further validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Overfitting and Underfitting\n",
    "\n",
    "#### 1. **Overfitting**\n",
    "   - **Description:** Occurs when a model learns the noise in the training data rather than the underlying pattern, leading to poor performance on new data.\n",
    "\n",
    "   - **Techniques to Handle Overfitting:**\n",
    "     - **Cross-Validation:**\n",
    "       - **Description:** Evaluates the model's performance on different subsets of the data to ensure it generalizes well.\n",
    "     - **Regularization:**\n",
    "       - **Description:** Adds a penalty to the loss function to constrain the model's complexity, e.g., L1 (Lasso) and L2 (Ridge) regularization.\n",
    "     - **Early Stopping:**\n",
    "       - **Description:** Stops training when the model’s performance on a validation set starts to degrade.\n",
    "     - **Pruning (for Tree-Based Models):**\n",
    "       - **Description:** Removes branches from decision trees to prevent them from becoming too complex.\n",
    "     - **Simplifying the Model:**\n",
    "       - **Description:** Reduces the complexity of the model by using fewer features or parameters.\n",
    "     - **Data Augmentation:**\n",
    "       - **Description:** Increases the diversity of the training data by applying transformations like rotations, flips, or scaling, particularly in image data.\n",
    "\n",
    "#### 2. **Underfitting**\n",
    "   - **Description:** Occurs when a model is too simple to capture the underlying pattern in the data, leading to poor performance on both training and new data.\n",
    "\n",
    "   - **Techniques to Handle Underfitting:**\n",
    "     - **Get More Data:**\n",
    "       - **Description:** Provides the model with more examples to learn from, which can help in capturing the underlying pattern better.\n",
    "     - **Increase Model Complexity:**\n",
    "       - **Description:** Uses a more complex model or algorithm that can capture more intricate patterns in the data.\n",
    "     - **Increase Number of Parameters:**\n",
    "       - **Description:** Adds more parameters to the model to allow it to learn more complex relationships.\n",
    "     - **Train for More Iterations:**\n",
    "       - **Description:** Extends the training process to give the model more opportunity to learn from the data.\n",
    "     - **Use Ensembling Techniques:**\n",
    "       - **Description:** Combines predictions from multiple models to improve performance and capture more complex patterns.\n",
    "     - **Data Augmentation:**\n",
    "       - **Description:** Enhances the training set by applying transformations to the data to help the model learn better.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Model Evaluation Metrics\n",
    "#### 1. **Regression Metrics**\n",
    "   - **R-squared (R²):**\n",
    "     - **Description:** Measures the proportion of variance in the dependent variable explained by the model.\n",
    "     - **Purpose:** Assesses model fit and explanatory power.\n",
    "   - **Adjusted R²:**\n",
    "     - **Description:** Adjusts R² for the number of predictors in the model.\n",
    "     - **Purpose:** Penalizes the inclusion of irrelevant features, providing a more accurate measure of model performance.\n",
    "   - **Mean Squared Error (MSE):**\n",
    "     - **Description:** Measures the average squared difference between predicted and actual values.\n",
    "     - **Purpose:** Evaluates model accuracy; lower values indicate better performance.\n",
    "   - **Mean Absolute Error (MAE):**\n",
    "     - **Description:** Measures the average absolute difference between predicted and actual values.\n",
    "     - **Purpose:** Provides a direct measure of prediction error; less sensitive to outliers than MSE.\n",
    "   - **Root Mean Squared Error (RMSE):**\n",
    "     - **Description:** The square root of the MSE, giving a measure in the same units as the target variable.\n",
    "     - **Purpose:** Provides a more interpretable measure of prediction error compared to MSE.\n",
    "#### 2. **Classification Metrics**\n",
    "   - **Confusion Matrix:**\n",
    "     - **Description:** A table summarizing the performance of a classification model by showing true positives, true negatives, false positives, and false negatives.\n",
    "   - **Precision:**\n",
    "     - **Description:** The proportion of positive predictions that are actually positive.\n",
    "     - **Formula:** ```Precision = True Positives / (True Positives + False Positives)```\n",
    "   - **Recall:**\n",
    "     - **Description:** The proportion of actual positive instances that are correctly predicted as positive.\n",
    "     - **Formula:** ```Recall = True Positives / (True Positives + False Negatives)```\n",
    "   - **Accuracy:**\n",
    "     - **Description:** The overall proportion of correct predictions (both positive and negative).\n",
    "     - **Formula:** ```Accuracy = (True Positives + True Negatives) / Total Predictions```\n",
    "   - **F-score:**\n",
    "     - **Description:** The harmonic mean of precision and recall.\n",
    "     - **Formula:** ```F_β = (1 + β²) * (Precision * Recall) / (β² * Precision + Recall)```\n",
    "   - **Type I Error (False Positive Rate, FPR):**\n",
    "     - **Description:** The rate of incorrectly predicting the positive class when the true class is negative.\n",
    "     - **Formula:** ```FPR = False Positives / (True Negatives + False Positives)```\n",
    "   - **Type II Error (False Negative Rate, FNR):**\n",
    "     - **Description:** The rate of incorrectly predicting the negative class when the true class is positive.\n",
    "     - **Formula:** ```FNR = False Negatives / (True Positives + False Negatives)```\n",
    "   - **ROC Curve:**\n",
    "     - **Description:** A plot showing the trade-off between the true positive rate (sensitivity) and the false positive rate.\n",
    "   - **AUC-ROC:**\n",
    "     - **Description:** The area under the ROC curve, representing the model's overall ability to distinguish between classes.\n",
    "     - **Purpose:** Higher AUC indicates better model performance.\n",
    "#### 3. **Clustering Metrics**\n",
    "   - **Silhouette Coefficient:**\n",
    "     - **Description:** Measures how similar a data point is to its own cluster compared to other clusters.\n",
    "     - **Purpose:** Helps assess the quality of clustering.\n",
    "   - **Davies-Bouldin Index:**\n",
    "     - **Description:** Measures the average similarity between each cluster and its nearest cluster.\n",
    "     - **Purpose:** Lower values indicate better clustering.\n",
    "   - **Calinski-Harabasz Index:**\n",
    "     - **Description:** Measures the ratio of between-cluster variance to within-cluster variance.\n",
    "     - **Purpose:** Higher values indicate more distinct clusters.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Terminology from Algorithms\n",
    "\n",
    "#### 1. **Bias and Variance**\n",
    "   - **Bias:**\n",
    "     - **Definition:** Error due to model assumptions; systematic error.\n",
    "   - **Variance:**\n",
    "     - **Definition:** Error due to variability in model predictions across different training sets.\n",
    "\n",
    "#### 2. **Cost Function**\n",
    "   - **Definition:** Measures the model's error; guides model training by minimizing error.\n",
    "\n",
    "#### 3. **Overfitting and Underfitting**\n",
    "   - **Overfitting:**\n",
    "     - **Definition:** Model performs well on training data but poorly on unseen data.\n",
    "   - **Underfitting:**\n",
    "     - **Definition:** Model performs poorly on both training and test data.\n",
    "\n",
    "#### 4. **Precision and Recall**\n",
    "   - **Precision:**\n",
    "     - **Definition:** Proportion of true positives among all positive predictions.\n",
    "   - **Recall:**\n",
    "     - **Definition:** Proportion of true positives among all actual positives.\n",
    "\n",
    "#### 5. **Regularization**\n",
    "   - **Purpose:** Prevents overfitting by penalizing complex models.\n",
    "   - **L1 Regularization (Lasso):** Encourages sparsity by setting some coefficients to zero.\n",
    "   - **L2 Regularization (Ridge):** Reduces the magnitude of coefficients.\n",
    "\n",
    "#### 6. **Interquartile Range (IQR)**\n",
    "   - **Definition:** Measure of dispersion; distance between the first and third quartiles.\n",
    "   - **Purpose:** Excludes outliers to show the spread of the central 50% of the data.\n",
    "\n",
    "### Box Plots, Percentiles, Entropy, and Information Gain\n",
    "\n",
    "#### 1. **Box Plots**\n",
    "   - **IQR:** Distance between the first (Q1) and third quartiles (Q3).\n",
    "   - **Outliers:** Data points outside 1.5 times the IQR from the quartiles.\n",
    "   - **Whiskers:** Extend from the box to the max/min within 1.5 times the IQR.\n",
    "\n",
    "#### 2. **Percentiles**\n",
    "   - **Definition:** Percentage of values below a certain threshold.\n",
    "   - **Calculation:** \\( \\frac{\\text{Number below the value}}{\\text{Total number of values}} \\times 100 \\)\n",
    "\n",
    "#### 3. **Entropy**\n",
    "   - **Definition:** Measure of impurity or uncertainty in a dataset.\n",
    "   - **Higher Entropy:** More uncertainty.\n",
    "   - **Lower Entropy:** More homogeneity.\n",
    "\n",
    "#### 4. **Information Gain**\n",
    "   - **Definition:** Reduction in entropy after splitting the dataset based on a feature.\n",
    "   - **Higher Information Gain:** More informative feature for classification.\n",
    "\n",
    "\n",
    "   ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter tuning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance techniques help identify which features have the most impact on your model's predictions. This is crucial for understanding your model, improving it, and gaining business insights. Here are several methods to determine feature importance:\n",
    "\n",
    "1. Tree-based Methods:\n",
    "\n",
    "   a) Random Forest Feature Importance:\n",
    "   ```python\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "   \n",
    "   rf = RandomForestClassifier()\n",
    "   rf.fit(X_train, y_train)\n",
    "   importances = rf.feature_importances_\n",
    "   ```\n",
    "\n",
    "   b) XGBoost Feature Importance:\n",
    "   ```python\n",
    "   import xgboost as xgb\n",
    "   \n",
    "   xgb_model = xgb.XGBClassifier()\n",
    "   xgb_model.fit(X_train, y_train)\n",
    "   importances = xgb_model.feature_importances_\n",
    "   ```\n",
    "\n",
    "2. Permutation Importance:\n",
    "   Works with any model, not just tree-based ones.\n",
    "   ```python\n",
    "   from sklearn.inspection import permutation_importance\n",
    "   \n",
    "   result = permutation_importance(model, X_test, y_test, n_repeats=10)\n",
    "   importances = result.importances_mean\n",
    "   ```\n",
    "\n",
    "3. SHAP (SHapley Additive exPlanations):\n",
    "   Provides both global and local feature importance.\n",
    "   ```python\n",
    "   import shap\n",
    "   \n",
    "   explainer = shap.TreeExplainer(model)\n",
    "   shap_values = explainer.shap_values(X)\n",
    "   shap.summary_plot(shap_values, X)\n",
    "   ```\n",
    "\n",
    "4. Recursive Feature Elimination (RFE):\n",
    "   ```python\n",
    "   from sklearn.feature_selection import RFE\n",
    "   \n",
    "   rfe = RFE(estimator=model, n_features_to_select=10)\n",
    "   rfe.fit(X, y)\n",
    "   ```\n",
    "\n",
    "5. Correlation Analysis:\n",
    "   For linear relationships between features and target.\n",
    "   ```python\n",
    "   correlation = X.corrwith(y)\n",
    "   ```\n",
    "\n",
    "6. Mutual Information:\n",
    "   Captures non-linear relationships.\n",
    "   ```python\n",
    "   from sklearn.feature_selection import mutual_info_classif\n",
    "   \n",
    "   mi_scores = mutual_info_classif(X, y)\n",
    "   ```\n",
    "\n",
    "\n",
    "After calculating importances, you can visualize them:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "For your churn prediction task, I recommend starting with methods like Random Forest or XGBoost feature importance, as they work well with the models you've been using. SHAP values can provide more detailed insights if needed.\n",
    "\n",
    "Would you like me to help you implement one of these methods for your specific churn prediction model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
