{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### deep learning automate the processs of ridge and lasso in neurone with dead neurons kind of things and handling categorical values at one neuron.....most of the ml hyperparameter tuning is done by neurones like one step one step by one neurones\n",
    "\n",
    "Yes, that's a great way to conceptualize how deep learning works! In deep learning models, especially in neural networks, different neurons and layers can learn various types of representations and regularization techniques simultaneously.\n",
    "\n",
    "In a neural network, each neuron specializes in detecting specific features of the input data by applying its own weights to the inputs received from the previous layer. During training, only the weights of neurons that significantly contribute to the prediction error are updated, allowing for selective learning. Neurons use activation functions to introduce non-linearity, determining whether to activate based on the weighted sum of inputs and bias. Early layers capture basic patterns, while deeper layers combine these patterns to recognize more complex structures, enhancing the model's generalization capabilities. This architecture enables deep learning models to effectively process and understand complex data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the explanation formatted for Jupyter Notebook's Markdown:\n",
    "\n",
    "---\n",
    "\n",
    "## Why Deep Learning Over Polynomial Regression?\n",
    "\n",
    "### 1. **Curse of Dimensionality**\n",
    "- In **polynomial regression**, the model's complexity grows exponentially with the degree of the polynomial. For high-dimensional data with many features, this quickly becomes computationally expensive and prone to overfitting.\n",
    "  \n",
    "  For example, in a dataset with \\( n \\) features, a second-degree polynomial regression would involve terms like:\n",
    "  \n",
    "  $$ y = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^2 + w_5x_1x_2 + \\dots $$\n",
    "  \n",
    "  As the number of features increases, the number of polynomial terms explodes, leading to a model that is hard to train and generalize.\n",
    "  \n",
    "### **Deep learning models** avoid this issue by using layers of neurons that learn to combine features **incrementally** through non-linear transformations, instead of directly expanding feature spaces with high-degree polynomials.\n",
    "\n",
    "### 2. **Feature Engineering**\n",
    "- Polynomial regression often requires **manual specification of the polynomial degree** and feature interactions. This means you need to decide whether to include squares, cross-terms, or higher-degree interactions, which can be impractical for high-dimensional or unstructured data (like images or text).\n",
    "\n",
    "- **Deep learning** automatically learns the most useful features and interactions by stacking layers, allowing neural networks to **self-organize** without requiring manual feature engineering.\n",
    "\n",
    "### 3. **Flexibility and Hierarchical Learning**\n",
    "- Polynomial regression fits specific types of non-linearity — **global** non-linearity based on predefined terms like \\( x_1^2 \\) or \\( x_1 x_2 \\). It cannot adapt well to **local variations** or hierarchical patterns in data.\n",
    "\n",
    "- **Deep learning models** can handle hierarchical structures, learning simple patterns (like edges) in lower layers and more abstract concepts (like shapes or objects) in higher layers.\n",
    "\n",
    "### 4. **Scalability**\n",
    "- Polynomial regression does not scale well with increasing data complexity. To model highly non-linear relationships, higher-degree polynomials are needed, making the model complex and inefficient.\n",
    "  \n",
    "- **Deep neural networks**, with their layered structure, scale more naturally to high-dimensional data. They can approximate very complex functions without needing high-degree polynomials.\n",
    "\n",
    "### 5. **Generalization**\n",
    "- **Polynomial models** tend to overfit when using high-degree terms, especially with limited data, meaning they may perform well on the training set but poorly on unseen data.\n",
    "\n",
    "- **Deep learning models**, with techniques like regularization, dropout, and tuning of hyperparameters, can generalize better to unseen data. Non-linear activations allow them to represent complex decision boundaries without explicit, high-degree polynomial terms.\n",
    "\n",
    "### 6. **Efficiency in Complex Domains**\n",
    "- **Deep learning** excels in domains like **computer vision, natural language processing, and reinforcement learning**, where learning abstract representations from raw data is essential. Neural networks can learn features directly from raw pixel values, while polynomial regression requires manual feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "This can be copy-pasted directly into a Jupyter Notebook Markdown cell for neatly formatted explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, the **dimensions** (number of neurons and layers) of a model are decided differently compared to linear or polynomial regression. Here's how it works:\n",
    "\n",
    "### 1. **Input Layer (First Layer)**\n",
    "The input layer's dimensions depend on the number of **features** in your data, just like in linear or polynomial regression. This is straightforward:\n",
    "- If your dataset has \\( n \\) features (e.g., 10 features for a tabular dataset), the input layer will have \\( n \\) neurons, one for each feature.\n",
    "- For image data, the input layer might receive pixel values, so the dimensionality depends on the size and channels of the image (e.g., a 28x28 grayscale image would have 784 neurons).\n",
    "\n",
    "### 2. **Hidden Layers**\n",
    "The dimensions of the **hidden layers** (number of neurons and layers) in deep learning are more flexible and **problem-dependent**. Unlike linear or polynomial regression, there's no fixed rule based on the number of features. Instead, you experiment or apply heuristics based on the complexity of the task. Here are some considerations:\n",
    "\n",
    "#### a. **Number of Hidden Layers**\n",
    "- **Shallow networks** (few hidden layers) work for simpler tasks or smaller datasets, but they might not capture complex patterns.\n",
    "- **Deep networks** (many hidden layers) are used to capture hierarchical and complex representations. For example, in computer vision tasks like image classification, deeper networks (like 50+ layers in ResNet) learn more abstract features in each subsequent layer.\n",
    "\n",
    "#### b. **Number of Neurons per Hidden Layer**\n",
    "- **Fewer neurons** in hidden layers can result in underfitting, where the model is too simple to capture the complexity of the data.\n",
    "- **More neurons** allow the network to capture more intricate patterns but increase the risk of overfitting. A good starting point is to have the number of neurons in hidden layers be **comparable to** or **slightly larger than** the number of features, then adjust based on validation performance.\n",
    "\n",
    "  A common heuristic is to reduce the number of neurons as you go deeper into the network.\n",
    "\n",
    "  Example:\n",
    "  - If the input layer has 100 features, the first hidden layer might have 128 neurons, the second layer 64 neurons, the third layer 32 neurons, and so on. However, the exact choice depends on your data and task.\n",
    "\n",
    "### 3. **Output Layer**\n",
    "The number of neurons in the output layer depends on the **type of problem** you're solving:\n",
    "- For **regression**, the output layer typically has one neuron that outputs a continuous value.\n",
    "- For **binary classification**, the output layer usually has one neuron with a sigmoid activation function to output a probability (between 0 and 1).\n",
    "- For **multi-class classification**, the output layer has as many neurons as there are classes, using softmax activation to output class probabilities.\n",
    "\n",
    "### 4. **Why Not Fixed Like Linear/Polynomial Regression?**\n",
    "In **linear regression**, the model is completely determined by the number of features. Each feature contributes linearly to the final prediction, so adding polynomial terms (like \\( x^2 \\) or \\( x_1x_2 \\)) increases the model's flexibility to fit the data.\n",
    "\n",
    "In **deep learning**, the flexibility comes from **non-linear transformations** and the ability to learn hierarchical representations. The dimensions of the network are not fixed by the input features but instead reflect the complexity of the data and the relationships it needs to capture.\n",
    "\n",
    "### 5. **Architecture Search**\n",
    "In practice, finding the best architecture (number of layers, number of neurons) for deep learning involves experimentation, hyperparameter tuning, and sometimes automated techniques like **neural architecture search (NAS)**. Common methods include:\n",
    "- **Grid search/random search** over the number of layers and neurons.\n",
    "- **Automated architecture search algorithms** like NAS, where the model automatically searches for the best architecture.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Input layer** dimensions depend on the number of input features (similar to regression models).\n",
    "- **Hidden layers** are designed based on the complexity of the task, with flexibility to add more layers and neurons as needed.\n",
    "- **Output layer** dimensions depend on the type of problem (regression, binary, or multi-class classification).\n",
    "  \n",
    "While linear and polynomial regression models have a more rigid relationship with feature count, deep learning models adapt to the complexity of the task by learning representations across multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What is Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML: automation of statistics and Often requires manual feature engineering.ml works good with small data \n",
    "DL: automation of ml like Automatically learns features from raw data.dl works good with big data\n",
    "\n",
    "DL architecture does require more data to leverage its full potential and avoid overfitting. ML models, being simpler, can often extract meaningful patterns from smaller datasets.\n",
    "Parameter count:\n",
    "DL models often have millions or billions of parameters, while traditional ML models might have hundreds or thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what deep learning is, we first need to understand the relationship deep learning has with machine learning, neural networks, and artificial intelligence.\n",
    "\n",
    "\n",
    "The best way to think of this relationship is to visualize them as concentric circles:\n",
    "\n",
    "![DL](https://www.safaribooksonline.com/library/view/deep-learning/9781491924570/assets/dpln_0101.png)\n",
    "\n",
    "\n",
    "At the outer most ring you have artificial intelligence (using computers to reason). One layer inside of that is machine learning. With artificial neural networks and deep learning at the center.\n",
    "\n",
    "\n",
    "Broadly speaking, deep learning is a more approachable name for an artificial neural network. The “deep” in deep learning refers to the depth of the network. An artificial neural network can be very shallow.\n",
    "\n",
    "\n",
    "Neural networks are inspired by the structure of the cerebral cortex. At the basic level is the perceptron, the mathematical representation of a biological neuron. Like in the cerebral cortex, there can be several layers of interconnected perceptrons.\n",
    "\n",
    "\n",
    "The first layer is the input layer. Each node in this layer takes an input, and then passes its output as the input to each node in the next layer. There are generally no connections between nodes in the same layer and the last layer produces the outputs.\n",
    "\n",
    "\n",
    "We call the middle part the hidden layer. These neurons have no connection to the outside (e.g. input or output) and are only activated by nodes in the previous layer.\n",
    "\n",
    "![layer](http://neuralnetworksanddeeplearning.com/images/tikz11.png)\n",
    "\n",
    "\n",
    "Think of deep learning as the technique for learning in neural networks that utilizes multiple layers of abstraction to solve pattern recognition problems. In the 1980s, most neural networks were a single layer due to the cost of computation and availability of data.\n",
    "\n",
    "\n",
    "Machine learning is considered a branch or approach of Artificial intelligence, whereas deep learning is a specialized type of machine learning.\n",
    "\n",
    "\n",
    "Machine learning involves computer intelligence that doesn’t know the answers up front. Instead, the program will run against training data, verify the success of its attempts, and modify its approach accordingly. Machine learning typical requires a sophisticated education, spanning software engineering and computer science to statistical methods and linear algebra.\n",
    "\n",
    "\n",
    "There are two broad classes of machine learning methods:\n",
    "\n",
    "   * Supervised learning\n",
    "   * Unsupervised learning\n",
    "\n",
    "\n",
    "In supervised learning, a machine learning algorithm uses a labeled dataset to infer the desired outcome. This takes a lot of data and time, since the data needs to be labeled by hand. Supervised learning is great for classification and regression problems.\n",
    "\n",
    "For example, let’s say that we were running a company and want to determine the effect of bonuses on employee retention. If we had historical data – i.e. employee bonus amount and tenure – we could use supervised machine learning.\n",
    "\n",
    "With unsupervised learning, there aren’t any predefined or corresponding answers. The goal is to figure out the hidden patterns in the data. It’s usually used for clustering and associative tasks, like grouping customers by behavior. Amazon’s “customers who also bought…” recommendations are a type of associative task.\n",
    "\n",
    "While supervised learning can be useful, we often have to resort to unsupervised learning. Deep learning has proven to be an effective unsupervised learning technique.\n",
    "\n",
    "## Why is Deep Learning Important?\n",
    "\n",
    "![importance](https://image.slidesharecdn.com/andrew-ng-extract-oct2015-nonotes-151124104249-lva1-app6891/95/andrew-ng-chief-scientist-at-baidu-30-638.jpg?cb=1448361887)\n",
    "\n",
    "Computers have long had techniques for recognizing features inside of images. The results weren’t always great. Computer vision has been a main beneficiary of deep learning. Computer vision using deep learning now rivals humans on many image recognition tasks.\n",
    "\n",
    "\n",
    "Facebook has had great success with identifying faces in photographs by using deep learning. It’s not just a marginal improvement, but a game changer: “Asked whether two unfamiliar photos of faces show the same person, a human being will get it right 97.53 percent of the time. New software developed by researchers at Facebook can score 97.25 percent on the same challenge, regardless of variations in lighting or whether the person in the picture is directly facing the camera.”\n",
    "\n",
    "\n",
    "Speech recognition is a another area that’s felt deep learning’s impact. Spoken languages are so vast and ambiguous. Baidu – one of the leading search engines of China – has developed a voice recognition system that is faster and more accurate than humans at producing text on a mobile phone. In both English and Mandarin.\n",
    "\n",
    "\n",
    "What is particularly fascinating, is that generalizing the two languages didn’t require much additional design effort: “Historically, people viewed Chinese and English as two vastly different languages, and so there was a need to design very different features,” Andrew Ng says, chief scientist at Baidu. “The learning algorithms are now so general that you can just learn.”\n",
    "\n",
    "Google is now using deep learning to manage the energy at the company’s data centers. They’ve cut their energy needs for cooling by 40%. That translates to about a 15% improvement in power usage efficiency for the company and hundreds of millions of dollars in savings.\n",
    "\n",
    "## Deep Learning Microservices\n",
    "\n",
    "Here’s a quick overview of some deep learning use cases and microservices.\n",
    "\n",
    "Illustration Tagger. An implementation of Illustration2Vec, this microservice can tag an image with the safe, questionable, or explicit rating, the copyright, and general category tag to understand what’s in the image. DeepFilter is a style transfer service for applying artistic filters to images.\n",
    "\n",
    "The age classifier uses face detection to determine the age of a person in a photo. The Places 365 Classifier uses a pre-trained CNN and based on Places: An Image Database for Deep Scene Understanding B. Zhou, et al., 2016 to identify particular locations in images, such as a courtyard, drugstore, hotel room, glacier, mountain, etc. Lastly, there is InceptionNet, a direct implementation of Google’s InceptionNet using TensorFlow. It takes an image (such as a car), and returns the top 5 classes the model predicts are relevant to the image.\n",
    "\n",
    "## Open Source Deep Learning Frameworks\n",
    "\n",
    "Deep learnings is made accessible by a number of open source projects. Some of the most popular technologies include, but are not limited to, Deeplearning4j (DL4j), Theano, Torch, TensorFlow, and Caffe. The deciding factors on which one to use are the tech stack they target, and if they are low-level, academic, or application focused. Here’s an overview of each:\n",
    "\n",
    "DL4J:\n",
    "\n",
    "   * JVM-based\n",
    "   * Distrubted\n",
    "   * Integrates with Hadoop and Spark\n",
    "   \n",
    "   \n",
    "Theano:\n",
    "\n",
    "   * Very popular in Academia\n",
    "   * Fairly low level\n",
    "   * Interfaced with via Python and Numpy\n",
    "\n",
    "\n",
    "Torch:\n",
    "\n",
    "   * Lua based\n",
    "   * In house versions used by Facebook and Twitter\n",
    "   * Contains pretrained models\n",
    "\n",
    "\n",
    "TensorFlow:\n",
    "\n",
    "   * Google written successor to Theano\n",
    "   * Interfaced with via Python and Numpy\n",
    "   * Highly parallel\n",
    "   * Can be somewhat slow for certain problem sets\n",
    "\n",
    "\n",
    "\n",
    "Caffe:\n",
    "\n",
    "   * Not general purpose. Focuses on machine-vision problems\n",
    "   * Implemented in C++ and is very fast\n",
    "   * Not easily extensible\n",
    "   * Has a Python interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McCulloch and Pitts Neuron\n",
    "\n",
    "In 1943, McCulloch and Pitts introduced a mathematical model of a neuron. It consisted of three components:\n",
    "\n",
    "1. A set of **weights** $w_i$ corresponding to synapses (inputs)\n",
    "2. An **adder** for summing input signals; analogous to cell membrane that collects charge\n",
    "3. An **activation function** for determining when the neuron fires, based on accumulated input\n",
    "\n",
    "The neuron model is shown schematically below. On the left are input nodes $\\{x_i\\}$, usually expressed as a vector. The strength with which the inputs are able to deliver the signal along the synapse is determined by their corresponding weights $\\{w_i\\}$. The adder then sums the inputs from all the synapses:\n",
    "\n",
    "$$h = \\sum_i w_i x_i$$\n",
    "\n",
    "The parameter $\\theta$ determines whether or not the neuron fires given a weighted input of $h$. If it fires, it returns a value $y=1$, otherwise $y=0$. For example, a simple **activation function** is using $\\theta$ as a simple fixed threshold:\n",
    "\n",
    "$$y = g(h) = \\left\\{ \\begin{array}{l}\n",
    "1, \\text{if } h \\gt \\theta \\\\\n",
    "0, \\text{if } h \\le \\theta\n",
    "\\end{array} \\right.$$\n",
    "\n",
    "this activation function may take any of several forms, such as a logistic function.\n",
    "\n",
    "![neuron](http://d.pr/i/9AMK+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron is not interesting, nor useful, from a learning perspective. It cannot learn; it simply receives inputs and either fires or not. Only when neurons are joined as a **network** can they perform useful work.\n",
    "\n",
    "Learning takes place by changing the weights of the connections in a neural network, and by changing the parameters of the activation functions of neurons.\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A collection of McCullough and Pitts neurons, along with a set of input nodes connected to the inputs via weighted edges, is a perceptron, the simplest neural network.\n",
    "\n",
    "Each neuron is independent of the others in the perceptron, in the sense that its behavior and performance depends only on its own weights and threshold values, and not of those for the other neurons. Though they share inputs, they operate independently.\n",
    "\n",
    "The number of inputs and outputs are determined by the data. Weights are stored as a `N x K` matrix, with N observations and K neurons, with $w_{ij}$ specifying the weight on the *i*th observation on the *j*th neuron.\n",
    "\n",
    "![perceptron](http://d.pr/i/4IWA+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the perceptron for statistical learning, we compare the outputs $y_j$ from each neuron to the obervation target $t_j$, and adjust the input weights when they do not correspond (*e.g.* if a neuron fires when it should not have).\n",
    "\n",
    "$$t_j - y_j$$\n",
    "\n",
    "We use this difference to update the weight $w_{ij}$, based on the input and a desired **learning rate**. This results in an update rule:\n",
    "\n",
    "$$w_{ij} \\leftarrow w_{ij} + \\eta (t_j - y_j) x_i$$\n",
    "\n",
    "After an incremental improvement, the perceptron is shown the training data again, resulting in another update. This is repeated until the performance no longer improves. Having a learning rate less than one results in a more stable learning rate, though this stability is traded off against having to expose the network to the data multiple times. Typical learning rates are in the 0.1-0.4 range.\n",
    "\n",
    "An additional input node is typically added to the perceptron model, which is a constant value (usually -1, 0, or 1) that acts analogously to an intercept in a regression model. This establishes a baseline input for the case when all inputs are zero.\n",
    "\n",
    "![bias](http://d.pr/i/105b5+)\n",
    "\n",
    "## Learning with Perceptrons\n",
    "\n",
    "1. Initialize weights $w_{ij}$ to small, random numbers.\n",
    "2. For each t in T iterations\n",
    "    * compute activation for each neuron *j* connected to each input vector *i*\n",
    "    $$y_j = g\\left( h=\\sum_i w_{ij} x_i \\right) = \\left\\{ \\begin{array}{l}\n",
    "1, \\text{if } h \\gt 0 \\\\\n",
    "0, \\text{if } h \\le 0\n",
    "\\end{array} \\right.$$\n",
    "    * update weights\n",
    "    $$w_{ij} \\leftarrow w_{ij} + \\eta (t_j - y_j) x_i$$\n",
    "\n",
    "\n",
    "This algorithm is $\\mathcal{O}(Tmn)$\n",
    "\n",
    "### Example: Logical functions\n",
    "\n",
    "Let's see how the perceptron learns by training it on a couple of of logical functions, AND and OR. For two variables `x1` and `x2`, the AND function returns 1 if both are true, or zero otherwise; the OR function returns 1 if either variable is true, or both. These functions can be expressed as simple lookup tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from scipy import optimize\n",
    "from ipywidgets import *\n",
    "from IPython.display import SVG\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  0\n",
       "2   1   0  0\n",
       "3   1   1  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AND = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,0,0,1)})\n",
    "AND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize weights to small, random values (can be positive and negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.random.randn(3)*1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a simple activation function for calculating $g(h)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = lambda inputs, weights: np.where(np.dot(inputs, weights)>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a training function that iterates the learning algorithm, returning the adapted weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(inputs, targets, weights, eta, n_iterations):\n",
    "\n",
    "    # Add the inputs that match the bias node\n",
    "    inputs = np.c_[inputs, -np.ones((len(inputs), 1))]\n",
    "\n",
    "    for n in range(n_iterations):\n",
    "\n",
    "        activations = g(inputs, weights);\n",
    "        weights -= eta*np.dot(np.transpose(inputs), activations - targets)\n",
    "        \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it first on the AND function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = AND[['x1','x2']]\n",
    "target = AND['y']\n",
    "\n",
    "w = train(inputs, target, w, 0.25, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(np.c_[inputs, -np.ones((len(inputs), 1))], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it has learned the function perfectly. Now for OR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  1\n",
       "2   1   0  1\n",
       "3   1   1  1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,1)})\n",
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.random.randn(3)*1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = OR[['x1','x2']]\n",
    "target = OR['y']\n",
    "\n",
    "w = train(inputs, target, w, 0.25, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(np.c_[inputs, -np.ones((len(inputs), 1))], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also 100% correct.\n",
    "\n",
    "### Exercise: XOR\n",
    "\n",
    "Now try running the model on the XOR function, where a one is returned for either `x1` or `x2` being true, but *not* both. What happens here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the problem graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f197120bdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AND.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter')\n",
    "plt.plot(np.linspace(0,1.4), 1.5 - 1*np.linspace(0,1.4), 'k--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1971109ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter')\n",
    "plt.plot(np.linspace(-.4,1), .5 - 1*np.linspace(-.4,1), 'k--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1971019860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "XOR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,0)})\n",
    "\n",
    "XOR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron tries to find a separating hyperplane for the two response classes. Namely, a set of weights that satisfies:\n",
    "\n",
    "$$\\mathbf{x_1}\\mathbf{w}^T=0$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\\mathbf{x_2}\\mathbf{w}^T=0$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{x}_1\\mathbf{w}^T &= \\mathbf{x}_2\\mathbf{w}^T \\\\\n",
    "\\Rightarrow (\\mathbf{x}_1 - \\mathbf{x}_2) \\mathbf{w}^T &= 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "This means that either the norms of $\\mathbf{x}_1 - \\mathbf{x}_2$ or $\\mathbf{w}$ are zero, or the cosine of the angle between them is equal to zero, due to the identity:\n",
    "\n",
    "$$\\mathbf{a}\\mathbf{b} = \\|a\\| \\|b\\| \\cos \\theta$$\n",
    "\n",
    "Since there is no reason for the norms to be zero in general, we need the two vectors to be at right angles to one another. So, we need a weight vector that is perpendicular to the decision boundary.\n",
    "\n",
    "Clearly, for the XOR function, the output classes are not linearly separable. So, the algorithm does not converge on an answer, but simply cycles through two incorrect solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron\n",
    "\n",
    "The solution to fitting more complex (*i.e.* non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply **add more weights**.\n",
    "\n",
    "There are two ways to add complexity:\n",
    "\n",
    "1. Add backward connections, so that output neurons feed back to input nodes, resulting in a **recurrent network**\n",
    "2. Add neurons between the input nodes and the outputs, creating an additional (\"hidden\") layer to the network, resulting in a **multi-layer perceptron**\n",
    "\n",
    "The latter approach is more common in applications of neural networks.\n",
    "\n",
    "![multilayer](http://d.pr/i/14BS1+)\n",
    "\n",
    "How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction.\n",
    "\n",
    "Updating a multi-layer perceptron (MLP) is a matter of: \n",
    "\n",
    "1. moving forward through the network, calculating outputs given inputs and current weight estimates\n",
    "2. moving backward updating weights according to the resulting error from forward propagation. \n",
    "\n",
    "In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is a method for efficiently computing the gradient of the cost function of a neural network with respect to its parameters.  These partial derivatives can then be used to update the network's parameters using, e.g., gradient descent.  This may be the most common method for training neural networks.  Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors. \n",
    "\n",
    "\n",
    "![bp](https://theclevermachine.files.wordpress.com/2014/09/neural-net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: The chain rule\n",
    "\n",
    "The chain rule is a way to compute the derivative of a function whose variables are themselves functions of other variables.  If $C$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another scalar variable $w$, then the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial z}\\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "For scalar-valued functions of more than one variable, the chain rule essentially becomes additive.  In other words, if $C$ is a scalar-valued function of $N$ variables $z_1, \\ldots, z_N$, each of which is a function of some variable $w$, the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\sum_{i = 1}^N \\frac{\\partial C}{\\partial z_i}\\frac{\\partial z_i}{\\partial w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "In the following derivation, we'll use the following notation:\n",
    "\n",
    "$L$ - Number of layers in the network.\n",
    "\n",
    "$N^n$ - Dimensionality of layer $n \\in \\{0, \\ldots, L\\}$.  $N^0$ is the dimensionality of the input; $N^L$ is the dimensionality of the output.\n",
    "\n",
    "$W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ - Weight matrix for layer $m \\in \\{1, \\ldots, L\\}$.  $W^m_{ij}$ is the weight between the $i^{th}$ unit in layer $m$ and the $j^{th}$ unit in layer $m - 1$.\n",
    "\n",
    "$b^m \\in \\mathbb{R}^{N^m}$ - Bias vector for layer $m$.\n",
    "\n",
    "$\\sigma^m$ - Nonlinear activation function of the units in layer $m$, applied elementwise.\n",
    "\n",
    "$z^m \\in \\mathbb{R}^{N^m}$ - Linear mix of the inputs to layer $m$, computed by $z^m = W^m a^{m - 1} + b^m$.\n",
    "\n",
    "$a^m \\in \\mathbb{R}^{N^m}$ - Activation of units in layer $m$, computed by $a^m = \\sigma^m(h^m) = \\sigma^m(W^m a^{m - 1} + b^m)$.  $a^L$ is the output of the network.  We define the special case $a^0$ as the input of the network.\n",
    "\n",
    "$y \\in \\mathbb{R}^{N^L}$ - Target output of the network.\n",
    "\n",
    "$C$ - Cost/error function of the network, which is a function of $a^L$ (the network output) and $y$ (treated as a constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in general\n",
    "\n",
    "In order to train the network using a gradient descent algorithm, we need to know the gradient of each of the parameters with respect to the cost/error function $C$; that is, we need to know $\\frac{\\partial C}{\\partial W^m}$ and $\\frac{\\partial C}{\\partial b^m}$.  It will be sufficient to derive an expression for these gradients in terms of the following terms, which we can compute based on the neural network's architecture:\n",
    "\n",
    "- $\\frac{\\partial C}{\\partial a^L}$: The derivative of the cost function with respect to its argument, the output of the network\n",
    "- $\\frac{\\partial a^m}{\\partial z^m}$: The derivative of the nonlinearity used in layer $m$ with respect to its argument\n",
    "\n",
    "To compute the gradient of our cost/error function $C$ to $W^m_{ij}$ (a single entry in the weight matrix of the layer $m$), we can first note that $C$ is a function of $a^L$, which is itself a function of the linear mix variables $z^m_k$, which are themselves functions of the weight matrices $W^m$ and biases $b^m$.  With this in mind, we can use the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial W^m_{ij}}$$\n",
    "\n",
    "Note that by definition \n",
    "$$\n",
    "z^m_k = \\sum_{l = 1}^{N^m} W^m_{kl} a_l^{m - 1} + b^m_k\n",
    "$$\n",
    "It follows that $\\frac{\\partial z^m_k}{\\partial W^m_{ij}}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any elements in $W^m$ except for those in the $k$<sup>th</sup> row, and we are only considering the entry $W^m_{ij}$.  When $i = k$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^m_i}{\\partial W^m_{ij}} &= \\frac{\\partial}{\\partial W^m_{ij}}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
    "&= a^{m - 1}_j\\\\\n",
    "\\rightarrow \\frac{\\partial z^m_k}{\\partial W^m_{ij}} &= \\begin{cases}\n",
    "0 & k \\ne i\\\\\n",
    "a^{m - 1}_j & k = i\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The fact that $\\frac{\\partial C}{\\partial a^m_k}$ is $0$ unless $k = i$ causes the summation above to collapse, giving\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\frac{\\partial C}{\\partial z^m_i} a^{m - 1}_j$$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m} a^{m - 1 \\top}$$\n",
    "\n",
    "Similarly for the bias variables $b^m$, we have\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m_i} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial b^m_i}$$\n",
    "\n",
    "As above, it follows that $\\frac{\\partial z^m_k}{\\partial b^m_i}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any element in $b^m$ except $b^m_k$.  When $i = k$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^m_i}{\\partial b^m_i} &= \\frac{\\partial}{\\partial b^m_i}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
    "&= 1\\\\\n",
    "\\rightarrow \\frac{\\partial z^m_i}{\\partial b^m_i} &= \\begin{cases}\n",
    "0 & k \\ne i\\\\\n",
    "1 & k = i\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The summation also collapses to give\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m_i} = \\frac{\\partial C}{\\partial z^m_i}$$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
    "\n",
    "Now, we must compute $\\frac{\\partial C}{\\partial z^m_k}$.  For the final layer ($m = L$), this term is straightforward to compute using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^L_k} = \\frac{\\partial C}{\\partial a^L_k} \\frac{\\partial a^L_k}{\\partial z^L_k}\n",
    "$$\n",
    "\n",
    "or, in vector form\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}\n",
    "$$\n",
    "\n",
    "The first term $\\frac{\\partial C}{\\partial a^L}$ is just the derivative of the cost function with respect to its argument, whose form depends on the cost function chosen.  Similarly, $\\frac{\\partial a^m}{\\partial z^m}$ (for any layer $m$ includling $L$) is the derivative of the layer's nonlinearity with respect to its argument and will depend on the choice of nonlinearity.  For other layers, we again invoke the chain rule:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^m_k} &= \\frac{\\partial C}{\\partial a^m_k} \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial z^{m + 1}_l}{\\partial a^m_k}\\right)\\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial}{\\partial a^m_k} \\left(\\sum_{h = 1}^{N^m} W^{m + 1}_{lh} a_h^m + b_l^{m + 1}\\right)\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l} W^{m + 1}_{lk}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}W^{m + 1\\top}_{kl} \\frac{\\partial C}{\\partial z^{m + 1}_l}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where the last simplification was made because by convention $\\frac{\\partial C}{\\partial z^{m + 1}_l}$ is a column vector, allowing us to write the following vector form:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z^m} = \\left(W^{m + 1\\top} \\frac{\\partial C}{\\partial z^{m + 1}}\\right) \\circ \\frac{\\partial a^m}{\\partial z^m}$$\n",
    "\n",
    "Note that we now have the ingredients to efficiently compute the gradient of the cost function with respect to the network's parameters:  First, we compute $\\frac{\\partial C}{\\partial z^L_k}$ based on the choice of cost function and nonlinearity.  Then, we recursively can compute $\\frac{\\partial C}{\\partial z^m}$ layer-by-layer based on the term $\\frac{\\partial C}{\\partial z^{m + 1}}$ computed from the previous layer and the nonlinearity of the layer (this is called the \"backward pass\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "\n",
    "As discussed above, the exact form of the updates depends on both the chosen cost function and each layer's chosen nonlinearity.  The following two table lists the some common choices for nonlinearities and the required partial derivative for deriving the gradient for each layer:\n",
    "\n",
    "| Nonlinearity | $a^m = \\sigma^m(z^m)$ | $\\frac{\\partial a^m}{\\partial z^m}$ | Notes |\n",
    "|--------------|---|---|---|\n",
    "| Sigmoid      | $\\frac{1}{1 + e^{z^m}}$ | $\\sigma^m(z^m)(1 - \\sigma^m(z^m)) = a^m(1 - a^m)$ | \"Squashes\" any input to the range $[0, 1]$ |\n",
    "| Tanh         | $\\frac{e^{z^m} - e^{-z^m}}{e^{z^m} + e^{-z^m}}$ | $1 - (\\sigma^m(z^m))^2 = 1 - (a^m)^2$ | Equivalent, up to scaling, to the sigmoid function |\n",
    "| ReLU         | $\\max(0, z^m)$ | $0, z^m < 0;\\; 1, z^m \\ge 0$ | Commonly used in neural networks with many layers|\n",
    "\n",
    "Similarly, the following table collects some common cost functions and the partial derivative needed to compute the gradient for the final layer:\n",
    "\n",
    "| Cost Function | $C$                                  | $\\frac{\\partial C}{\\partial a^L}$ | Notes |\n",
    "|---------------|--------------------------------------|-----------------------------------|---|\n",
    "| Squared Error | $\\frac{1}{2}(y - a^L)^\\top(y - a^L)$ | $y - a^L$                         | Commonly used when the output is not constrained to a specific range |\n",
    "| Cross-Entropy | $(y - 1)\\log(1 - a^L) - y\\log(a^L)$  | $\\frac{a^L - y}{a^L(1 - a^L)}$    | Commonly used for binary classification tasks; can yield faster convergence |\n",
    "\n",
    "In practice, backpropagation proceeds in the following manner for each training sample:\n",
    "\n",
    "1. Forward pass: Given the network input $a^0$, compute $a^m$ recursively by\n",
    " $$a^1 = \\sigma^1(W^1 a^0 + b^1), \\ldots, a^L = \\sigma^L(W^L a^{L - 1} + b^L)$$\n",
    "1. Backward pass: Compute \n",
    "$$\\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}$$\n",
    "for the final layer based on the tables above, then recursively compute\n",
    "$$\\frac{\\partial C}{\\partial z^m} = \\left(W^{m + 1\\top} \\frac{\\partial C}{\\partial z^{m + 1}}\\right) \\circ \\frac{\\partial a^m}{\\partial z^m}$$\n",
    "for all other layers.  Plug these values into \n",
    "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m_i} a^{m - 1 \\top}$$\n",
    "and\n",
    "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
    "to obtain the updates.\n",
    "\n",
    "### Example: Sigmoid network with cross-entropy loss using gradient descent\n",
    "\n",
    "A common network architecture is one with fully connected layers where each layer's nonlinearity is the sigmoid function $a^m = \\frac{1}{1 + e^{z^m}}$ and the cost function is the cross-entropy loss $(y - 1)\\log(1 - a^L) - y\\log(a^L)$.  To compute the updates for gradient descent, we first compute (based on the tables above)\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^L} &= \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}\\\\\n",
    "&= \\left(\\frac{a^L - y}{a^L(1 - a^L)}\\right)a^L(1 - a^L)\\\\\n",
    "&= a^L - y\n",
    "\\end{align*}\n",
    "From here, we can compute\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^{L - 1}} &= \\left(W^{L\\top} \\frac{\\partial C}{\\partial z^L} \\right) \\circ \\frac{\\partial a^{L - 1}}{\\partial z^{L - 1}}\\\\\n",
    "&= W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1})\\\\\n",
    "\\frac{\\partial C}{\\partial z^{L - 2}} &= \\left(W^{L - 1\\top} \\frac{\\partial C}{\\partial z^{L - 1}} \\right) \\circ \\frac{\\partial a^{L - 2}}{\\partial z^{L - 2}}\\\\\n",
    "&= W^{L - 1\\top} \\left(W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1})\\right) \\circ a^{L - 2}(1 - a^{L - 2})\n",
    "\\end{align*}\n",
    "and so on, until we have computed $\\frac{\\partial C}{\\partial z^m}$ for $m \\in \\{1, \\ldots, L\\}$.  This allows us to compute  $\\frac{\\partial C}{\\partial W^m_{ij}}$ and $\\frac{\\partial C}{\\partial b^m_i}$, e.g.\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial W^L} &= \\frac{\\partial C}{\\partial z^L} a^{L - 1 \\top}\\\\\n",
    "&= (a^L - y)a^{L - 1\\top}\\\\\n",
    "\\frac{\\partial C}{\\partial W^{L - 1}} &= \\frac{\\partial C}{\\partial z^{L - 1}} a^{L - 2 \\top}\\\\\n",
    "&= W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1}) a^{L - 2\\top}\n",
    "\\end{align*}\n",
    "and so on.  Standard gradient descent then updates each parameter as follows:\n",
    "$$W^m = W^m - \\lambda \\frac{\\partial C}{\\partial W^m}$$\n",
    "$$b^m = b^m - \\lambda \\frac{\\partial C}{\\partial b^m}$$\n",
    "where $\\lambda$ is the learning rate.  This process is repeated until some stopping criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Python example\n",
    "\n",
    "Due to the recursive nature of the backpropagation algorithm, it lends itself well to software implementations.  The following code implements a multi-layer perceptron which is trained using backpropagation with user-supplied nonlinearities, layer sizes, and cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure python 3 forward compatibility\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.W = np.random.randn(n_output, n_input)\n",
    "        self.b = np.random.randn(n_output, 1)\n",
    "    def output(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        return sigmoid(self.W.dot(X) + self.b)\n",
    "\n",
    "class SigmoidNetwork:\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        '''\n",
    "        :parameters:\n",
    "            - layer_sizes : list of int\n",
    "                List of layer sizes of length L+1 (including the input dimensionality)\n",
    "        '''\n",
    "        self.layers = []\n",
    "        for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            self.layers.append(SigmoidLayer(n_input, n_output))\n",
    "    \n",
    "    def train(self, X, y, learning_rate=0.2):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(1, -1)\n",
    "        \n",
    "        # Forward pass - compute a^n for n in {0, ... L}\n",
    "        layer_outputs = [X]\n",
    "        for layer in self.layers:\n",
    "            layer_outputs.append(layer.output(layer_outputs[-1]))\n",
    "        \n",
    "        # Backward pass - compute \\partial C/\\partial z^m for m in {L, ..., 1}\n",
    "        cost_partials = [layer_outputs[-1] - y]\n",
    "        for layer, layer_output in zip(reversed(self.layers), reversed(layer_outputs[:-1])):\n",
    "            cost_partials.append(layer.W.T.dot(cost_partials[-1])*layer_output*(1 - layer_output))\n",
    "        cost_partials.reverse()\n",
    "        \n",
    "        # Compute weight gradient step\n",
    "        W_updates = []\n",
    "        for cost_partial, layer_output in zip(cost_partials[1:], layer_outputs[:-1]):\n",
    "            W_updates.append(cost_partial.dot(layer_output.T)/X.shape[1])\n",
    "        # and biases\n",
    "        b_updates = [cost_partial.mean(axis=1).reshape(-1, 1) for cost_partial in cost_partials[1:]]\n",
    "        \n",
    "        for W_update, b_update, layer in zip(W_updates, b_updates, self.layers):\n",
    "            layer.W -= W_update*learning_rate\n",
    "            layer.b -= b_update*learning_rate\n",
    "\n",
    "    def output(self, X):\n",
    "        a = np.array(X)\n",
    "        if a.ndim == 1:\n",
    "            a = a.reshape(-1, 1)\n",
    "        for layer in self.layers:\n",
    "            a = layer.output(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\tOutput\tQuantized\n",
      "[0, 0]\t0.5125\t[1]\n",
      "[1, 0]\t0.4927\t[0]\n",
      "[0, 1]\t0.9788\t[1]\n",
      "[1, 1]\t0.0161\t[0]\n"
     ]
    }
   ],
   "source": [
    "nn = SigmoidNetwork([2, 2, 1])\n",
    "X = np.array([[0, 1, 0, 1], \n",
    "              [0, 0, 1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "for n in range(int(1e3)):\n",
    "    nn.train(X, y, learning_rate=1.)\n",
    "print(\"Input\\tOutput\\tQuantized\")\n",
    "for i in [[0, 0], [1, 0], [0, 1], [1, 1]]:\n",
    "    print(\"{}\\t{:.4f}\\t{}\".format(i, nn.output(i)[0, 0], 1*(nn.output(i)[0] > .5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1970f1bd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic = lambda h, beta: 1./(1 + np.exp(-beta * h))\n",
    "\n",
    "@interact(beta=(-1, 25))\n",
    "def logistic_plot(beta=5):\n",
    "    hvals = np.linspace(-2, 2)\n",
    "    plt.plot(hvals, logistic(hvals, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has the advantage of having a simple derivative:\n",
    "\n",
    "$$\\frac{dg}{dh} = \\beta g(h)(1 - g(h))$$\n",
    "\n",
    "Alternatively, the hyperbolic tangent function is also sigmoid:\n",
    "\n",
    "$$g(h) = \\tanh(h) = \\frac{\\exp(h) - \\exp(-h)}{\\exp(h) + \\exp(-h)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1970ef98d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperbolic_tangent = lambda h: (np.exp(h) - np.exp(-h)) / (np.exp(h) + np.exp(-h))\n",
    "\n",
    "@interact(theta=(-1, 25))\n",
    "def tanh_plot(theta=5):\n",
    "    hvals = np.linspace(-2, 2)\n",
    "    h = hvals*theta\n",
    "    plt.plot(hvals, hyperbolic_tangent(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "---\n",
    "The simplest algorithm for iterative minimization of differentiable functions is known as just **gradient descent**.\n",
    "Recall that the gradient of a function is defined as the vector of partial derivatives:\n",
    "\n",
    "$$\\nabla f(x) =  [{\\partial{f}{x_1}, \\partial{f}{x_2}, \\ldots, \\partial{f}{x_n}}]$$\n",
    "\n",
    "and that the gradient of a function always points towards the direction of maximal increase at that point.\n",
    "\n",
    "Equivalently, it points *away* from the direction of maximum decrease - thus, if we start at any point, and keep moving in the direction of the negative gradient, we will eventually reach a local minimum.\n",
    "\n",
    "This simple insight leads to the Gradient Descent algorithm. Outlined algorithmically, it looks like this:\n",
    "\n",
    "1. Pick a point $x_0$ as your initial guess.\n",
    "2. Compute the gradient at your current guess:\n",
    "$v_i = \\nabla f(x_i)$\n",
    "3. Move by $\\alpha$ (your step size) in the direction of that gradient:\n",
    "$x_{i+1} = x_i + \\alpha v_i$\n",
    "4. Repeat steps 1-3 until your function is close enough to zero (until $f(x_i) < \\varepsilon$ for some small tolerance $\\varepsilon$)\n",
    "\n",
    "Note that the step size, $\\alpha$, is simply a parameter of the algorithm and has to be fixed in advance. \n",
    "\n",
    "![gd](http://ludovicarnold.altervista.org/wp-content/uploads/2015/01/gradient-trajectory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the hyperbolic tangent function asymptotes at -1 and 1, rather than 0 and 1, which is sometimes beneficial, and its derivative is simple:\n",
    "\n",
    "$$\\frac{d \\tanh(x)}{dx} = 1 - \\tanh^2(x)$$\n",
    "\n",
    "Performing gradient descent will allow us to change the weights in the direction that optimially reduces the error. The next trick will be to employ the **chain rule** to decompose how the error changes as a function of the input weights into the change in error as a function of changes in the inputs to the weights, mutliplied by the changes in input values as a function of changes in the weights. \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial h}\\frac{\\partial h}{\\partial w}$$\n",
    "\n",
    "This will allow us to write a function describing the activations of the output weights as a function of the activations of the hidden layer nodes and the output weights, which will allow us to propagate error backwards through the network.\n",
    "\n",
    "The second term in the chain rule simplifies to:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial h_k}{\\partial w_{jk}} &= \\frac{\\partial \\sum_l w_{lk} a_l}{\\partial w_{jk}}  \\\\\n",
    "&= \\sum_l \\frac{\\partial w_{lk} a_l}{\\partial w_{jk}} \\\\\n",
    "& = a_j\n",
    "\\end{align}$$\n",
    "\n",
    "where $a_j$ is the activation of the jth hidden layer neuron.\n",
    "\n",
    "For the first term in the chain rule above, we decompose it as well:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial h_k} = \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial h_k} = \\frac{\\partial E}{\\partial g(h_k)}\\frac{\\partial g(h_k)}{\\partial h_k}$$\n",
    "\n",
    "The second term of this chain rule is just the derivative of the activation function, which we have chosen to have a conveneint form, while the first term simplifies to:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial g(h_k)} = \\frac{\\partial}{\\partial g(h_k)}\\left[\\frac{1}{2} \\sum_k (t_k - y_k)^2 \\right] = t_k - y_k$$\n",
    "\n",
    "Combining these, and assuming (for illustration) a logistic activiation function, we have the gradient:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "Which ends up getting plugged into the weight update formula that we saw in the single-layer perceptron:\n",
    "\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "Note that here we are *subtracting* the second term, rather than adding, since we are doing gradient descent.\n",
    "\n",
    "We can now outline the MLP learning algorithm:\n",
    "\n",
    "1. Initialize all $w_{jk}$ to small random values\n",
    "2. For each input vector, conduct forward propagation:\n",
    "    * compute activation of each neuron $j$ in hidden layer (here, sigmoid):\n",
    "    $$h_j = \\sum_i x_i v_{ij}$$\n",
    "    $$a_j = g(h_j) = \\frac{1}{1 + \\exp(-\\beta h_j)}$$\n",
    "    * when the output layer is reached, calculate outputs similarly:\n",
    "    $$h_k = \\sum_k a_j w_{jk}$$\n",
    "    $$y_k = g(h_k) = \\frac{1}{1 + \\exp(-\\beta h_k)}$$\n",
    "3. Calculate loss for resulting predictions:\n",
    "    * compute error at output:\n",
    "    $$\\delta_k = (t_k - y_k) y_k (1-y_k)$$\n",
    "4. Conduct backpropagation to get partial derivatives of cost with respect to weights, and use these to update weights:\n",
    "    * compute error of the hidden layers:\n",
    "    $$\\delta_{hj} = \\left[\\sum_k w_{jk} \\delta_k \\right] a_j(1-a_j)$$\n",
    "    * update output layer weights:\n",
    "    $$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j$$\n",
    "    * update hidden layer weights:\n",
    "    $$v_{ij} \\leftarrow v_{ij} - \\eta \\delta_{hj} x_i$$\n",
    "    \n",
    "Return to (2) and iterate until learning completes. Best practice is to shuffle input vectors to avoid training in the same order.\n",
    "\n",
    "Its important to be aware that because gradient descent is a hill-climbing (or descending) algorithm, it is liable to be caught in local minima with respect to starting values. Therefore, it is worthwhile training several networks using a range of starting values for the weights, so that you have a better chance of discovering a globally-competitive solution.\n",
    "\n",
    "One useful performance enhancement for the MLP learning algorithm is the addition of **momentum** to the weight updates. This is just a coefficient on the previous weight update that increases the correlation between the current weight and the weight after the next update. This is particularly useful for complex models, where falling into local mimima is an issue; adding momentum will give some weight to the previous direction, making the resulting weights essentially a weighted average of the two directions. Adding momentum, along with a smaller learning rate, usually results in a more stable algorithm with quicker convergence. When we use momentum, we lose this guarantee, but this is generally seen as a small price to pay for the improvement momentum usually gives.\n",
    "\n",
    "A weight update with momentum looks like this:\n",
    "\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j + \\alpha \\Delta w_{jk}^{t-1}$$\n",
    "\n",
    "where $\\alpha$ is the momentum (regularization) parameter and $\\Delta w_{jk}^{t-1}$ the update from the previous iteration.\n",
    "\n",
    "The multi-layer pereptron is implemented below in the `MLP` class. The implementation uses the scikit-learn interface, so it is uses in the same way as other supervised learning algorithms in that package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
