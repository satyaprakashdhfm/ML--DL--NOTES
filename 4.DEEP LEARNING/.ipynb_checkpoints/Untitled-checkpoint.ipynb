{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib as mlp \n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#x_train,y_train,x_test,y_test,classes = load_data()\n",
    "x_train = np.array([[0.1,0.3]])\n",
    "y_train = np.array([1])\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_ = x_train\n",
    "target_ = y_train\n",
    "num_hidden_layers = 1\n",
    "num_input_dim = 2\n",
    "num_target_dim = 1  # binary classifier\n",
    "print_loss_ = True\n",
    "epochs = 2000 # forward pass + backward pass\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.001 # regularization strength \n",
    "layer_dims = [2,2,1] # neurons in each layer of the network. Input layer followed by 3 hidden layers and one target layer\n",
    "\n",
    "def initialize_weights_n_biases(layer_dims):\n",
    "\t# Initialize the parameters to random values. We need to learn these.\n",
    "\tparameters = {}\n",
    "\tparameters['W'+str(1)] = np.array([[-0.1,0.2],[0.2,-0.3]])\n",
    "\tparameters['b'+str(1)] = np.array([0,0])\n",
    "\tparameters['W'+str(2)] = np.array([[0.3],[-0.1]])\n",
    "\tparameters['b'+str(2)] = np.array([0])\n",
    "\n",
    "\treturn parameters\n",
    "\n",
    "def linear_forward(z, w, b):\n",
    "\tZ = z.dot(w)\n",
    "\tZ = Z + b\n",
    "\tstash = (z,w,b)\n",
    "\t\n",
    "\treturn Z,stash\n",
    "\n",
    "def activation_function(activation, z):\n",
    "\tif activation == \"relu\":\n",
    "\t\ta = np.maximum(0,z)\n",
    "\t\tstash = z\n",
    "\telif activation == \"sigmoid\":\n",
    "\t\ta = 1/(1+np.exp(-1*z))\n",
    "\t\tstash = z\n",
    "\treturn a,stash\n",
    "\t\n",
    "def forward_pass(X, params):\n",
    "\tstashes = []\n",
    "\tA = X\n",
    "\tL = len(params) // 2\n",
    "\tfor l in range(1,L):\n",
    "\t\tZ_prev = A\n",
    "\t\tZ,linear_stash = linear_forward(Z_prev,params['W'+str(l)],params['b'+str(l)])\n",
    "\t\tA,activation_stash = activation_function(\"relu\",Z)\n",
    "\t\tstash = (linear_stash,activation_stash)\n",
    "\t\tstashes.append(stash)\n",
    "\n",
    "\tZ_,linear_stash = linear_forward(A,params['W'+str(L)],params['b'+str(L)])\n",
    "\tA_,activation_stash = activation_function(\"sigmoid\",Z_)\n",
    "\tstash = (linear_stash,activation_stash)\n",
    "\tstashes.append(stash)\n",
    "\treturn A_,stashes\n",
    "\n",
    "def compute_loss(loss_func, A, Y,i):\n",
    "\tm = Y.shape[0]\n",
    "\tif loss_func == \"binary_crossentropy\":\n",
    "\t\tA = A.flatten()\n",
    "\t\tcost = np.sum((-(Y*np.log(A))-((1-Y)*np.log(1-A))),axis=0,keepdims = 1)\n",
    "\t\tcost = np.squeeze(cost)\n",
    "\treturn cost\n",
    "\n",
    "def intermediate_differentiation(dEA, activation, stash):\n",
    "\tif activation == \"relu\":\n",
    "\t\tz = stash\n",
    "\t\tdAZ = 1\n",
    "\t\tdEZ = np.array(dEA,copy=True)\n",
    "\t\tdEZ[z<=0] = 0\n",
    "\telif activation == \"sigmoid\":\n",
    "\t\tz = stash\n",
    "\t\ttmp = 1/(1+np.exp(-z))\n",
    "\t\tdAZ = tmp*(1-tmp) # differentiation of output w.r.t input dA/dZ\n",
    "\t\t# multiply dE/dA * dA/dZ \n",
    "\t\tdEZ = dEA*dAZ\n",
    "\treturn dEZ\n",
    "\n",
    "def error_rate_calc(dEZ, stash):\n",
    "\tz,w,b = stash\n",
    "\tm = z.shape[0]\n",
    "\tdZW = z.T # rate of change of input w.r.t weight, dZ/dW = z\n",
    "\tdEW = np.dot(dZW,dEZ)/m # rate of change of error w.r.t weight, dE/dW = dE/dZ * dZ/dW\n",
    "\tdEb = np.sum(dEZ,axis=0,keepdims=1)/m # rate of change of error w.r.t bias, dE/db = dE/dZ * dZ/db\n",
    "\tdA_prev = np.dot(dEZ,w.T) # error propagated backward\n",
    "\n",
    "\treturn dA_prev,dEW, dEb\n",
    "\n",
    "def linear_backward(dEA, stash, activation):\n",
    "\tlinear_stash, activation_stash = stash\n",
    "\tdEZ = intermediate_differentiation(dEA,activation,activation_stash) # dE/dZ\n",
    "\tdA_prev,dW, db = error_rate_calc(dEZ, linear_stash)\n",
    "\treturn dA_prev,dW,db\n",
    "\n",
    "def backward_pass(A,Y,stashes):\n",
    "\tgrads = {}\n",
    "\tL =len(stashes)\n",
    "\tY = Y.reshape(A.shape)\n",
    "\n",
    "\tdEA = -(np.divide(Y,A)-np.divide(1-Y,1-A)) # differentiation of error w.r.t output dE/dA\n",
    "\tcurrent_stash = stashes[L-1]\n",
    "\tgrads['dA'+str(L-1)],grads['dW'+str(L)],grads['db'+str(L)] = linear_backward(dEA,current_stash,\"sigmoid\")\n",
    "\tfor l in reversed(range(L-1)):\n",
    "\t\tcurrent_stash = stashes[l]\n",
    "\t\tgrads['dA'+str(l)],grads['dW'+str(l+1)],grads['db'+str(l+1)] = linear_backward(grads['dA'+str(l+1)],current_stash,\"relu\")\n",
    "\n",
    "\treturn grads\n",
    "\n",
    "def update_parameters(parameters, grads):\n",
    "\tL = len(parameters) // 2\n",
    "\tfor l in range(L):\n",
    "\t\tparameters['W'+str(l+1)] = parameters['W'+str(l+1)] - epsilon * grads['dW'+str(l+1)] # W = W - lr*(dE/dW)\n",
    "\t\tparameters['b'+str(l+1)] = parameters['b'+str(l+1)] - epsilon * grads['db'+str(l+1)] # b = b - lr*(dE/db)\n",
    "\treturn parameters\n",
    "\n",
    "# Build Sequential model\n",
    "def build_sequential_model(X, Y, layer_dims, print_loss = False):\n",
    "\tparams = initialize_weights_n_biases(layer_dims)\n",
    "\n",
    "\tcosts = []\n",
    "\tfor i in range(0,epochs):\n",
    "\n",
    "\t\t# Forward Propagation\n",
    "\t\tA,caches = forward_pass(X, params)\n",
    "\t\t# Error Calculation \n",
    "\t\tcost = compute_loss(\"binary_crossentropy\",A,Y,i)\n",
    "\t\t# Backward Propagation\n",
    "\t\tgrads = backward_pass(A, Y, caches)\n",
    "\t\t# Update parameters\n",
    "\t\tparams = update_parameters(params,grads)\n",
    "\t\tcosts.append(cost)\n",
    "\t\tif(print_loss == True and i%100 == 0):\n",
    "\t\t\tprint(\"cost at iteration {} is {}\".format(i,cost))\n",
    "\n",
    "\treturn params\n",
    "\n",
    "\n",
    "model = build_sequential_model(input_, target_, layer_dims, print_loss_)\n",
    "\n",
    "def predict(X,Y,model):\n",
    "\tm = X.shape[0]\n",
    "\tres = np.zeros(m)\n",
    "\tprobabs, stashes = forward_pass(X,model)\n",
    "\tfor i in range(0,probabs.shape[0]):\n",
    "\t\tif probabs[i][0] > 0.5:\n",
    "\t\t\tres[i] = 1\n",
    "\t\telse:\n",
    "\t\t\tres[i] = 0\n",
    "\tprint(\"Accuracy: \"+str(np.sum(res == Y)/m))\n",
    "\treturn res\n",
    "\n",
    "train_data_prediction = predict(x_train,y_train,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
