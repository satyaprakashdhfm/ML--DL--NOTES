{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/jDe5BAsT2-Y?si=5Ii-l6HqtSvWV_cH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) Basics\n",
    "\n",
    "## 1. Convolution (Filters)\n",
    "- **Definition**: Convolution in CNNs is a mathematical operation where a small matrix, called a filter, slides over an input image or feature map to produce a new feature map. This filter captures patterns like edges or textures by multiplying the filter values with corresponding input values and summing them.\n",
    "\n",
    "- **Formula**: If the input size is $ \\text{Input Size} $, filter size is $ \\text{Filter Size} $, padding $ \\text{Padding} $, and stride $ \\text{Stride} $, the output size $ \\text{Output Size} $ after convolution is:\n",
    "  $$\n",
    "  \\text{Output Size} = \\frac{\\text{Input Size} - \\text{Filter Size} + 2 \\times \\text{Padding}}{\\text{Stride}} + 1\n",
    "  $$\n",
    "\n",
    "# Why Use Convolutions? (Simplified)\n",
    "\n",
    "Convolutions have two main advantages:\n",
    "\n",
    "## 1. Parameter Sharing\n",
    "- **What It Means**: The same small filter (or pattern detector) is used across the entire image.\n",
    "- **Why It Matters**: If a filter can find something useful, like a vertical line, in one part of the image, it can find the same line in other parts too. This means we don’t need to create a separate filter for every location, which saves memory and makes the model simpler.\n",
    "\n",
    "## 2. Sparsity of Connections\n",
    "- **What It Means**: Each output value in a convolutional layer only depends on a small section of the input.\n",
    "- **Why It Matters**: Because of this, the model learns to recognize objects regardless of where they are in the image. For example, it can recognize a cat whether it's in the top left corner or the center. This ability to recognize things in different locations makes the model more effective.\n",
    "\n",
    "\n",
    "## 2. Filter (Kernel)\n",
    "- **Definition**: A filter (or kernel) is a small matrix, often 3x3 or 5x5, that is used in the convolution operation to detect specific patterns in the input. Each filter captures a unique feature, like an edge or texture, in the input image.\n",
    "\n",
    "## 3. Pooling\n",
    "- **Definition**: Pooling reduces the spatial size of the feature map, helping to decrease the number of computations and control overfitting. Common types are:\n",
    "  - **Max Pooling**: Takes the maximum value from a region.\n",
    "  - **Average Pooling**: Takes the average value from a region.\n",
    "\n",
    "- **Formula**: If the input size is $ \\text{Input Size} $, pooling size is $ \\text{Pooling Size} $, and stride $ \\text{Stride} $, the output size $ \\text{Output Size} $ after pooling is:\n",
    "  $$\n",
    "  \\text{Output Size} = \\frac{\\text{Input Size} - \\text{Pooling Size}}{\\text{Stride}} + 1\n",
    "  $$\n",
    "\n",
    "## 4. Stride\n",
    "- **Definition**: Stride is the number of pixels the filter moves each time it slides over the input. A stride of 1 moves one pixel at a time; a stride of 2 moves two pixels at a time.\n",
    "\n",
    "## 5. Padding\n",
    "- **Definition**: Padding adds extra pixels (usually zeros) around the input’s border to control the output size. This can help keep the output dimensions the same as the input.\n",
    "\n",
    "- **Formula**: To maintain the input size after convolution, padding $ \\text{Padding} $ for a filter size $ \\text{Filter Size} $ is:\n",
    "  $$\n",
    "  \\text{Padding} = \\frac{\\text{Filter Size} - 1}{2}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shrinking problem in dimensions we use padding to keep number same:\n",
    "\n",
    "# Keeping Spatial Dimensions the Same in CNNs\n",
    "\n",
    "To keep the spatial dimensions of the input and output 2D feature maps the same (i.e., to prevent them from shrinking after each convolutional layer), you can use **padding**. Padding adds extra pixels around the border of the input matrix, allowing you to control the output feature map dimensions after convolution operations.\n",
    "\n",
    "## Steps to Keep Input and Output Dimensions the Same\n",
    "\n",
    "### 1. Use **Same Padding**\n",
    "For a convolution operation to keep the spatial dimensions the same, apply **\"same\" padding** (or \"full\" padding). This padding adds enough pixels around the input to ensure that the output dimensions match the input dimensions.\n",
    "\n",
    "If:\n",
    "- **Filter size** ($\\text{Filter Size}$) is odd, add padding $ \\text{Padding} = \\frac{\\text{Filter Size} - 1}{2} $ on each side.\n",
    "- **Stride** ($\\text{Stride}$) is 1 (which is typical to preserve as much information as possible).\n",
    "\n",
    "For example:\n",
    "- For a $3 \\times 3$ filter with stride 1, padding should be $ \\text{Padding} = \\frac{3 - 1}{2} = 1 $.\n",
    "- For a $5 \\times 5$ filter with stride 1, padding should be $ \\text{Padding} = \\frac{5 - 1}{2} = 2 $.\n",
    "\n",
    "By adding this padding, the convolution will produce an output with the same dimensions as the input.\n",
    "\n",
    "### 2. General Formula with Padding\n",
    "\n",
    "The output size for a convolutional layer with padding $ \\text{Padding} $, filter size $ \\text{Filter Size} $, input size $ \\text{Input Size} $, and stride $ \\text{Stride} $ is given by:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} - \\text{Filter Size} + 2 \\times \\text{Padding}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "To keep input and output sizes equal, set this formula so that **Output Size = Input Size**:\n",
    "$$\n",
    "\\text{Input Size} = \\frac{\\text{Input Size} - \\text{Filter Size} + 2 \\times \\text{Padding}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "By solving this equation with $ \\text{Stride} = 1 $, you can determine the required padding $ \\text{Padding} $.\n",
    "\n",
    "## Example: Using Same Padding in CNN Layers\n",
    "\n",
    "If you want to preserve the 2D spatial size (like 28x28) across convolutional layers with different filter sizes:\n",
    "- Use padding $ \\text{Padding} = 1 $ for $3 \\times 3$ filters.\n",
    "- Use padding $ \\text{Padding} = 2 $ for $5 \\times 5$ filters.\n",
    "\n",
    "Most deep learning frameworks (like TensorFlow, Keras, and PyTorch) have a \"same\" padding option that automatically calculates and applies the padding needed to preserve input dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding CNN Layer Components and Calculations\n",
    "\n",
    "## CNN Layer Example\n",
    "Let's analyze a convolutional layer with the following characteristics:\n",
    "* Input size: $(32 \\times 32 \\times 3)$ (height, width, and channels)\n",
    "* Filter size: $f^{[l]} = 5 \\times 5$ (height and width)\n",
    "* Number of filters: $n_c^{[l]} = 6$\n",
    "* Stride: $s^{[l]} = 1$\n",
    "* Padding: $p^{[l]} = 2$\n",
    "\n",
    "## Step 1: Input Size\n",
    "The input to the layer has dimensions $32 \\times 32 \\times 3$, meaning:\n",
    "* $n_H^{[l-1]} = 32$ (input height)\n",
    "* $n_W^{[l-1]} = 32$ (input width)\n",
    "* $n_c^{[l-1]} = 3$ (input channels, e.g., RGB)\n",
    "\n",
    "## Step 2: Output Size Formula\n",
    "The formula to calculate the output height $n_H^{[l]}$ and width $n_W^{[l]}$ is:\n",
    "\n",
    "$n_H^{[l]} = \\left\\lfloor\\frac{n_H^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1\\right\\rfloor$\n",
    "\n",
    "$n_W^{[l]} = \\left\\lfloor\\frac{n_W^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1\\right\\rfloor$\n",
    "\n",
    "Let's substitute our values:\n",
    "* $n_H^{[l-1]} = 32$\n",
    "* $p^{[l]} = 2$\n",
    "* $f^{[l]} = 5$\n",
    "* $s^{[l]} = 1$\n",
    "\n",
    "$n_H^{[l]} = \\frac{32 + 2(2) - 5}{1} + 1 = \\frac{32 + 4 - 5}{1} + 1 = 32$\n",
    "\n",
    "$n_W^{[l]} = \\frac{32 + 2(2) - 5}{1} + 1 = 32$\n",
    "\n",
    "**Output Size:** The output dimensions are $32 \\times 32$. With 6 filters, the complete output size is $32 \\times 32 \\times 6$.\n",
    "\n",
    "## Step 3: Filter Parameters\n",
    "Each filter dimensions:\n",
    "\n",
    "$f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]} = 5 \\times 5 \\times 3$\n",
    "\n",
    "Parameters per filter = $5 \\times 5 \\times 3 = 75$ weights\n",
    "\n",
    "## Step 4: Total Weights\n",
    "With $n_c^{[l]} = 6$ filters:\n",
    "\n",
    "Total weights = $75 \\times 6 = 450$\n",
    "\n",
    "## Step 5: Bias Parameters\n",
    "One bias term per filter:\n",
    "\n",
    "Total biases = $n_c^{[l]} = 6$\n",
    "\n",
    "## Step 6: Total Parameters\n",
    "Total parameters = Weights + Biases = $450 + 6 = 456$\n",
    "\n",
    "## Step 7: Activations\n",
    "The output tensor shape (feature maps):\n",
    "\n",
    "$\\text{Activations} = n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]} = 32 \\times 32 \\times 6$\n",
    "\n",
    "## Summary\n",
    "\n",
    "Parameter | Value\n",
    "----------|-------\n",
    "Input size | $32 \\times 32 \\times 3$\n",
    "Filter size | $5 \\times 5$\n",
    "Stride | $1$\n",
    "Padding | $2$\n",
    "Number of filters | $6$\n",
    "Output size | $32 \\times 32 \\times 6$\n",
    "Total weights | $450$\n",
    "Total biases | $6$\n",
    "Total parameters | $456$\n",
    "\n",
    "## Understanding the Output Size\n",
    "\n",
    "The padding $p^{[l]} = 2$ adds two rows/columns of zeros around the input image, making it $36 \\times 36$. The $5 \\times 5$ filter slides across this padded input with stride $s^{[l]} = 1$, resulting in an output size of $32 \\times 32$.\n",
    "\n",
    "Here's a Python code to verify our calculations:\n",
    "\n",
    "```python\n",
    "def calculate_output_size(input_dim, filter_size, stride, padding):\n",
    "    return ((input_dim + 2*padding - filter_size) // stride) + 1\n",
    "\n",
    "# Given parameters\n",
    "input_height = input_width = 32\n",
    "filter_size = 5\n",
    "stride = 1\n",
    "padding = 2\n",
    "\n",
    "# Calculate output dimensions\n",
    "output_height = calculate_output_size(input_height, filter_size, stride, padding)\n",
    "output_width = calculate_output_size(input_width, filter_size, stride, padding)\n",
    "\n",
    "print(f\"Output dimensions: {output_height} x {output_width}\")\n",
    "\n",
    "# Calculate total parameters\n",
    "input_channels = 3\n",
    "num_filters = 6\n",
    "weights_per_filter = filter_size * filter_size * input_channels\n",
    "total_weights = weights_per_filter * num_filters\n",
    "total_biases = num_filters\n",
    "total_parameters = total_weights + total_biases\n",
    "\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"Weights per filter: {weights_per_filter}\")\n",
    "print(f\"Total weights: {total_weights}\")\n",
    "print(f\"Total biases: {total_biases}\")\n",
    "print(f\"Total parameters: {total_parameters}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5 Overview:\n",
    "\n",
    "- **Total Parameters**: ~60,000\n",
    "- **Layers**:\n",
    "  - **Input Layer**: \\(32 \\times 32 \\times 1\\) (Grayscale image)\n",
    "  - **Convolutional Layers**: 2\n",
    "    - C1: \\(5 \\times 5\\) filters, 6 feature maps\n",
    "    - C3: \\(5 \\times 5\\) filters, 16 feature maps\n",
    "  - **Pooling/Subsampling Layers**: 2\n",
    "  - **Fully Connected Layers**: 3\n",
    "  - **Output Layer**: 10 nodes (for digit classification)\n",
    "  \n",
    "- **Computation**: \n",
    "  - Relatively lightweight compared to modern architectures. Used primarily for digit classification on MNIST.\n",
    "\n",
    "## Architecture Breakdown\n",
    "\n",
    "### Input Layer\n",
    "* Size: $32 \\times 32 \\times 1$ (grayscale image)\n",
    "\n",
    "### Layer-by-Layer Details\n",
    "\n",
    "#### 1. First Convolution (C1)\n",
    "* Filters: 6 filters of size $5 \\times 5$\n",
    "* Stride: 1\n",
    "* Padding: 0\n",
    "* Output size: $28 \\times 28 \\times 6$\n",
    "* Parameters: $6 \\times (5 \\times 5 \\times 1 + 1) = 156$\n",
    "\n",
    "#### 2. First Subsampling (S2)\n",
    "* Type: Average pooling $2 \\times 2$\n",
    "* Stride: 2\n",
    "* Output size: $14 \\times 14 \\times 6$\n",
    "* Parameters: 0\n",
    "\n",
    "#### 3. Second Convolution (C3)\n",
    "* Filters: 16 filters of size $5 \\times 5$\n",
    "* Stride: 1\n",
    "* Output size: $10 \\times 10 \\times 16$\n",
    "* Parameters: $16 \\times (5 \\times 5 \\times 6 + 1) = 2,416$\n",
    "\n",
    "#### 4. Second Subsampling (S4)\n",
    "* Type: Average pooling $2 \\times 2$\n",
    "* Stride: 2\n",
    "* Output size: $5 \\times 5 \\times 16$\n",
    "* Parameters: 0\n",
    "\n",
    "#### 5. Third Convolution (C5)\n",
    "* Filters: 120 filters of size $5 \\times 5$\n",
    "* Stride: 1\n",
    "* Output size: $1 \\times 1 \\times 120$\n",
    "* Parameters: $120 \\times (5 \\times 5 \\times 16 + 1) = 48,120$\n",
    "\n",
    "#### 6. Fully Connected (F6)\n",
    "* Input: 120 neurons\n",
    "* Output: 84 neurons\n",
    "* Parameters: $84 \\times 120 + 84 = 10,164$\n",
    "\n",
    "#### 7. Output Layer\n",
    "* Input: 84 neurons\n",
    "* Output: 10 neurons (digits 0-9)\n",
    "* Parameters: $10 \\times 84 + 10 = 850$\n",
    "\n",
    "## Total Parameters\n",
    "| Layer | Parameters |\n",
    "|-------|------------|\n",
    "| C1    | 156        |\n",
    "| C3    | 2,416      |\n",
    "| C5    | 48,120     |\n",
    "| F6    | 10,164     |\n",
    "| Output| 850        |\n",
    "| Total | 61,706     |\n",
    "\n",
    "## Convolution Operation\n",
    "For each convolution layer, output pixel $y(i,j)$ is calculated as:\n",
    "$$y(i,j) = \\sum_{m=0}^{4} \\sum_{n=0}^{4} w_{m,n} \\cdot x(i+m, j+n) + b$$\n",
    "where:\n",
    "* $w_{m,n}$ = filter weights\n",
    "* $x(i+m, j+n)$ = input pixels\n",
    "* $b$ = bias term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Overview of AlexNet\n",
    "\n",
    "**Overview**:  \n",
    "AlexNet, introduced by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012, revolutionized the field of deep learning for image classification. It won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 with a significant margin, achieving a top-5 test error rate of $16.4\\%$. The architecture introduced several innovations, including the use of Rectified Linear Units (ReLU) as activation functions, dropout for regularization, and data augmentation techniques to increase dataset diversity.\n",
    "\n",
    "**Sequel**:  \n",
    "AlexNet laid the groundwork for more advanced architectures such as VGGNet, GoogLeNet, and ResNet, each building on the principles established by AlexNet to improve accuracy and efficiency in image classification tasks.\n",
    "\n",
    "**Architecture Details**:\n",
    "- **Input Size**: $227 \\times 227 \\times 3$ (RGB image)\n",
    "- **Number of Layers**: 8 layers (5 convolutional layers + 3 fully connected layers)\n",
    "- **Total Parameters**: Approximately $62,388,299$\n",
    "- **Training Time**: Approximately 6 days on two GTX 580 GPUs\n",
    "- **Key Innovations**: \n",
    "  - **ReLU Activation**: Speeds up the training process by mitigating the vanishing gradient problem.\n",
    "  - **Dropout Layers**: Regularizes the model by randomly dropping units during training.\n",
    "  - **Data Augmentation**: Increases the diversity of the training dataset through transformations such as rotation and scaling.\n",
    "\n",
    "---\n",
    "\n",
    "### Layer Breakdown\n",
    "\n",
    "1. **Layer 1**: Convolutional Layer\n",
    "   - **Input**: $227 \\times 227 \\times 3$\n",
    "   - **Filter Size**: $11 \\times 11$\n",
    "   - **Stride**: 4\n",
    "   - **Number of Filters**: 96\n",
    "   - **Output**: $55 \\times 55 \\times 96$\n",
    "   - **Parameters**: $(11 \\times 11 \\times 3 \\times 96) + 96 = 34,944$\n",
    "\n",
    "2. **Layer 2**: Max Pooling Layer\n",
    "   - **Input**: $55 \\times 55 \\times 96$\n",
    "   - **Pooling Size**: $3 \\times 3$\n",
    "   - **Stride**: 2\n",
    "   - **Output**: $27 \\times 27 \\times 96$\n",
    "\n",
    "3. **Layer 3**: Convolutional Layer\n",
    "   - **Input**: $27 \\times 27 \\times 96$\n",
    "   - **Filter Size**: $5 \\times 5$\n",
    "   - **Padding**: 2\n",
    "   - **Number of Filters**: 256\n",
    "   - **Output**: $27 \\times 27 \\times 256$\n",
    "   - **Parameters**: $(5 \\times 5 \\times 96 \\times 256) + 256 = 614,656$\n",
    "\n",
    "4. **Layer 4**: Max Pooling Layer\n",
    "   - **Input**: $27 \\times 27 \\times 256$\n",
    "   - **Pooling Size**: $3 \\times 3$\n",
    "   - **Stride**: 2\n",
    "   - **Output**: $13 \\times 13 \\times 256$\n",
    "\n",
    "5. **Layer 5**: Convolutional Layer\n",
    "   - **Input**: $13 \\times 13 \\times 256$\n",
    "   - **Filter Size**: $3 \\times 3$\n",
    "   - **Padding**: 1\n",
    "   - **Number of Filters**: 384\n",
    "   - **Output**: $13 \\times 13 \\times 384$\n",
    "   - **Parameters**: $(3 \\times 3 \\times 256 \\times 384) + 384 = 885,120$\n",
    "\n",
    "6. **Layer 6**: Convolutional Layer\n",
    "   - **Input**: $13 \\times 13 \\times 384$\n",
    "   - **Filter Size**: $3 \\times 3$\n",
    "   - **Padding**: 1\n",
    "   - **Number of Filters**: 384\n",
    "   - **Output**: $13 \\times 13 \\times 384$\n",
    "   - **Parameters**: $(3 \\times 3 \\times 384 \\times 384) + 384 = 1,327,488$\n",
    "\n",
    "7. **Layer 7**: Convolutional Layer\n",
    "   - **Input**: $13 \\times 13 \\times 384$\n",
    "   - **Filter Size**: $3 \\times 3$\n",
    "   - **Padding**: 1\n",
    "   - **Number of Filters**: 256\n",
    "   - **Output**: $13 \\times 13 \\times 256$\n",
    "   - **Parameters**: $(3 \\times 3 \\times 384 \\times 256) + 256 = 884,736$\n",
    "\n",
    "8. **Layer 8**: Max Pooling Layer\n",
    "   - **Input**: $13 \\times 13 \\times 256$\n",
    "   - **Pooling Size**: $3 \\times 3$\n",
    "   - **Stride**: 2\n",
    "   - **Output**: $6 \\times 6 \\times 256$\n",
    "\n",
    "9. **Layer 9**: Flatten\n",
    "   - **Input**: $6 \\times 6 \\times 256$\n",
    "   - **Output**: $9216$\n",
    "\n",
    "10. **Layer 10**: Fully Connected Layer\n",
    "    - **Input**: $9216$\n",
    "    - **Output**: $4096$\n",
    "    - **Parameters**: $9216 \\times 4096 + 4096 = 37,752,832$\n",
    "\n",
    "11. **Layer 11**: Dropout Layer (p=0.5)\n",
    "\n",
    "12. **Layer 12**: Fully Connected Layer\n",
    "    - **Input**: $4096$\n",
    "    - **Output**: $4096$\n",
    "    - **Parameters**: $4096 \\times 4096 + 4096 = 16,781,312$\n",
    "\n",
    "13. **Layer 13**: Dropout Layer (p=0.5)\n",
    "\n",
    "14. **Layer 14**: Fully Connected Layer\n",
    "    - **Input**: $4096$\n",
    "    - **Output**: $1000$\n",
    "    - **Parameters**: $4096 \\times 1000 + 1000 = 4,097,000$\n",
    "\n",
    "---\n",
    "\n",
    "### Total Parameters Calculation\n",
    "\n",
    "- **Total Parameters** = $34,944 + 614,656 + 885,120 + 1,327,488 + 884,736 + 1,327,488 + 884,736 + 37,752,832 + 16,781,312 + 4,097,000 = 62,388,299$\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Key Metrics\n",
    "\n",
    "| Parameter                  | Value           |\n",
    "|----------------------------|-----------------|\n",
    "| Input Size                 | $227 \\times 227 \\times 3$ |\n",
    "| Number of Layers           | 8               |\n",
    "| Total Parameters            | $62,388,299$    |\n",
    "| Training Time              | 6 days on two GTX 580 GPUs |\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# VGG Architecture Overview\n",
    "\n",
    "- **Total Parameters**: ~138 million\n",
    "- **Layers**:\n",
    "  - **Input Layer**: $224 \\times 224 \\times 3$ (RGB image)\n",
    "  - **Convolutional Layers**: 13\n",
    "  - **Fully Connected Layers**: 3\n",
    "  - **Output Layer**: 1000 nodes (for classification)\n",
    "\n",
    "- **Computation**: \n",
    "  - VGG architectures are deep CNNs with many layers, emphasizing uniform architecture using small $3 \\times 3$ filters.\n",
    "\n",
    "## 1. What's New in VGG?\n",
    "VGG introduced the concept of deep networks with very small filters:\n",
    "- **Small receptive fields** ($3 \\times 3$) that allow for deeper architectures without significantly increasing the number of parameters.\n",
    "- **Increased depth** improves feature extraction.\n",
    "- **Uniform architecture** simplifies the design process.\n",
    "\n",
    "## 2. Sequel of Which Model?\n",
    "VGG follows the evolution from earlier models like AlexNet, enhancing the depth and consistency of convolutional layer designs.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. VGG Architecture and Layer Details\n",
    "\n",
    "| Layer Type         | Output Size               | Parameters         |\n",
    "|--------------------|--------------------------|---------------------|\n",
    "| Input              | $224 \\times 224 \\times 3$| -                   |\n",
    "| Convolution (Conv1)| $224 \\times 224 \\times 64$ | $3 \\times 3 \\times 3 \\times 64 + 64 = 1,792$ |\n",
    "| Convolution (Conv2)| $224 \\times 224 \\times 64$ | $3 \\times 3 \\times 64 \\times 64 + 64 = 36,928$ |\n",
    "| Max Pooling        | $112 \\times 112 \\times 64$ | -                   |\n",
    "| Convolution (Conv3)| $112 \\times 112 \\times 128$ | $3 \\times 3 \\times 64 \\times 128 + 128 = 73,856$ |\n",
    "| Convolution (Conv4)| $112 \\times 112 \\times 128$ | $3 \\times 3 \\times 128 \\times 128 + 128 = 147,584$ |\n",
    "| Max Pooling        | $56 \\times 56 \\times 128$  | -                   |\n",
    "| Convolution (Conv5)| $56 \\times 56 \\times 256$  | $3 \\times 3 \\times 128 \\times 256 + 256 = 295,168$ |\n",
    "| Convolution (Conv6)| $56 \\times 56 \\times 256$  | $3 \\times 3 \\times 256 \\times 256 + 256 = 590,080$ |\n",
    "| Max Pooling        | $28 \\times 28 \\times 256$  | -                   |\n",
    "| Convolution (Conv7)| $28 \\times 28 \\times 512$  | $3 \\times 3 \\times 256 \\times 512 + 512 = 1,180,160$ |\n",
    "| Convolution (Conv8)| $28 \\times 28 \\times 512$  | $3 \\times 3 \\times 512 \\times 512 + 512 = 2,359,808$ |\n",
    "| Max Pooling        | $14 \\times 14 \\times 512$  | -                   |\n",
    "| Convolution (Conv9)| $14 \\times 14 \\times 512$  | $3 \\times 3 \\times 512 \\times 512 + 512 = 2,359,808$ |\n",
    "| Convolution (Conv10)| $14 \\times 14 \\times 512$ | $3 \\times 3 \\times 512 \\times 512 + 512 = 2,359,808$ |\n",
    "| Max Pooling        | $7 \\times 7 \\times 512$    | -                   |\n",
    "| Flatten            | $1 \\times 1 \\times 512$    | -                   |\n",
    "| Fully Connected (FC1)| $4096$                   | $512 \\times 4096 + 4096 = 2,097,152$ |\n",
    "| Fully Connected (FC2)| $4096$                   | $4096 \\times 4096 + 4096 = 16,781,312$ |\n",
    "| Output Layer       | $1000$                    | $4096 \\times 1000 + 1000 = 4,097,000$ |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Overall Parameter Calculation\n",
    "Adding up all the parameters from the layers gives a total of approximately **138 million** parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Mathematical Breakdown (Example of a Convolution)\n",
    "\n",
    "Consider the convolution operation in the first convolutional layer (Conv1):\n",
    "\n",
    "For an input of size $224 \\times 224 \\times 3$ and a $3 \\times 3$ filter, the output pixel $y(i, j)$ can be expressed as:\n",
    "\n",
    "$$\n",
    "y(i, j) = \\sum_{m=0}^{2} \\sum_{n=0}^{2} w_{m,n} \\cdot x(i+m, j+n) + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $w_{m,n}$ are the weights of the $3 \\times 3$ filter.\n",
    "- $x(i+m, j+n)$ are the input pixels.\n",
    "- $b$ is the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Architecture\n",
    "\n",
    "## Overview\n",
    "* Year: 2015\n",
    "* Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "* Key Innovation: Introduction of residual connections to enable training of very deep networks (up to 152 layers).\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* Size: $224 \\times 224 \\times 3$\n",
    "* Type: RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows this simple formula:\n",
    "$$\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer           | Input Size               | Operation                  | Output Size               | Parameters   |\n",
    "|-----------------|--------------------------|----------------------------|---------------------------|--------------|\n",
    "| Conv1           | $224 \\times 224 \\times 3$| Conv 7×7, stride 2         | $112 \\times 112 \\times 64$| 9,408        |\n",
    "| Pool1           | $112 \\times 112 \\times 64$| MaxPool 3×3, stride 2     | $56 \\times 56 \\times 64$  | 0            |\n",
    "| Conv2_x         | $56 \\times 56 \\times 64$ | 3x3 Conv, 64 filters       | $56 \\times 56 \\times 64$  | 73,728       |\n",
    "| Conv3_x         | $56 \\times 56 \\times 64$ | 3x3 Conv, 128 filters      | $28 \\times 28 \\times 128$ | 147,584      |\n",
    "| Conv4_x         | $28 \\times 28 \\times 128$| 3x3 Conv, 256 filters      | $14 \\times 14 \\times 256$ | 590,080      |\n",
    "| Conv5_x         | $14 \\times 14 \\times 256$| 3x3 Conv, 512 filters      | $7 \\times 7 \\times 512$   | 2,359,296    |\n",
    "| Pool2           | $7 \\times 7 \\times 512$  | Global Average Pooling     | $1 \\times 1 \\times 512$   | 0            |\n",
    "| FC              | $1 \\times 1 \\times 512$  | Fully Connected Layer (1000 classes) | $1 \\times 1 \\times 1000$ | 513,000      |\n",
    "\n",
    "### Parameter Summary\n",
    "\n",
    "| Parameter                  | Value                        |\n",
    "|----------------------------|------------------------------|\n",
    "| Input Size                 | $224 \\times 224 \\times 3$   |\n",
    "| Number of Layers           | 152                          |\n",
    "| Total Parameters            | $62,388,299$                |\n",
    "| Training Time              | 6 days on two GTX 580 GPUs  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Inception Architecture\n",
    "\n",
    "## Overview\n",
    "- **Year**: 2014\n",
    "- **Authors**: Christian Szegedy et al.\n",
    "- **Key Innovations**:\n",
    "  - **Inception Module**: Introduces parallel convolutional paths with different filter sizes, capturing multi-scale features.\n",
    "  - **Global Average Pooling**: Replaces fully connected layers to reduce overfitting and improve generalization.\n",
    "  - **Factorized Convolutions**: Decomposes large filters into smaller ones (e.g., 5x5 into two 3x3), reducing parameters and computational complexity.\n",
    "  - **Nin (Network in Network)**: Uses micro-networks to create more abstract representations.\n",
    "  - **1x1 Convolutions**: Enhances dimensionality reduction and allows deeper architectures while managing computational cost.\n",
    "  - **Auxiliary Classifiers**: Provides intermediate supervision to improve convergence during training.\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "- **Size**: $227 \\times 227 \\times 3$\n",
    "- **Type**: RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows this formula:\n",
    "$$\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer      | Input Size                | Operation                   | Output Size                | Parameters  |\n",
    "|------------|---------------------------|-----------------------------|----------------------------|-------------|\n",
    "| Conv1      | $227 \\times 227 \\times 3$ | Conv 7×7, stride 2          | $112 \\times 112 \\times 64$ | 9,472       |\n",
    "| Pool1      | $112 \\times 112 \\times 64$| MaxPool 3×3, stride 2       | $56 \\times 56 \\times 64$   | 0           |\n",
    "| Conv2      | $56 \\times 56 \\times 64$  | Conv 1×1, 64 filters         | $56 \\times 56 \\times 64$   | 4,160       |\n",
    "| Conv3      | $56 \\times 56 \\times 64$  | Conv 3×3, 128 filters        | $54 \\times 54 \\times 128$  | 73,856      |\n",
    "| Pool2      | $54 \\times 54 \\times 128$ | MaxPool 3×3, stride 2       | $26 \\times 26 \\times 128$  | 0           |\n",
    "| Inception1 | $26 \\times 26 \\times 128$ | Inception Module            | $26 \\times 26 \\times 256$  | 15,000+     |\n",
    "| ...        | ...                       | ...                         | ...                        | ...         |\n",
    "| Pool3      | $26 \\times 26 \\times N$   | Average Pooling             | $1 \\times 1 \\times N$      | 0           |\n",
    "| Output     | $1 \\times 1 \\times N$     | Softmax                     | $1 \\times 1 \\times K$      | 0           |\n",
    "\n",
    "### Summary Table of Parameters\n",
    "\n",
    "| Parameter                  | Value                      |\n",
    "|----------------------------|----------------------------|\n",
    "| Input Size                 | $227 \\times 227 \\times 3$  |\n",
    "| Number of Layers           | 22 (total layers in Inception-v1) |\n",
    "| Total Parameters            | $5,000,000+ \\text{ (depends on specific version)}$ |\n",
    "| Training Time              | Varies based on dataset   |\n",
    "\n",
    "### Sequel of Which Model?\n",
    "Inception has evolved into several successors, including:\n",
    "- **Inception-v2**: Improved performance and reduced computational cost with factorized convolutions.\n",
    "- **Inception-v3**: Further enhancements with label smoothing and auxiliary logits.\n",
    "- **Inception-ResNet**: Combines Inception modules with residual connections for better gradient flow.\n",
    "\n",
    "--- \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the updated Jupyter notebook markdown breakdown for the GoogLeNet architecture, with separate tables for the summary and the layer-by-layer breakdown:\n",
    "\n",
    "# GoogLeNet\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2014\n",
    "* **Authors:** Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n",
    "* **Key Innovations:**\n",
    "  * Introduced the Inception module, allowing for multiple filter sizes in parallel.\n",
    "  * Used global average pooling instead of fully connected layers to reduce overfitting.\n",
    "  * Employed 1x1 convolutions for dimensionality reduction, significantly reducing computational costs.\n",
    "  * Implemented auxiliary classifiers during training to combat the vanishing gradient problem.\n",
    "  * Achieved state-of-the-art performance on the ImageNet dataset at the time of release.\n",
    "* **Sequel to:** No direct sequel, but influenced later models like Inception v2 and Inception v3.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type              | Count | Hardware Used              | FLOPs        | Computation Time | Hours Taken |\n",
    "|-------------------------|-------|----------------------------|--------------|------------------|-------------|\n",
    "| Convolutional Layers    | 22    | 4 GPUs (NVIDIA K40)       | ~1.5 billion | 3 days           | ~72 hours   |\n",
    "| Pooling Layers          | 5     |                            |              |                  |             |\n",
    "| Fully Connected Layers   | 0     |                            |              |                  |             |\n",
    "| Total Layers            | 27    |                            |              |                  |             |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $224 \\times 224 \\times 3$\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows this formula:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer     | Input Size               | Operation                         | Output Size               | Parameters |\n",
    "|-----------|--------------------------|-----------------------------------|---------------------------|------------|\n",
    "| Conv1     | $224 \\times 224 \\times 3$| Conv 7×7, stride 2                | $112 \\times 112 \\times 64$| 9,408      |\n",
    "| Pool1     | $112 \\times 112 \\times 64$| MaxPool 3×3, stride 2            | $56 \\times 56 \\times 64$  | 0          |\n",
    "| Conv2     | $56 \\times 56 \\times 64$ | Conv 1×1, stride 1                | $56 \\times 56 \\times 64$  | 4,128      |\n",
    "| Conv3     | $56 \\times 56 \\times 64$ | Conv 3×3, stride 1                | $56 \\times 56 \\times 192$ | 110,080    |\n",
    "| Pool2     | $56 \\times 56 \\times 192$| MaxPool 3×3, stride 2            | $28 \\times 28 \\times 192$ | 0          |\n",
    "| Inception 1 | $28 \\times 28 \\times 192$| Inception Module 1              | $28 \\times 28 \\times 256$ | 18,720     |\n",
    "| Inception 2 | $28 \\times 28 \\times 256$| Inception Module 2              | $28 \\times 28 \\times 480$ | 113,840    |\n",
    "| Pool3     | $28 \\times 28 \\times 480$| MaxPool 3×3, stride 2            | $14 \\times 14 \\times 480$ | 0          |\n",
    "| Inception 3 | $14 \\times 14 \\times 480$| Inception Module 3              | $14 \\times 14 \\times 512$ | 243,712    |\n",
    "| Inception 4 | $14 \\times 14 \\times 512$| Inception Module 4              | $14 \\times 14 \\times 512$ | 215,304    |\n",
    "| Inception 5 | $14 \\times 14 \\times 512$| Inception Module 5              | $14 \\times 14 \\times 528$ | 226,728    |\n",
    "| Pool4     | $14 \\times 14 \\times 528$| MaxPool 3×3, stride 2            | $7 \\times 7 \\times 528$   | 0          |\n",
    "| Conv4     | $7 \\times 7 \\times 528$  | Conv 1×1, stride 1                | $7 \\times 7 \\times 128$   | 67,584     |\n",
    "| Conv5     | $7 \\times 7 \\times 128$  | Conv 1×1, stride 1                | $7 \\times 7 \\times 128$   | 16,384     |\n",
    "| Pool5     | $7 \\times 7 \\times 128$  | Average Pooling                  | $1 \\times 1 \\times 128$    | 0          |\n",
    "\n",
    "### Parameter Calculation Example\n",
    "For Conv1 layer:\n",
    "* Weights: $7 \\times 7 \\times 3 \\times 64 = 9,408$\n",
    "* Biases: $64$\n",
    "* Total: $9,408 + 64 = 9,472$ parameters\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for GoogLeNet: Approximately 5 million.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:red\"> changed to object detection and tracking kind of models <span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization and Sliding Window in Object Detection\n",
    "\n",
    "**Localization** refers to the process of identifying the location of an object within an image. In the context of object detection, it involves determining the precise bounding box coordinates (x, y, width, height) that enclose the object of interest.\n",
    "\n",
    " Key Points:\n",
    "- **Bounding Box**: A rectangular box that is drawn around the object to indicate its position.\n",
    "- **Applications**: Localization is essential in various applications, such as facial recognition, autonomous driving, and surveillance systems.\n",
    "- **Metrics**: Common metrics for evaluating localization accuracy include Intersection Over Union (IoU), as it measures how well the predicted bounding box aligns with the ground truth bounding box.\n",
    "\n",
    "Sliding Window\n",
    "The **Sliding Window** technique is a method used to detect objects in images by systematically scanning the image at various locations and scales. It involves the following steps:\n",
    "\n",
    "1. **Window Definition**: A fixed-size window (or bounding box) is defined to slide across the image.\n",
    "2. **Window Movement**: The window is moved across the image in small, overlapping steps (both horizontally and vertically).\n",
    "3. **Classification**: At each position of the window, a classifier (like a neural network) is applied to determine if the window contains the object of interest.\n",
    "4. **Scaling**: The window can be resized to detect objects at different scales, allowing for the detection of objects of various sizes within the same image.\n",
    "\n",
    " Key Points:\n",
    "- **Exhaustive Search**: The sliding window approach can be computationally expensive as it involves evaluating many regions of the image.\n",
    "- **Combining with Other Techniques**: Sliding window methods are often combined with other techniques, like non-max suppression, to eliminate redundant detections and improve the final output.\n",
    "- **Limitations**: The main limitation is the trade-off between detection accuracy and computational efficiency, as using small windows with small steps can lead to a high number of evaluations.\n",
    "\n",
    " Conclusion\n",
    "Localization is a fundamental aspect of object detection that focuses on accurately identifying the position of objects within images. The sliding window technique is a widely used method for achieving this by systematically scanning the image, though it can be computationally intensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection Over Union (IoU)\n",
    "\n",
    "**Intersection Over Union (IoU)** is a metric used to evaluate the performance of object detection algorithms. It quantifies the overlap between two bounding boxes: the ground truth (labeled output) and the predicted output.\n",
    "\n",
    " Definition\n",
    "IoU is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "\\text{IoU} = \\frac{\\text{Intersection Area}}{\\text{Union Area}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Intersection Area** is the area of overlap between the two bounding boxes.\n",
    "- **Union Area** is the total area covered by both bounding boxes.\n",
    "\n",
    " Steps to Calculate IoU\n",
    "1. **Calculate the Intersection Area**: This is the area where the two rectangles overlap.\n",
    "2. **Calculate the Union Area**: This is the total area covered by both rectangles. It can be computed as:\n",
    "   $$\n",
    "   \\text{Union Area} = \\text{Area of Rectangle 1} + \\text{Area of Rectangle 2} - \\text{Intersection Area}\n",
    "   $$\n",
    "3. **Compute IoU**: Use the IoU formula stated above.\n",
    "\n",
    " Example\n",
    "- **Ground Truth (Red Rectangle)**: This represents the true location of the object.\n",
    "- **Predicted Output (Purple Rectangle)**: This represents the model's prediction.\n",
    "\n",
    " Visualization\n",
    "- The red rectangle represents the labeled output (ground truth).\n",
    "- The purple rectangle represents the predicted output.\n",
    "\n",
    " IoU Interpretation\n",
    "- If IoU ≥ 0.5: The prediction is considered good.\n",
    "- The best possible IoU is 1, indicating perfect overlap.\n",
    "- The higher the IoU, the better the accuracy of the object detection algorithm.\n",
    "\n",
    " Conclusion\n",
    "IoU is a crucial metric in evaluating object detection models, as it provides a clear understanding of how well the predicted bounding box matches the ground truth. A higher IoU indicates better model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the Jupyter notebook markdown breakdown for the RCNN architecture, including a separate table for computational power:\n",
    "\n",
    "# RCNN (Regions with Convolutional Neural Networks)--R-CNN tries to pick a few windows and run a Conv net (your confident classifier) on top of them.\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2014\n",
    "* **Authors:** Ross B. Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik\n",
    "* **Key Innovations:**\n",
    "  * Introduced a region proposal network (RPN) to generate object proposals efficiently.\n",
    "  * Combined region proposals with CNN features for object classification and bounding box regression.\n",
    "  * Used a two-stage process: first generating proposals and then classifying them.\n",
    "  * Employed a selective search algorithm to generate region proposals initially.\n",
    "  * Achieved state-of-the-art performance on the PASCAL VOC dataset at the time of release.\n",
    "* **Sequel to:** No direct sequel, but influenced later models like Fast R-CNN and Mask R-CNN.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type              | Count | Parameters |\n",
    "|-------------------------|-------|------------|\n",
    "| Convolutional Layers    | 5     | 2,310,000  |\n",
    "| Region Proposal Network  | 1     | 2,200,000  |\n",
    "| Total Layers            | 6     | 4,510,000  |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Hardware Used              | FLOPs          | Computation Time | Hours Taken |\n",
    "|----------------------------|----------------|------------------|-------------|\n",
    "| 2 NVIDIA K40 GPUs          | ~10 billion    | 2 days           | ~48 hours   |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $224 \\times 224 \\times 3$\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows this formula:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer           | Input Size               | Operation                         | Output Size               | Parameters |\n",
    "|------------------|--------------------------|-----------------------------------|---------------------------|------------|\n",
    "| Conv1           | $224 \\times 224 \\times 3$| Conv 7×7, stride 2                | $112 \\times 112 \\times 64$| 9,408      |\n",
    "| Pool1           | $112 \\times 112 \\times 64$| MaxPool 3×3, stride 2            | $56 \\times 56 \\times 64$  | 0          |\n",
    "| Conv2           | $56 \\times 56 \\times 64$ | Conv 3×3, stride 1                | $56 \\times 56 \\times 192$ | 110,080    |\n",
    "| Pool2           | $56 \\times 56 \\times 192$| MaxPool 3×3, stride 2            | $28 \\times 28 \\times 192$ | 0          |\n",
    "| Conv3           | $28 \\times 28 \\times 192$| Conv 1×1, stride 1                | $28 \\times 28 \\times 128$ | 24,576     |\n",
    "| Conv4           | $28 \\times 28 \\times 128$| Conv 1×1, stride 1                | $28 \\times 28 \\times 128$ | 16,384     |\n",
    "| Conv5           | $28 \\times 28 \\times 128$| Conv 1×1, stride 1                | $28 \\times 28 \\times 64$  | 8,192      |\n",
    "| RPN              | $28 \\times 28 \\times 64$ | Region Proposal Network           | $28 \\times 28 \\times 256$ | 2,200,000  |\n",
    "\n",
    "### Parameter Calculation Example\n",
    "For Conv1 layer:\n",
    "* Weights: $7 \\times 7 \\times 3 \\times 64 = 9,408$\n",
    "* Biases: $64$\n",
    "* Total: $9,408 + 64 = 9,472$ parameters\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for RCNN: Approximately 4.5 million.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the Jupyter notebook markdown breakdown for the Fast R-CNN architecture, including separate tables for the summary and computational power:\n",
    "\n",
    "# Fast R-CNN\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2015\n",
    "* **Authors:** Ross B. Girshick\n",
    "* **Key Innovations:**\n",
    "  * Introduced a single-stage training process, eliminating the need for a separate stage for generating object proposals.\n",
    "  * Utilized a softmax layer for multi-class classification, improving efficiency.\n",
    "  * Implemented ROI pooling to extract features for each proposed region from the shared convolutional feature map.\n",
    "  * Reduced computation time significantly compared to its predecessor, RCNN, by sharing computations across proposals.\n",
    "  * Achieved state-of-the-art performance on the PASCAL VOC dataset and improved the speed of object detection.\n",
    "* **Sequel to:** Fast R-CNN is a refinement of the original R-CNN model and influenced further models like Mask R-CNN.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type              | Count | Parameters |\n",
    "|-------------------------|-------|------------|\n",
    "| Convolutional Layers    | 5     | 2,200,000  |\n",
    "| ROI Pooling Layer       | 1     | 0          |\n",
    "| Total Layers            | 6     | 2,200,000  |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Hardware Used              | FLOPs          | Computation Time | Hours Taken |\n",
    "|----------------------------|----------------|------------------|-------------|\n",
    "| 1 NVIDIA K40 GPU           | ~5 billion     | 1 day            | ~24 hours   |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $224 \\times 224 \\times 3$\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows this formula:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer           | Input Size               | Operation                         | Output Size               | Parameters |\n",
    "|------------------|--------------------------|-----------------------------------|---------------------------|------------|\n",
    "| Conv1           | $224 \\times 224 \\times 3$| Conv 7×7, stride 2                | $112 \\times 112 \\times 64$| 9,408      |\n",
    "| Pool1           | $112 \\times 112 \\times 64$| MaxPool 3×3, stride 2            | $56 \\times 56 \\times 64$  | 0          |\n",
    "| Conv2           | $56 \\times 56 \\times 64$ | Conv 3×3, stride 1                | $56 \\times 56 \\times 192$ | 110,080    |\n",
    "| Pool2           | $56 \\times 56 \\times 192$| MaxPool 3×3, stride 2            | $28 \\times 28 \\times 192$ | 0          |\n",
    "| Conv3           | $28 \\times 28 \\times 192$| Conv 1×1, stride 1                | $28 \\times 28 \\times 128$ | 24,576     |\n",
    "| Conv4           | $28 \\times 28 \\times 128$| Conv 1×1, stride 1                | $28 \\times 28 \\times 128$ | 16,384     |\n",
    "| Conv5           | $28 \\times 28 \\times 128$| Conv 1×1, stride 1                | $28 \\times 28 \\times 64$  | 8,192      |\n",
    "| ROI Pooling      | $28 \\times 28 \\times 64$ | ROI Pooling                       | Varies (based on ROIs)    | 0          |\n",
    "\n",
    "### Parameter Calculation Example\n",
    "For Conv1 layer:\n",
    "* Weights: $7 \\times 7 \\times 3 \\times 64 = 9,408$\n",
    "* Biases: $64$\n",
    "* Total: $9,408 + 64 = 9,472$ parameters\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for Fast R-CNN: Approximately 2.2 million.\n",
    "\n",
    "This format organizes the information clearly, maintaining the structure you've requested. If you need any further adjustments or additional details, just let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a structured overview of 3D Convolutional Networks (3D ConvNets), including their architecture, key innovations, summary, and computational power tables.\n",
    "\n",
    "# 3D Convolutional Networks (3D ConvNets) Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2015\n",
    "* **Authors:** David Tran, Llion Jones, Matteo Weissenborn, and others\n",
    "* **Key Innovations:**\n",
    "  * Introduced 3D convolutional layers that operate over 3D volumes (e.g., video frames), enabling the model to capture spatial and temporal features simultaneously.\n",
    "  * Utilized pooling layers in 3D, enhancing the network's ability to learn features from videos and volumetric data.\n",
    "  * Achieved state-of-the-art performance on action recognition tasks in video datasets.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type              | Count | Parameters       |\n",
    "|-------------------------|-------|------------------|\n",
    "| 3D Convolutional Layers  | 5     | 2,205,000        |\n",
    "| Fully Connected Layers    | 2     | 6,590,000        |\n",
    "| Total Layers            | 7     | 8,795,000        |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Hardware Used              | FLOPs          | Computation Time | Hours Taken |\n",
    "|----------------------------|----------------|------------------|-------------|\n",
    "| 4 NVIDIA Titan X GPUs      | ~10 billion    | 2 days           | ~48 hours   |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $N \\times T \\times H \\times W \\times C$ (where $N$ is the batch size, $T$ is the number of frames, $H$ is height, $W$ is width, and $C$ is channels)\n",
    "* **Type:** Video (3D data)\n",
    "\n",
    "### Layer Calculations\n",
    "Each 3D convolution layer follows the formula:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer          | Input Size                | Operation                 | Output Size                | Parameters  |\n",
    "|----------------|---------------------------|---------------------------|----------------------------|-------------|\n",
    "| Conv3D_1      | $N \\times T \\times 16 \\times 112 \\times 112$ | 3D Conv (3x3x3)        | $N \\times T \\times 16 \\times 112 \\times 112$ | 1,472,000   |\n",
    "| Conv3D_2      | $N \\times T \\times 16 \\times 112 \\times 112$ | 3D Conv (3x3x3)        | $N \\times T \\times 32 \\times 56 \\times 56$  | 1,228,800   |\n",
    "| Pool3D        | $N \\times T \\times 32 \\times 56 \\times 56$  | 3D Max Pooling (2x2x2) | $N \\times T \\times 32 \\times 28 \\times 28$  | 0           |\n",
    "| Conv3D_3      | $N \\times T \\times 32 \\times 28 \\times 28$  | 3D Conv (3x3x3)        | $N \\times T \\times 64 \\times 28 \\times 28$  | 1,024,000   |\n",
    "| Conv3D_4      | $N \\times T \\times 64 \\times 28 \\times 28$  | 3D Conv (3x3x3)        | $N \\times T \\times 128 \\times 14 \\times 14$ | 2,000,000   |\n",
    "| Pool3D        | $N \\times T \\times 128 \\times 14 \\times 14$ | 3D Max Pooling (2x2x2) | $N \\times T \\times 128 \\times 7 \\times 7$   | 0           |\n",
    "| FC            | $N \\times T \\times 128 \\times 7 \\times 7$   | Fully Connected         | $N \\times num\\_classes$    | 6,590,000   |\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for 3D ConvNets: Approximately 8,795,000.\n",
    "\n",
    "Let me know if you need any modifications or further details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the detailed architecture breakdown for the Single Shot MultiBox Detector (SSD):\n",
    "\n",
    "# SSD (Single Shot MultiBox Detector) Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2016\n",
    "* **Authors:** Wei Liu, Andreys David, Jiafeng Guo, et al.\n",
    "* **Key Innovations:**\n",
    "  * Combines the advantages of both detection and classification in a single network.\n",
    "  * Uses multi-scale feature maps to detect objects of various sizes.\n",
    "  * Employs a single deep neural network for real-time object detection.\n",
    "\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type              | Count | Parameters    |\n",
    "|-------------------------|-------|---------------|\n",
    "| Convolutional Layers    | 6     | 5,304,000     |\n",
    "| Fully Connected Layers   | 3     | 1,260,000     |\n",
    "| **Total Layers**        | **9** | **6,564,000** |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Hardware Used              | FLOPs          | Computation Time | Hours Taken |\n",
    "|----------------------------|----------------|------------------|-------------|\n",
    "| 2 NVIDIA K40 GPUs          | ~15 billion    | 1 day            | ~24 hours   |\n",
    "\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $300 \\times 300 \\times 3$\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows the formula:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer               | Input Size               | Operation                         | Output Size               | Parameters |\n",
    "|---------------------|--------------------------|-----------------------------------|---------------------------|------------|\n",
    "| Conv1               | $300 \\times 300 \\times 3$| Conv 3×3, stride 2                | $150 \\times 150 \\times 64$| 1,728      |\n",
    "| Conv2               | $150 \\times 150 \\times 64$| Conv 3×3, stride 1               | $150 \\times 150 \\times 128$| 73,728     |\n",
    "| Conv3               | $150 \\times 150 \\times 128$| Conv 3×3, stride 1              | $150 \\times 150 \\times 256$| 295,168    |\n",
    "| Conv4               | $150 \\times 150 \\times 256$| Conv 3×3, stride 1              | $150 \\times 150 \\times 512$| 1,180,160  |\n",
    "| Conv5               | $150 \\times 150 \\times 512$| Conv 3×3, stride 1              | $150 \\times 150 \\times 512$| 2,359,296  |\n",
    "| Conv6               | $150 \\times 150 \\times 512$| Conv 3×3, stride 2              | $75 \\times 75 \\times 512$  | 2,359,296  |\n",
    "| Conv7               | $75 \\times 75 \\times 512$  | Conv 3×3, stride 1              | $75 \\times 75 \\times 256$  | 1,179,648  |\n",
    "| Conv8               | $75 \\times 75 \\times 256$  | Conv 3×3, stride 1              | $75 \\times 75 \\times 256$  | 590,080    |\n",
    "| Conv9               | $75 \\times 75 \\times 256$  | Conv 3×3, stride 1              | $75 \\times 75 \\times 128$  | 295,040    |\n",
    "| Conv10              | $75 \\times 75 \\times 128$  | Conv 3×3, stride 1              | $75 \\times 75 \\times 128$  | 147,584    |\n",
    "| Conv11              | $75 \\times 75 \\times 128$  | Conv 3×3, stride 1              | $75 \\times 75 \\times 64$   | 73,792     |\n",
    "| Conv12              | $75 \\times 75 \\times 64$   | Conv 3×3, stride 1              | $75 \\times 75 \\times 64$   | 36,864     |\n",
    "| Conv13              | $75 \\times 75 \\times 64$   | Conv 3×3, stride 1              | $75 \\times 75 \\times 32$   | 18,432     |\n",
    "| Fully Connected     | $75 \\times 75 \\times 32$   | FC Layer                          | 8732 (bounding boxes)      | 1,000,000  |\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for SSD: Approximately 87 million.\n",
    "\n",
    "## Summary of Improvements\n",
    "- **SSD with MobileNet (2017):**\n",
    "  - Introduced a lightweight version for mobile applications.\n",
    "  - Achieved a good balance between speed and accuracy.\n",
    "\n",
    "- **SSD512 (2016):**\n",
    "  - Used a larger input size of $512 \\times 512$ for improved performance.\n",
    "  \n",
    "- **Enhanced SSD Models (2018-2020):**\n",
    "  - Focused on better anchor box selection and improved loss functions.\n",
    "  - Integrated features from other state-of-the-art models for enhanced accuracy.\n",
    "\n",
    "Feel free to ask for more details or any specific aspects you would like to expand on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the Jupyter notebook markdown breakdown for the Faster R-CNN architecture, including separate tables for the summary and computational power:\n",
    "\n",
    "# Faster R-CNN\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2015\n",
    "* **Authors:** Shaoqing Ren, Kaiming He, Ross B. Girshick, Jian Sun\n",
    "* **Key Innovations:**\n",
    "  * Introduced a Region Proposal Network (RPN) that shares convolutional features with the detection network, significantly speeding up the proposal generation process.\n",
    "  * Eliminated the need for external region proposal algorithms, making the model fully end-to-end trainable.\n",
    "  * Improved detection accuracy by allowing the RPN to generate high-quality region proposals for the object detection task.\n",
    "  * Utilized anchor boxes for better localization of objects in various shapes and sizes.\n",
    "  * Achieved state-of-the-art results on the PASCAL VOC and COCO datasets while improving the overall processing speed.\n",
    "* **Sequel to:** Builds upon the Fast R-CNN model and has inspired subsequent architectures like Mask R-CNN.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type              | Count | Parameters |\n",
    "|-------------------------|-------|------------|\n",
    "| Convolutional Layers    | 5     | 2,500,000  |\n",
    "| Region Proposal Network  | 1     | 2,200,000  |\n",
    "| ROI Pooling Layer       | 1     | 0          |\n",
    "| Total Layers            | 7     | 4,700,000  |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Hardware Used              | FLOPs          | Computation Time | Hours Taken |\n",
    "|----------------------------|----------------|------------------|-------------|\n",
    "| 2 NVIDIA K80 GPUs          | ~10 billion    | 1 day            | ~24 hours   |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $224 \\times 224 \\times 3$\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows this formula:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer               | Input Size               | Operation                         | Output Size               | Parameters |\n",
    "|---------------------|--------------------------|-----------------------------------|---------------------------|------------|\n",
    "| Conv1               | $224 \\times 224 \\times 3$| Conv 7×7, stride 2                | $112 \\times 112 \\times 64$| 9,408      |\n",
    "| Pool1               | $112 \\times 112 \\times 64$| MaxPool 3×3, stride 2            | $56 \\times 56 \\times 64$  | 0          |\n",
    "| Conv2               | $56 \\times 56 \\times 64$ | Conv 3×3, stride 1                | $56 \\times 56 \\times 192$ | 110,080    |\n",
    "| Pool2               | $56 \\times 56 \\times 192$| MaxPool 3×3, stride 2            | $28 \\times 28 \\times 192$ | 0          |\n",
    "| Conv3               | $28 \\times 28 \\times 192$| Conv 1×1, stride 1                | $28 \\times 28 \\times 128$ | 24,576     |\n",
    "| Conv4               | $28 \\times 28 \\times 128$| Conv 1×1, stride 1                | $28 \\times 28 \\times 128$ | 16,384     |\n",
    "| Conv5               | $28 \\times 28 \\times 128$| Conv 1×1, stride 1                | $28 \\times 28 \\times 64$  | 8,192      |\n",
    "| RPN                  | $28 \\times 28 \\times 64$ | Region Proposal Network           | $28 \\times 28 \\times 256$ | 2,200,000  |\n",
    "| ROI Pooling          | Varies (based on ROIs)  | ROI Pooling                       | Varies (based on ROIs)    | 0          |\n",
    "\n",
    "### Parameter Calculation Example\n",
    "For Conv1 layer:\n",
    "* Weights: $7 \\times 7 \\times 3 \\times 64 = 9,408$\n",
    "* Biases: $64$\n",
    "* Total: $9,408 + 64 = 9,472$ parameters\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for Faster R-CNN: Approximately 4.7 million.\n",
    "\n",
    "This format provides a comprehensive breakdown of the Faster R-CNN architecture while keeping the structure clear and organized. If you need further modifications or additional details, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Summary Table for YOLO Versions\n",
    "\n",
    "| Version      | Year | Key Innovations                                             | Parameters       |\n",
    "|--------------|------|-----------------------------------------------------------|-------------------|\n",
    "| YOLOv1      | 2016 | Single-stage detector, grid-based bounding box prediction  | 7,055,221         |\n",
    "| YOLOv2      | 2017 | Improved mAP, multi-scale predictions, batch normalization | 19,500,000        |\n",
    "| YOLOv3      | 2018 | Multi-label classification, feature pyramid networks      | 61,000,000        |\n",
    "| YOLOv4      | 2020 | CSPNet, PANet, self-adversarial training                  | 64,000,000        |\n",
    "| YOLOv5      | 2020 | PyTorch implementation, multiple model sizes              | 7M - 46M (varies) |\n",
    "| YOLOv6      | 2022 | Enhanced speed and accuracy for real-time applications     | 12M - 40M (varies) |\n",
    "| YOLOv7      | 2022 | Improved performance on benchmarks, new training methods   | 8M - 60M (varies) |\n",
    "\n",
    "### Computational Power Table for YOLO Versions\n",
    "\n",
    "| Version      | Hardware Used            | FLOPs          | Computation Time | Hours Taken |\n",
    "|--------------|--------------------------|----------------|------------------|-------------|\n",
    "| YOLOv1      | 1 NVIDIA Titan X        | ~40 billion    | 3 days           | ~72 hours   |\n",
    "| YOLOv2      | 2 NVIDIA Titan X        | ~10 billion    | 2 days           | ~48 hours   |\n",
    "| YOLOv3      | 2 NVIDIA RTX 2080 Ti    | ~66 billion    | 1 day            | ~24 hours   |\n",
    "| YOLOv4      | 2 NVIDIA A100 GPUs       | ~57 billion    | 1 day            | ~24 hours   |\n",
    "| YOLOv5      | 2 NVIDIA V100 GPUs       | ~18 billion    | 1 day            | ~24 hours   |\n",
    "| YOLOv6      | 2 NVIDIA A100 GPUs       | ~12 billion    | 1 day            | ~24 hours   |\n",
    "| YOLOv7      | 2 NVIDIA A100 GPUs       | ~15 billion    | 1 day            | ~24 hours   |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# YOLO (You Only Look Once) Architectures\n",
    "\n",
    "## YOLOv1\n",
    "\n",
    "### Overview\n",
    "* **Year:** 2016\n",
    "* **Authors:** Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\n",
    "* **Key Innovations:**\n",
    "  * Real-time object detection at 45 FPS.\n",
    "  * Single neural network architecture for both localization and classification.\n",
    "  * Divided the image into an $S \\times S$ grid and predicted bounding boxes and class probabilities directly.\n",
    "\n",
    "### Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $448 \\times 448 \\times 3$\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows the formula:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer               | Input Size               | Operation                         | Output Size               | Parameters |\n",
    "|---------------------|--------------------------|-----------------------------------|---------------------------|------------|\n",
    "| Conv1               | $448 \\times 448 \\times 3$| Conv 7×7, stride 2                | $224 \\times 224 \\times 64$| 9,472      |\n",
    "| Conv2               | $224 \\times 224 \\times 64$| Conv 3×3, stride 1               | $224 \\times 224 \\times 192$| 110,464    |\n",
    "| Conv3               | $224 \\times 224 \\times 192$| Conv 1×1, stride 1              | $224 \\times 224 \\times 128$| 24,576     |\n",
    "| Conv4               | $224 \\times 224 \\times 128$| Conv 3×3, stride 1              | $224 \\times 224 \\times 256$| 295,168    |\n",
    "| Conv5               | $224 \\times 224 \\times 256$| Conv 1×1, stride 1              | $224 \\times 224 \\times 256$| 65,536     |\n",
    "| Conv6               | $224 \\times 224 \\times 256$| Conv 3×3, stride 1              | $224 \\times 224 \\times 512$| 1,180,160  |\n",
    "| Fully Connected 1   | $7 \\times 7 \\times 512$  | FC Layer                          | 4096                      | 1,049,088  |\n",
    "| Fully Connected 2   | 4096                     | FC Layer                          | 1470 (bounding boxes)     | 6,053,100  |\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for YOLOv1: Approximately 7 million.\n",
    "\n",
    "## YOLOv2\n",
    "\n",
    "### Overview\n",
    "* **Year:** 2017\n",
    "* **Authors:** Joseph Redmon, Ali Farhadi\n",
    "* **Key Innovations:**\n",
    "  * Introduced multi-scale training for improved performance.\n",
    "  * Added anchor boxes for better localization.\n",
    "  * Integrated batch normalization for faster convergence.\n",
    "\n",
    "### Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $416 \\times 416 \\times 3$\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer               | Input Size               | Operation                         | Output Size               | Parameters |\n",
    "|---------------------|--------------------------|-----------------------------------|---------------------------|------------|\n",
    "| Conv1               | $416 \\times 416 \\times 3$| Conv 16×16, stride 1              | $416 \\times 416 \\times 16$| 432        |\n",
    "| Conv2               | $416 \\times 416 \\times 16$| Conv 3×3, stride 1               | $416 \\times 416 \\times 32$| 4,640      |\n",
    "| Conv3               | $416 \\times 416 \\times 32$| Conv 3×3, stride 1               | $416 \\times 416 \\times 64$| 18,496     |\n",
    "| Conv4               | $416 \\times 416 \\times 64$| Conv 3×3, stride 1               | $416 \\times 416 \\times 128$| 73,728     |\n",
    "| Conv5               | $416 \\times 416 \\times 128$| Conv 3×3, stride 1              | $416 \\times 416 \\times 256$| 295,168    |\n",
    "| Conv6               | $416 \\times 416 \\times 256$| Conv 3×3, stride 1              | $416 \\times 416 \\times 512$| 1,180,160  |\n",
    "| Fully Connected     | $13 \\times 13 \\times 1024$| FC Layer                          | 1470 (bounding boxes)     | 3,655,700  |\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for YOLOv2: Approximately 19 million.\n",
    "\n",
    "## YOLOv3\n",
    "\n",
    "### Overview\n",
    "* **Year:** 2018\n",
    "* **Authors:** Joseph Redmon, Ali Farhadi\n",
    "* **Key Innovations:**\n",
    "  * Introduced a feature pyramid network to detect objects at different scales.\n",
    "  * Improved the detection head with multiple scales.\n",
    "  * Employed logistic regression for objectness score prediction.\n",
    "\n",
    "### Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $608 \\times 608 \\times 3$\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer               | Input Size               | Operation                         | Output Size               | Parameters |\n",
    "|---------------------|--------------------------|-----------------------------------|---------------------------|------------|\n",
    "| Conv1               | $608 \\times 608 \\times 3$| Conv 3×3, stride 1                | $606 \\times 606 \\times 32$| 896        |\n",
    "| Conv2               | $606 \\times 606 \\times 32$| Conv 3×3, stride 1               | $604 \\times 604 \\times 64$| 18,496     |\n",
    "| Conv3               | $604 \\times 604 \\times 64$| Conv 3×3, stride 1               | $602 \\times 602 \\times 128$| 73,728     |\n",
    "| Conv4               | $602 \\times 602 \\times 128$| Conv 3×3, stride 1              | $600 \\times 600 \\times 256$| 295,168    |\n",
    "| Conv5               | $600 \\times 600 \\times 256$| Conv 3×3, stride 1              | $598 \\times 598 \\times 512$| 1,180,160  |\n",
    "| Conv6               | $598 \\times 598 \\times 512$| Conv 3×3, stride 1              | $596 \\times 596 \\times 1024$| 4,719,360  |\n",
    "| Fully Connected     | $19 \\times 19 \\times 1024$| FC Layer                          | 1470 (bounding boxes)     | 3,655,700  |\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for YOLOv3: Approximately 63 million.\n",
    "\n",
    "## Summary of Improvements\n",
    "- **YOLOv4 (2020):**\n",
    "  - Integrated CSPNet architecture for enhanced accuracy.\n",
    "  - Introduced data augmentation techniques like Mosaic.\n",
    "  \n",
    "- **YOLOv5 (2020):**\n",
    "  - Focused on usability with a PyTorch implementation.\n",
    "  - Offered different model sizes for scalability.\n",
    "\n",
    "- **YOLOv6 (2022):**\n",
    "  - Improved architecture for inference speed and accuracy.\n",
    "  \n",
    "- **YOLOv7 (2022):**\n",
    "  - Introduced dynamic label assignment and achieved state-of-the-art performance on benchmarks.\n",
    "\n",
    "Feel free to ask for any specific details or further modifications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the detailed architecture breakdown for Detectron 2:\n",
    "\n",
    "# Detectron 2 Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2019\n",
    "* **Authors:** Facebook AI Research (FAIR)\n",
    "* **Key Innovations:**\n",
    "  * Built on the original Detectron framework with improved modularity and flexibility.\n",
    "  * Incorporates advanced deep learning techniques such as Mask R-CNN for instance segmentation.\n",
    "  * Utilizes the PyTorch framework for easier model customization and training.\n",
    "  * Supports a wide range of architectures (e.g., ResNet, FPN) and backbones.\n",
    "  * Provides strong baseline models for object detection and segmentation tasks.\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** Variable (Commonly $800 \\times 1333$)\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows the formula:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer               | Input Size               | Operation                         | Output Size               | Parameters |\n",
    "|---------------------|--------------------------|-----------------------------------|---------------------------|------------|\n",
    "| Backbone (ResNet)   | Variable                  | ResNet with FPN                   | Variable                  | Variable   |\n",
    "| RPN (Region Proposal Network) | Variable      | Conv 3×3, stride 1               | Variable                  | Variable   |\n",
    "| ROI Align           | Variable                  | ROI Align Layer                   | Variable                  | Variable   |\n",
    "| Mask Branch         | Variable                  | Conv 3×3, stride 1                | Variable                  | Variable   |\n",
    "| Box Branch          | Variable                  | Fully Connected Layer             | Variable                  | Variable   |\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for Detectron 2: Approximately 50 million (depends on the backbone).\n",
    "\n",
    "## Computational Power\n",
    "\n",
    "### Computation Time\n",
    "| Hardware Used          | Days | FLOPs          |\n",
    "|------------------------|------|----------------|\n",
    "| NVIDIA V100 GPU       | 2    | 300 GFLOPs     |\n",
    "| NVIDIA A100 GPU       | 1    | 450 GFLOPs     |\n",
    "\n",
    "### Key Improvements and Features\n",
    "- **Modularity:** Components are easily interchangeable, allowing for rapid experimentation.\n",
    "- **Multi-task Learning:** Supports detection, instance segmentation, and keypoint detection in a unified framework.\n",
    "- **Performance:** Achieves state-of-the-art results on COCO dataset benchmarks.\n",
    "\n",
    "Detectron 2's flexibility and robust architecture make it suitable for a variety of object detection and segmentation tasks. Let me know if you need more details or any specific aspects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "# DETR (DEtection TRansformer)\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2020\n",
    "* **Authors:** Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Antoine Demeester, Sergey S. Kolesnikov\n",
    "* **Key Innovations:**\n",
    "  * Introduced a transformer-based architecture for object detection, treating it as a direct set prediction problem.\n",
    "  * Utilizes attention mechanisms to capture long-range dependencies in images, improving the handling of complex scenes.\n",
    "  * Eliminates the need for anchor boxes, simplifying the model architecture and reducing hyperparameter tuning.\n",
    "  * Incorporates a bipartite matching loss to match predicted bounding boxes with ground truth.\n",
    "  * Achieves state-of-the-art performance on the COCO dataset while simplifying the object detection pipeline.\n",
    "* **Sequel to:** Introduces a new paradigm for object detection, diverging from traditional CNN-based approaches.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type              | Count | Parameters |\n",
    "|-------------------------|-------|------------|\n",
    "| Convolutional Layers    | 5     | 7,300,000  |\n",
    "| Transformer Encoder      | 6     | 35,000,000 |\n",
    "| Transformer Decoder      | 6     | 25,000,000 |\n",
    "| Total Layers            | 17    | 67,300,000 |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Hardware Used              | FLOPs          | Computation Time | Hours Taken |\n",
    "|----------------------------|----------------|------------------|-------------|\n",
    "| 4 NVIDIA V100 GPUs         | ~22 billion    | 1 day            | ~24 hours   |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $800 \\times 800 \\times 3$\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows this formula:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer               | Input Size               | Operation                         | Output Size               | Parameters |\n",
    "|---------------------|--------------------------|-----------------------------------|---------------------------|------------|\n",
    "| Conv1               | $800 \\times 800 \\times 3$| Conv 3×3, stride 2                | $400 \\times 400 \\times 64$| 1,792      |\n",
    "| Conv2               | $400 \\times 400 \\times 64$| Conv 3×3, stride 2               | $200 \\times 200 \\times 128$| 73,728    |\n",
    "| Conv3               | $200 \\times 200 \\times 128$| Conv 3×3, stride 2              | $100 \\times 100 \\times 256$| 294,912   |\n",
    "| Conv4               | $100 \\times 100 \\times 256$| Conv 3×3, stride 2              | $50 \\times 50 \\times 512$ | 1,179,648  |\n",
    "| Conv5               | $50 \\times 50 \\times 512$| Conv 3×3, stride 2                | $25 \\times 25 \\times 512$ | 2,359,296  |\n",
    "| Transformer Encoder  | Varies                  | Transformer Encoding               | Varies                    | 35,000,000 |\n",
    "| Transformer Decoder  | Varies                  | Transformer Decoding               | Varies                    | 25,000,000 |\n",
    "\n",
    "### Parameter Calculation Example\n",
    "For Conv1 layer:\n",
    "* Weights: $3 \\times 3 \\times 3 \\times 64 = 1,792$\n",
    "* Biases: $64$\n",
    "* Total: $1,792 + 64 = 1,856$ parameters\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for DETR: Approximately 67.3 million.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the Swin Transformer architecture breakdown with the updated summary and computational power tables formatted as you specified:\n",
    "\n",
    "# Swin Transformer Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2021\n",
    "* **Authors:** Ze Liu, Yutong Lin, Yue Cao, et al.\n",
    "* **Key Innovations:**\n",
    "  * Introduces a hierarchical structure with shifted windows for local and global attention mechanisms.\n",
    "  * Achieves state-of-the-art performance on various vision tasks, including image classification, object detection, and semantic segmentation.\n",
    "  * Combines the advantages of convolutional neural networks (CNNs) and transformers for improved efficiency and flexibility.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type           | Count | Parameters |\n",
    "|----------------------|-------|------------|\n",
    "| Patch Embedding      | 1     | 1,152      |\n",
    "| Swin Transformer Block| 4     | 1,568,512  |\n",
    "| Swin Transformer Block| 4     | 4,474,752  |\n",
    "| Swin Transformer Block| 6     | 10,348,032 |\n",
    "| Swin Transformer Block| 3     | 23,592,960 |\n",
    "| MLP Head             | 1     | 769,792    |\n",
    "| **Total**            | **N/A**| **~88 million** |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Hardware Used              | FLOPs          | Computation Time | Hours Taken |\n",
    "|----------------------------|----------------|------------------|-------------|\n",
    "| 2 NVIDIA A100 GPUs         | ~4 billion     | 2 days           | ~48 hours   |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $224 \\times 224 \\times 3$\n",
    "* **Type:** RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows the formula:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer                     | Input Size               | Operation                                 | Output Size               | Parameters |\n",
    "|---------------------------|--------------------------|-------------------------------------------|---------------------------|------------|\n",
    "| Patch Embedding           | $224 \\times 224 \\times 3$| Linear Projection (patch size $4 \\times 4$)| $56 \\times 56 \\times 96$  | 1,152      |\n",
    "| Stage 1                   | $56 \\times 56 \\times 96$ | Swin Transformer Block (4 layers)       | $56 \\times 56 \\times 96$  | 1,568,512  |\n",
    "| Stage 2                   | $56 \\times 56 \\times 96$ | Swin Transformer Block (4 layers)       | $28 \\times 28 \\times 192$ | 4,474,752  |\n",
    "| Stage 3                   | $28 \\times 28 \\times 192$| Swin Transformer Block (6 layers)       | $14 \\times 14 \\times 384$ | 10,348,032 |\n",
    "| Stage 4                   | $14 \\times 14 \\times 384$| Swin Transformer Block (3 layers)       | $7 \\times 7 \\times 768$   | 23,592,960 |\n",
    "| Class Token               | $7 \\times 7 \\times 768$  | Class Token Projection                    | $1 \\times 768$            | 768        |\n",
    "| Final Layer               | $7 \\times 7 \\times 768$  | MLP Head (Classification)                | 1000 (classes)            | 769,792    |\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for Swin Transformer: Approximately 88 million.\n",
    "\n",
    "## Updates in Subsequent Versions\n",
    "\n",
    "### Swin Transformer V2 (2022)\n",
    "* **Key Improvements:**\n",
    "  - Enhanced training efficiency and robustness to various data distributions.\n",
    "  - Improved performance on downstream tasks with fewer parameters.\n",
    "  - Introduced a new patch merging strategy to reduce computational cost.\n",
    "\n",
    "### Swin Transformer V2.0 (2023)\n",
    "* **Key Improvements:**\n",
    "  - Extended capabilities for dense prediction tasks like segmentation and detection.\n",
    "  - Introduced a new approach for incorporating multi-scale features.\n",
    "  - Increased flexibility in adapting to different input sizes and aspect ratios.\n",
    "\n",
    "### General Updates\n",
    "- **Enhanced Performance:** Continuous updates in model training methodologies have led to better accuracy on benchmarks like ImageNet and COCO.\n",
    "- **Real-time Applications:** Adaptations for real-time applications in mobile and edge devices, maintaining efficiency while improving speed.\n",
    "- **Broader Adoption:** Gained popularity in various vision tasks, solidifying its place among state-of-the-art architectures.\n",
    "\n",
    "Let me know if you need further modifications or additional details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:red\"> changed to Generative models of images and videos <span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/rtx03_iC46U?si=3BpD7b0saxR4c1Yb|\n",
    "\n",
    "Yes, many contemporary generative models, particularly in the realm of image synthesis, can trace their conceptual roots back to probabilistic frameworks, including **probabilistic graphical models (PGMs)**. Here’s a breakdown of how this foundation influences various generative models:\n",
    "\n",
    "### Foundation of Generative Models\n",
    "\n",
    "1. **Probabilistic Graphical Models (PGMs)**:\n",
    "   - PGMs, which include Bayesian networks and Markov random fields, provide a structured way to represent the joint distribution of random variables. \n",
    "   - They allow for the modeling of complex relationships and dependencies between variables, making them a powerful framework for generative tasks.\n",
    "\n",
    "2. **Generative Models**:\n",
    "   - Generative models aim to learn the underlying distribution of data to generate new samples that resemble the training data. This can include images, text, or other types of data.\n",
    "   - Examples include:\n",
    "     - **Variational Autoencoders (VAEs)**: VAEs use latent variables and encode input data into a lower-dimensional space, from which they can sample to generate new data. The latent space is structured to approximate the data distribution, relying heavily on the principles of PGMs.\n",
    "     - **Generative Adversarial Networks (GANs)**: While GANs do not explicitly use PGMs, their adversarial training framework indirectly relates to probabilistic modeling, as the generator and discriminator learn to capture the data distribution through competitive optimization.\n",
    "     - **Diffusion Models**: These models, like Denoising Diffusion Probabilistic Models (DDPM), explicitly incorporate probabilistic frameworks to model data generation through a diffusion process. They can be viewed as a form of PGM that progressively transforms noise into data.\n",
    "\n",
    "3. **Evolution and Impact**:\n",
    "   - The understanding gained from PGMs has influenced how newer models are structured and trained, emphasizing the importance of distributional representations and stochastic processes.\n",
    "   - Diffusion models, in particular, have drawn inspiration from the idea of incrementally refining noise into structured data, akin to how PGMs might represent gradual transformations between states.\n",
    "\n",
    "Here’s the refined version focusing on the key connections between **Probabilistic Graphical Models (PGMs)** and **Generative AI**, including paper links:\n",
    "\n",
    "### Key Connections Between PGMs and Generative AI\n",
    "\n",
    "1. **Variational Autoencoders (VAEs)**:\n",
    "   - **Connection**: VAEs combine neural networks with PGMs. They use an encoder to approximate the posterior distribution of latent variables and a decoder to generate data.\n",
    "   - **Paper**: [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) by D. P. Kingma and M. Welling (2013) introduced this concept, showing how PGMs could be integrated with deep learning to create generative models.\n",
    "\n",
    "2. **Generative Adversarial Networks (GANs)**:\n",
    "   - **Connection**: While GANs don't explicitly use PGMs, the underlying principles of probabilistic reasoning and distribution modeling are present. The generator and discriminator can be thought of as two competing distributions, where the generator aims to produce samples from a target distribution.\n",
    "   - **Paper**: [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661) by I. Goodfellow et al. (2014) established GANs as a powerful generative modeling framework.\n",
    "\n",
    "3. **Diffusion Models**:\n",
    "   - **Connection**: Diffusion models can be seen as a probabilistic framework where data is generated through a process of gradual denoising. They employ a forward process (adding noise) and a reverse process (denoising) that can be analyzed through PGMs.\n",
    "   - **Paper**: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) by J. Ho et al. (2020) demonstrated this approach, showing its effectiveness for image generation.\n",
    "\n",
    "4. **Bayesian Neural Networks**:\n",
    "   - **Connection**: These models integrate PGMs into neural networks by treating weights as distributions rather than fixed values, allowing for uncertainty quantification in predictions.\n",
    "   - **Paper**: [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/abs/1506.02142) by Y. Gal and Z. Ghahramani (2016) explored this concept.\n",
    "\n",
    "These connections highlight how the principles of PGMs have influenced the development of generative models, creating a rich interplay between probabilistic reasoning and deep learning techniques. If you need further details or specific aspects, feel free to ask!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAEs and CNNs:\n",
    " VAEs utilized CNN architectures to process and generate images, building on the advancements in deep learning during the early 2010s.\n",
    "Competition with GANs: The introduction of GANs influenced the development of VAEs, as researchers explored ways to improve generative models.\n",
    "### Diffusion Models :\n",
    " as a New Paradigm: The introduction of diffusion models represented a shift in generative modeling, drawing from both VAEs and GANs but employing a different methodology focused on noise and denoising processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the architecture breakdown for Variational Autoencoders (VAEs), including a summary and computational power tables.\n",
    "\n",
    "# Variational Autoencoder (VAE) Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year**: 2013\n",
    "* **Authors**: D. P. Kingma, M. Welling\n",
    "* **Key Innovations**:\n",
    "  - Introduced the concept of variational inference in latent variable models.\n",
    "  - Combined neural networks with probabilistic modeling.\n",
    "  - Utilized reparameterization trick for efficient gradient descent.\n",
    "  - Enabled the generation of new data points from learned distributions.\n",
    "  - Allowed for efficient training of complex generative models.\n",
    "\n",
    "* **Sequel to**: Traditional Autoencoders\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type              | Count | Parameters  |\n",
    "|-------------------------|-------|-------------|\n",
    "| Encoder (Convolutional) | 4     | 1,200,000   |\n",
    "| Decoder (Convolutional) | 4     | 1,200,000   |\n",
    "| Total Layers            | 8     | 2,400,000   |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Hardware Used              | FLOPs         | Computation Time | Hours Taken |\n",
    "|----------------------------|---------------|------------------|-------------|\n",
    "| 1 NVIDIA GTX 1080 Ti       | ~10 billion   | 1 day            | ~24 hours   |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size**: $64 \\times 64 \\times 3$\n",
    "* **Type**: RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows this simple formula:\n",
    "$$\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer | Input Size         | Operation                | Output Size          | Parameters   |\n",
    "|-------|--------------------|-------------------------|----------------------|--------------|\n",
    "| Conv1 | $64 \\times 64 \\times 3$  | Conv 3×3, stride 2    | $32 \\times 32 \\times 32$ | 896          |\n",
    "| Conv2 | $32 \\times 32 \\times 32$ | Conv 3×3, stride 2    | $16 \\times 16 \\times 64$ | 18,496       |\n",
    "| Conv3 | $16 \\times 16 \\times 64$ | Conv 3×3, stride 2    | $8 \\times 8 \\times 128$   | 73,856       |\n",
    "| Conv4 | $8 \\times 8 \\times 128$   | Conv 3×3, stride 2    | $4 \\times 4 \\times 256$   | 295,168      |\n",
    "| Flatten | $4 \\times 4 \\times 256$ | Flatten                | $4096$               | 0            |\n",
    "| Dense (Mean) | $4096$             | Dense                  | $128$                | 524,288      |\n",
    "| Dense (Log Variance) | $4096$       | Dense                  | $128$                | 524,288      |\n",
    "| Dense (Latent) | $128$             | Dense                  | $2$                  | 258          |\n",
    "| Dense (Latent Decoder) | $2$        | Dense                  | $128$                | 384          |\n",
    "| Dense (Decoder 1) | $128$          | Dense                  | $4096$               | 528,384      |\n",
    "| Reshape | $4096$             | Reshape                | $4 \\times 4 \\times 256$   | 0            |\n",
    "| ConvTranspose1 | $4 \\times 4 \\times 256$ | ConvTranspose 3×3, stride 2 | $8 \\times 8 \\times 128$   | 295,168      |\n",
    "| ConvTranspose2 | $8 \\times 8 \\times 128$ | ConvTranspose 3×3, stride 2 | $16 \\times 16 \\times 64$   | 73,856       |\n",
    "| ConvTranspose3 | $16 \\times 16 \\times 64$ | ConvTranspose 3×3, stride 2 | $32 \\times 32 \\times 32$ | 18,496       |\n",
    "| ConvTranspose4 | $32 \\times 32 \\times 32$ | ConvTranspose 3×3, stride 2 | $64 \\times 64 \\times 3$   | 896          |\n",
    "| **Total** | -                  | -                       | -                    | **2,400,000** |\n",
    "\n",
    "### Summary of VAE Architecture\n",
    "VAEs leverage convolutional layers to encode input images into a latent space while maintaining probabilistic characteristics. The decoder then reconstructs images from this latent representation, making use of the learned distributions for effective image generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "# Generative Adversarial Networks (GANs) Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2014\n",
    "* **Authors:** Ian Goodfellow et al.\n",
    "* **Key Innovations:**\n",
    "  * Introduced the concept of adversarial training using two neural networks: a generator and a discriminator.\n",
    "  * The generator creates fake data, while the discriminator attempts to distinguish between real and fake data.\n",
    "  * Pioneered the use of GANs for various applications, including image generation, video generation, and more.\n",
    "\n",
    "### Summary Table of Innovations\n",
    "\n",
    "| Model       | Year | Key Innovations                                        | Parameters       |\n",
    "|-------------|------|------------------------------------------------------|-------------------|\n",
    "| GAN         | 2014 | Basic architecture with generator and discriminator   | Varies            |\n",
    "| CycleGAN    | 2017 | Cycle consistency loss for unpaired image-to-image translation | Varies       |\n",
    "| BigGAN      | 2018 | Large batch training, class-conditional generation    | 350 million       |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Model       | Hardware Used             | FLOPs          | Computation Time | Hours Taken |\n",
    "|-------------|---------------------------|----------------|------------------|-------------|\n",
    "| GAN         | 1 NVIDIA Titan X         | ~100 million   | 1 day            | ~24 hours   |\n",
    "| CycleGAN    | 2 NVIDIA Titan X         | ~50 billion    | 2 days           | ~48 hours   |\n",
    "| BigGAN      | 4 NVIDIA V100 GPUs       | ~200 billion   | 3 days           | ~72 hours   |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### GAN Architecture Overview\n",
    "\n",
    "1. **Generator Network**\n",
    "   - **Input:** Random noise vector $z$ (e.g., sampled from a uniform or normal distribution).\n",
    "   - **Output:** Fake data (e.g., generated images).\n",
    "   - **Structure:** Typically uses transposed convolutional layers to upsample the input noise into high-dimensional data.\n",
    "\n",
    "2. **Discriminator Network**\n",
    "   - **Input:** Real or fake data (e.g., images).\n",
    "   - **Output:** Probability of input being real (1) or fake (0).\n",
    "   - **Structure:** Usually consists of convolutional layers that downsample the input data.\n",
    "\n",
    "### Layer Calculations\n",
    "Both the generator and discriminator can have their layers structured similarly to CNNs, with layers that include:\n",
    "\n",
    "* Convolutional layers (Conv)\n",
    "* Batch normalization layers (BatchNorm)\n",
    "* Activation functions (e.g., ReLU, Leaky ReLU)\n",
    "* Fully connected layers (FC)\n",
    "\n",
    "### Layer by Layer Breakdown (Example for Generator)\n",
    "\n",
    "| Layer       | Input Size        | Operation                    | Output Size        | Parameters  |\n",
    "|-------------|-------------------|------------------------------|---------------------|-------------|\n",
    "| Dense       | $z$ (noise vector)| Fully Connected               | $N \\times 128$      | 1,024       |\n",
    "| Reshape     | $N \\times 128$    | Reshape to (N, 8, 8, 2)      | $N \\times 8 \\times 8 \\times 2$ | 0           |\n",
    "| ConvTranspose| $N \\times 2 \\times 8 \\times 8$| Transposed Conv (4x4, stride 2)| $N \\times 2 \\times 16 \\times 16$| 128         |\n",
    "| ConvTranspose| $N \\times 2 \\times 16 \\times 16$| Transposed Conv (4x4, stride 2)| $N \\times 2 \\times 32 \\times 32$| 512         |\n",
    "| Conv        | $N \\times 2 \\times 32 \\times 32$| Conv (3x3)| $N \\times 3 \\times 32 \\times 32$| 576         |\n",
    "\n",
    "### Total Parameters for GAN\n",
    "* Total parameters for a simple GAN architecture can vary, typically ranging from a few thousand to several million depending on the architecture depth and complexity.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a structured overview of the Pix2Pix architecture, including its details, key innovations, summary table, and computational power table.\n",
    "\n",
    "# Pix2Pix Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2016\n",
    "* **Authors:** Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros\n",
    "* **Key Innovations:**\n",
    "  * Introduced a conditional GAN (cGAN) framework that enables paired image-to-image translation.\n",
    "  * Utilizes a generator that translates input images to output images, while a discriminator assesses the realism of the generated images in relation to the input images.\n",
    "  * The use of a loss function that combines adversarial loss and L1 loss to enhance the quality of generated images.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type              | Count | Parameters       |\n",
    "|-------------------------|-------|------------------|\n",
    "| Convolutional Layers    | 5     | 7,350,000        |\n",
    "| Transposed Convolutional Layers | 5 | 3,500,000      |\n",
    "| Total Layers            | 10    | 10,850,000       |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Hardware Used              | FLOPs          | Computation Time | Hours Taken |\n",
    "|----------------------------|----------------|------------------|-------------|\n",
    "| 1 NVIDIA Titan X           | ~30 billion    | 1 day            | ~24 hours   |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size:** $256 \\times 256 \\times 3$\n",
    "* **Type:** Paired images (input and target)\n",
    "\n",
    "### Layer Calculations\n",
    "The Pix2Pix model uses convolutional layers and transposed convolutional layers to encode and decode images, respectively. The basic operation for each convolution layer is given by:\n",
    "$$\n",
    "\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1\n",
    "$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer                | Input Size                  | Operation                     | Output Size                  | Parameters  |\n",
    "|----------------------|-----------------------------|-------------------------------|------------------------------|-------------|\n",
    "| Conv2D_1             | $256 \\times 256 \\times 3$   | Conv (4x4, stride 2)         | $128 \\times 128 \\times 64$  | 3,136       |\n",
    "| Conv2D_2             | $128 \\times 128 \\times 64$ | Conv (4x4, stride 2)         | $64 \\times 64 \\times 128$   | 131,200     |\n",
    "| Conv2D_3             | $64 \\times 64 \\times 128$  | Conv (4x4, stride 2)         | $32 \\times 32 \\times 256$   | 524,288     |\n",
    "| Conv2D_4             | $32 \\times 32 \\times 256$  | Conv (4x4, stride 2)         | $16 \\times 16 \\times 512$   | 1,048,576   |\n",
    "| Conv2D_5             | $16 \\times 16 \\times 512$  | Conv (4x4, stride 2)         | $8 \\times 8 \\times 512$     | 2,097,152   |\n",
    "| ConvTranspose_1       | $8 \\times 8 \\times 512$     | Transposed Conv (4x4, stride 2)| $16 \\times 16 \\times 256$   | 2,097,152   |\n",
    "| ConvTranspose_2       | $16 \\times 16 \\times 256$   | Transposed Conv (4x4, stride 2)| $32 \\times 32 \\times 128$   | 524,288     |\n",
    "| ConvTranspose_3       | $32 \\times 32 \\times 128$   | Transposed Conv (4x4, stride 2)| $64 \\times 64 \\times 64$    | 131,200     |\n",
    "| ConvTranspose_4       | $64 \\times 64 \\times 64$    | Transposed Conv (4x4, stride 2)| $128 \\times 128 \\times 3$   | 3,136       |\n",
    "\n",
    "### Total Parameters\n",
    "* Total parameters for Pix2Pix: Approximately 10,850,000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Video GAN Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2016\n",
    "* **Authors:** T. Zhang et al.\n",
    "* **Key Innovations:**\n",
    "  * Extension of GANs to video generation by conditioning on past frames to generate future frames.\n",
    "  * Utilizes a spatio-temporal generative model to capture both spatial and temporal dependencies in video data.\n",
    "  \n",
    "### Architecture Details\n",
    "\n",
    "#### Input Layer\n",
    "* **Size:** $T \\times H \\times W \\times C$ (where $T$ is the number of frames, $H$ and $W$ are height and width, and $C$ is the number of channels)\n",
    "\n",
    "#### Layer by Layer Breakdown\n",
    "\n",
    "| Layer                | Input Size                  | Operation                     | Output Size                  | Parameters  |\n",
    "|----------------------|-----------------------------|-------------------------------|------------------------------|-------------|\n",
    "| Conv3D_1             | $T \\times H \\times W \\times C$ | Conv3D (4x4x4, stride 2)   | $T \\times \\frac{H}{2} \\times \\frac{W}{2} \\times 64$ | 3,200       |\n",
    "| Conv3D_2             | $T \\times \\frac{H}{2} \\times \\frac{W}{2} \\times 64$ | Conv3D (4x4x4, stride 2)   | $T \\times \\frac{H}{4} \\times \\frac{W}{4} \\times 128$ | 131,200     |\n",
    "| Conv3D_3             | $T \\times \\frac{H}{4} \\times \\frac{W}{4} \\times 128$ | Conv3D (4x4x4, stride 2)   | $T \\times \\frac{H}{8} \\times \\frac{W}{8} \\times 256$ | 524,288     |\n",
    "| Conv3D_4             | $T \\times \\frac{H}{8} \\times \\frac{W}{8} \\times 256$ | Conv3D (4x4x4, stride 2)   | $T \\times \\frac{H}{16} \\times \\frac{W}{16} \\times 512$ | 2,097,152   |\n",
    "| ConvTranspose3D_1    | $T \\times \\frac{H}{16} \\times \\frac{W}{16} \\times 512$ | Transposed Conv3D (4x4x4, stride 2) | $T \\times \\frac{H}{8} \\times \\frac{W}{8} \\times 256$ | 2,097,152   |\n",
    "| ConvTranspose3D_2    | $T \\times \\frac{H}{8} \\times \\frac{W}{8} \\times 256$ | Transposed Conv3D (4x4x4, stride 2) | $T \\times \\frac{H}{4} \\times \\frac{W}{4} \\times 128$ | 524,288     |\n",
    "\n",
    "### Total Parameters for Video GAN\n",
    "* Approximately 5,000,000.\n",
    "\n",
    "---\n",
    "\n",
    "# MoCo GAN Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2020\n",
    "* **Authors:** X. Chen et al.\n",
    "* **Key Innovations:**\n",
    "  * Introduces a memory bank mechanism to improve the diversity and quality of generated images by leveraging previous latent codes.\n",
    "  * Enhances stability and performance in training GANs with limited data.\n",
    "\n",
    "### Architecture Details\n",
    "\n",
    "#### Input Layer\n",
    "* **Size:** $256 \\times 256 \\times 3$\n",
    "* **Type:** Random noise vector and memory bank samples\n",
    "\n",
    "#### Layer by Layer Breakdown\n",
    "\n",
    "| Layer                | Input Size                  | Operation                     | Output Size                  | Parameters  |\n",
    "|----------------------|-----------------------------|-------------------------------|------------------------------|-------------|\n",
    "| Conv_1               | $256 \\times 256 \\times 3$   | Conv (4x4, stride 2)         | $128 \\times 128 \\times 64$  | 3,136       |\n",
    "| Conv_2               | $128 \\times 128 \\times 64$ | Conv (4x4, stride 2)         | $64 \\times 64 \\times 128$   | 131,200     |\n",
    "| Conv_3               | $64 \\times 64 \\times 128$  | Conv (4x4, stride 2)         | $32 \\times 32 \\times 256$   | 524,288     |\n",
    "| Conv_4               | $32 \\times 32 \\times 256$  | Conv (4x4, stride 2)         | $16 \\times 16 \\times 512$   | 1,048,576   |\n",
    "| ConvTranspose_1      | $16 \\times 16 \\times 512$  | Transposed Conv (4x4, stride 2)| $32 \\times 32 \\times 256$   | 2,097,152   |\n",
    "| ConvTranspose_2      | $32 \\times 32 \\times 256$  | Transposed Conv (4x4, stride 2)| $64 \\times 64 \\times 128$    | 524,288     |\n",
    "\n",
    "### Total Parameters for MoCo GAN\n",
    "* Approximately 10,000,000.\n",
    "\n",
    "---\n",
    "\n",
    "# TGAN Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2017\n",
    "* **Authors:** X. Wang et al.\n",
    "* **Key Innovations:**\n",
    "  * Focuses on generating video sequences from random noise, with a focus on temporal coherence across generated frames.\n",
    "  * Utilizes recurrent structures to capture temporal dependencies in videos.\n",
    "\n",
    "### Architecture Details\n",
    "\n",
    "#### Input Layer\n",
    "* **Size:** $T \\times 64 \\times 64 \\times 3$\n",
    "* **Type:** Random noise vector for T frames\n",
    "\n",
    "#### Layer by Layer Breakdown\n",
    "\n",
    "| Layer                | Input Size                  | Operation                     | Output Size                  | Parameters  |\n",
    "|----------------------|-----------------------------|-------------------------------|------------------------------|-------------|\n",
    "| Conv3D_1             | $T \\times 64 \\times 64 \\times 3$ | Conv3D (4x4x4, stride 2)   | $T \\times 32 \\times 32 \\times 64$ | 3,200       |\n",
    "| Conv3D_2             | $T \\times 32 \\times 32 \\times 64$ | Conv3D (4x4x4, stride 2)   | $T \\times 16 \\times 16 \\times 128$ | 131,200     |\n",
    "| Conv3D_3             | $T \\times 16 \\times 16 \\times 128$ | Conv3D (4x4x4, stride 2)   | $T \\times 8 \\times 8 \\times 256$ | 524,288     |\n",
    "| ConvTranspose3D_1    | $T \\times 8 \\times 8 \\times 256$ | Transposed Conv3D (4x4x4, stride 2) | $T \\times 16 \\times 16 \\times 128$ | 1,048,576   |\n",
    "| ConvTranspose3D_2    | $T \\times 16 \\times 16 \\times 128$ | Transposed Conv3D (4x4x4, stride 2) | $T \\times 32 \\times 32 \\times 64$  | 262,144     |\n",
    "\n",
    "### Total Parameters for TGAN\n",
    "* Approximately 5,000,000.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Model      | Year | Key Innovations                                        | Parameters       |\n",
    "|------------|------|------------------------------------------------------|-------------------|\n",
    "| Video GAN  | 2016 | Spatio-temporal model for video generation           | ~5,000,000        |\n",
    "| MoCo GAN   | 2020 | Memory bank mechanism for diversity                   | ~10,000,000       |\n",
    "| TGAN       | 2017 | Recurrent structures for temporal coherence           | ~5,000,000        |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Model      | Hardware Used             | FLOPs          | Computation Time | Hours Taken |\n",
    "|------------|---------------------------|----------------|------------------|-------------|\n",
    "| Video GAN  | 2 NVIDIA K40 GPUs         | ~30 billion    | 2 days           | ~48 hours   |\n",
    "| MoCo GAN   | 1 NVIDIA V100 GPU         | ~50 billion    | 1 day            | ~24 hours   |\n",
    "| TGAN       | 2 NVIDIA Titan X          | ~25 billion    | 1 day            | ~24 hours   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evolution of diffusion models in generative modeling represents a significant shift in how images and other complex data are generated. Here’s a brief overview of the evolution of diffusion models, their connections to previous architectures, and why they have become so popular:\n",
    "\n",
    "### Early Development of Diffusion Models\n",
    "\n",
    "1. **Origins**:\n",
    "   - Diffusion models can trace their origins to ideas in physics and thermodynamics, specifically the concept of diffusion processes, which describe how particles spread over time. This concept was later adapted into a probabilistic framework for generating data.\n",
    "   - Early work in generative models primarily focused on Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). These models laid the groundwork for generative tasks but faced challenges related to mode collapse and lack of diversity in generated samples.\n",
    "\n",
    "2. **Connection to VAEs**:\n",
    "   - Variational Autoencoders (introduced in 2013) became popular for their ability to model complex distributions and generate new samples. However, they had limitations in capturing high-quality details in generated images.\n",
    "   - Researchers began exploring diffusion processes as a way to improve upon VAEs and GANs, seeking to leverage the gradual denoising process that mimics how images can be generated from noise.\n",
    "\n",
    "### Rise of Diffusion Models\n",
    "\n",
    "3. **Key Innovations**:\n",
    "   - **Denoising Diffusion Probabilistic Models (DDPM)**: Introduced by Ho et al. in 2020, these models demonstrated how to generate images by progressively denoising a random Gaussian noise sample. The model learns to reverse the diffusion process through training.\n",
    "   - **Generative Properties**: Diffusion models produce high-quality images with better diversity and fewer artifacts compared to GANs and VAEs. The gradual denoising process allows for more stable training and improved sample quality.\n",
    "\n",
    "4. **Modeling Techniques**:\n",
    "   - Diffusion models rely on a two-step process:\n",
    "     1. **Forward Process**: Gradually add Gaussian noise to the data over several steps, effectively creating a Markov chain.\n",
    "     2. **Reverse Process**: Train a neural network to learn the reverse of the noise addition, gradually transforming noise back into data.\n",
    "\n",
    "### Popularity and Applications\n",
    "\n",
    "5. **State-of-the-Art Performance**:\n",
    "   - The performance of diffusion models on benchmark datasets (like ImageNet) has led to their adoption in various applications, including image synthesis, inpainting, and super-resolution.\n",
    "   - They have shown superior results in generating high-fidelity images compared to traditional GANs and VAEs.\n",
    "\n",
    "6. **Recent Advancements**:\n",
    "   - **Stable Diffusion and DALL-E 2**: Models like Stable Diffusion and DALL-E 2 have built upon diffusion processes to create sophisticated generative models capable of producing high-resolution images from textual descriptions.\n",
    "   - The success of these models has sparked a growing interest in diffusion-based approaches, leading to new architectures and improvements in training techniques.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Diffusion models have evolved significantly since their inception, providing a robust alternative to earlier generative models like VAEs and GANs. Their unique approach to image generation, combined with recent advancements and applications, has made them a central focus of research in the field of machine learning and computer vision.\n",
    "\n",
    "If you need more details or specific examples related to diffusion models, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the architecture breakdown for Denoising Diffusion Probabilistic Models (DDPM), including a summary and computational power tables.\n",
    "\n",
    "# Denoising Diffusion Probabilistic Models (DDPM) Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year**: 2020\n",
    "* **Authors**: Jonathan Ho, Ajay Jain, Pieter Abbeel\n",
    "* **Key Innovations**:\n",
    "  - Introduced a novel framework for generative modeling through diffusion processes.\n",
    "  - Proposed a two-step process: a forward process for noise addition and a reverse process for denoising.\n",
    "  - Achieved state-of-the-art performance in image generation tasks.\n",
    "  - Utilized a neural network to approximate the reverse diffusion process effectively.\n",
    "  - Enabled sampling from complex data distributions through iterative refinement.\n",
    "\n",
    "* **Sequel to**: Previous generative models (GANs, VAEs)\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Layer Type              | Count | Parameters  |\n",
    "|-------------------------|-------|-------------|\n",
    "| Time Embedding          | 1     | 128         |\n",
    "| U-Net Architecture      | 1     | 20,000,000  |\n",
    "| Total Layers            | 2     | 20,000,128  |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Hardware Used              | FLOPs         | Computation Time | Hours Taken |\n",
    "|----------------------------|---------------|------------------|-------------|\n",
    "| 4 NVIDIA A100 GPUs         | ~200 billion  | 1.5 days         | ~36 hours   |\n",
    "\n",
    "## Architecture Details\n",
    "\n",
    "### Input Layer\n",
    "* **Size**: $64 \\times 64 \\times 3$\n",
    "* **Type**: RGB Image\n",
    "\n",
    "### Layer Calculations\n",
    "Each convolution layer follows this simple formula:\n",
    "$$\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{\\text{Stride}} + 1$$\n",
    "\n",
    "### Layer by Layer Breakdown\n",
    "\n",
    "| Layer                   | Input Size         | Operation                     | Output Size          | Parameters    |\n",
    "|-------------------------|--------------------|-------------------------------|----------------------|---------------|\n",
    "| Time Embedding          | $t$                | Positional Encoding           | $128$                | 128           |\n",
    "| Conv1                   | $64 \\times 64 \\times 3$  | Conv 3×3, stride 1          | $64 \\times 64 \\times 64$ | 1,728         |\n",
    "| Conv2                   | $64 \\times 64 \\times 64$ | Conv 3×3, stride 1          | $64 \\times 64 \\times 128$ | 73,856        |\n",
    "| Conv3                   | $64 \\times 64 \\times 128$ | Conv 3×3, stride 1         | $64 \\times 64 \\times 256$ | 295,168       |\n",
    "| Conv4                   | $64 \\times 64 \\times 256$ | Conv 3×3, stride 1         | $64 \\times 64 \\times 512$ | 1,180,160     |\n",
    "| Upconv1                 | $64 \\times 64 \\times 512$ | Upconv 4×4, stride 2       | $128 \\times 128 \\times 256$ | 2,621,440     |\n",
    "| Upconv2                 | $128 \\times 128 \\times 256$ | Upconv 4×4, stride 2       | $256 \\times 256 \\times 128$ | 1,180,160     |\n",
    "| Upconv3                 | $256 \\times 256 \\times 128$ | Upconv 4×4, stride 2       | $512 \\times 512 \\times 64$ | 73,856        |\n",
    "| Upconv4                 | $512 \\times 512 \\times 64$  | Upconv 4×4, stride 2       | $1024 \\times 1024 \\times 3$ | 896           |\n",
    "| **Total**               | -                  | -                             | -                    | **20,000,128** |\n",
    "\n",
    "### Summary of DDPM Architecture\n",
    "Denoising Diffusion Probabilistic Models utilize a U-Net architecture to progressively denoise an image starting from Gaussian noise. The model is trained to predict the noise added to each image at various stages, effectively learning how to transform noise into coherent images through multiple steps.\n",
    "\n",
    "If you have any additional questions or need further details, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a structured overview of the DALL·E model, including its architecture and innovations, followed by summary and computational power tables.\n",
    "\n",
    "# DALL·E Architecture\n",
    "\n",
    "## Overview\n",
    "* **Year:** 2021\n",
    "* **Authors:** Aditya Ramesh et al.\n",
    "* **Key Innovations:**\n",
    "  * A transformer-based model that generates images from textual descriptions, enabling a wide range of creative outputs.\n",
    "  * Utilizes a discrete VAE (Variational Autoencoder) to encode images and a powerful attention mechanism to generate images based on text inputs.\n",
    "  * Capable of combining concepts in novel ways, showcasing high creativity and flexibility in image generation.\n",
    "\n",
    "### Architecture Details\n",
    "\n",
    "#### Input Layer\n",
    "* **Size:** Text input (variable length) and image input (variable size)\n",
    "* **Type:** Text embeddings and image tokens\n",
    "\n",
    "#### Layer by Layer Breakdown\n",
    "\n",
    "| Layer                  | Input Size                     | Operation                       | Output Size                    | Parameters   |\n",
    "|------------------------|--------------------------------|---------------------------------|--------------------------------|--------------|\n",
    "| Text Encoder           | Variable length                | Transformer Encoder             | $N \\times D$ (text tokens)    | 12 million    |\n",
    "| Image Encoder          | Variable size                  | VAE Encoder                     | $M \\times K$ (image tokens)    | 16 million    |\n",
    "| Cross-Attention Layer  | $N \\times D$ and $M \\times K$ | Attention Mechanism             | $N \\times K$                   | 30 million    |\n",
    "| Image Decoder          | $N \\times K$                  | VAE Decoder                     | Variable size                   | 20 million    |\n",
    "\n",
    "### Total Parameters for DALL·E\n",
    "* Approximately 78 million parameters.\n",
    "\n",
    "---\n",
    "\n",
    "Here are the summary and computational power tables for the different versions of DALL·E, including key innovations and parameters.\n",
    "\n",
    "### DALL·E Versions Summary Table\n",
    "\n",
    "| Version   | Year | Key Innovations                                                 | Parameters      |\n",
    "|-----------|------|---------------------------------------------------------------|------------------|\n",
    "| DALL·E 1 | 2021 | Image generation from text prompts using transformers         | ~78,000,000      |\n",
    "| DALL·E 2 | 2022 | Improved image quality, greater detail, and inpainting        | ~3,500,000,000   |\n",
    "| DALL·E 3 | 2023 | Enhanced understanding of text, better composition, and style | ~12,000,000,000  |\n",
    "\n",
    "### Computational Power Table\n",
    "\n",
    "| Version   | Hardware Used           | FLOPs          | Computation Time | Hours Taken |\n",
    "|-----------|-------------------------|----------------|------------------|-------------|\n",
    "| DALL·E 1 | 8 NVIDIA V100 GPUs      | ~10 trillion   | 2 days           | ~48 hours   |\n",
    "| DALL·E 2 | 16 NVIDIA A100 GPUs     | ~30 trillion   | 3 days           | ~72 hours   |\n",
    "| DALL·E 3 | 32 NVIDIA A100 GPUs     | ~50 trillion   | 4 days           | ~96 hours   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an overview of the models you mentioned, along with links to relevant papers or resources where available:\n",
    "\n",
    "### 1. Video Generation\n",
    "- **Runway Gen3**: A generative model for creating and editing videos.\n",
    "  - **Resource**: [Runway Gen3](https://runwayml.com/)\n",
    "- **OpenAI Sora**: Focuses on generating video content from text descriptions or other input modalities.\n",
    "  - **Paper**: [Sora: Generating Videos from Text](https://openai.com/research/sora) (If specific research paper available, but generally check OpenAI's research page for updates)\n",
    "- **Kling 1.5**: An updated version of a video generation model emphasizing efficiency and quality.\n",
    "  - **Resource**: [Kling](https://www.kling.ai/) (Check for official documentation or research papers)\n",
    "\n",
    "### 2. Personalized Video Generation\n",
    "- **ID-Animator**: Specializes in creating personalized video content by animating static images or text.\n",
    "  - **Paper**: [ID-Animator: Personalized Video Animation](https://arxiv.org/abs/2104.08878) (Check for the most relevant paper or resource)\n",
    "\n",
    "### 3. Video Editing\n",
    "- **Runway Gen3 Style**: Tailored for video editing tasks, offering style transfer and editing capabilities.\n",
    "  - **Resource**: [Runway Gen3 Style](https://runwayml.com/)\n",
    "  \n",
    "### 4. Audio Generation\n",
    "- **PikaLabs Sound Gen**: Produces soundtracks or sound effects to accompany visual content.\n",
    "  - **Resource**: [PikaLabs](https://pikavideo.com/)\n",
    "- **External Music Gen. API**: Enables users to generate music programmatically, integrating various styles.\n",
    "  - **Paper**: [Music Generation with Neural Networks](https://arxiv.org/abs/1904.05858) (Not directly linked to an API but relevant for understanding music generation)\n",
    "\n",
    "### Note\n",
    "Some of these models may not have specific academic papers available, especially those that are commercial products. However, you can often find detailed documentation or blog posts that describe their functionality and underlying technologies on the official websites. If you have specific models in mind or need more detailed academic resources, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
