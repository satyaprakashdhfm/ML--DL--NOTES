{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Class Imbalance in Image Classification\n",
    "\n",
    "Handling class imbalance in image classification involves techniques to ensure that the model doesn't become biased toward the majority class. Here are common approaches:\n",
    "\n",
    "**1. Data-Level Techniques**\n",
    "\n",
    "   - **Oversampling the Minority Class**: Duplicate or augment images from the minority classes to increase their representation. Data augmentation techniques (e.g., rotation, cropping, flipping) can help create diverse samples for minority classes without introducing exact duplicates.\n",
    "   - **Undersampling the Majority Class**: Randomly reduce the number of samples in majority classes to match the minority class size. This is more feasible with larger datasets, though it risks losing important information.\n",
    "\n",
    "**2. Algorithm-Level Techniques**\n",
    "\n",
    "   - **Class Weights Adjustment**: Many deep learning frameworks allow specifying a weight for each class in the loss function. This penalizes misclassifications of the minority class more than the majority class, encouraging the model to pay more attention to the minority class.\n",
    "   - **Focal Loss**: Focal loss is designed for class imbalance by dynamically scaling the loss for hard-to-classify examples, typically from minority classes. It modifies the cross-entropy loss by adding a scaling factor that reduces the loss for well-classified examples and focuses on hard examples.\n",
    "\n",
    "   $$ \n",
    "   \\text{Focal Loss} = -\\alpha (1 - p_t)^\\gamma \\log(p_t)\n",
    "   $$\n",
    "\n",
    "   where \\( p_t \\) is the predicted probability for the true class, \\( \\alpha \\) is a balancing factor for class imbalance, and \\( \\gamma \\) controls the focus on hard examples.\n",
    "\n",
    "**3. Hybrid and Advanced Techniques**\n",
    "\n",
    "   - **Two-Stage Training**: Train the model first on the original data, then fine-tune with balanced classes or using only the minority class. This approach helps retain information while enhancing sensitivity to minority classes.\n",
    "   - **Synthetic Data Generation**: Use techniques like **Generative Adversarial Networks (GANs)** to generate synthetic images for the minority class. GANs can create realistic, diverse images that augment the dataset.\n",
    "   - **Self-Supervised Learning**: In self-supervised learning, the model learns from unlabeled data, which can later be fine-tuned on a smaller, balanced labeled dataset, improving minority class recognition.\n",
    "\n",
    "**4. Evaluation Adjustments**\n",
    "\n",
    "   - **Metrics Beyond Accuracy**: Use metrics like precision, recall, F1-score, or area under the ROC curve (AUC) to get a more balanced view of performance on imbalanced data, as accuracy can be misleading with class imbalance.\n",
    "   - **Confusion Matrix Analysis**: Reviewing the confusion matrix helps identify if the model is biased toward majority classes, guiding further balancing efforts.\n",
    "\n",
    "Each technique can be combined depending on the severity of imbalance, dataset size, and model complexity, but balancing data effectively often requires experimenting with several methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# While both data augmentation and oversampling aim to improve model performance, they address different challenges in machine learning. Data augmentation enhances dataset diversity, whereas oversampling focuses on correcting class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Techniques in Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Data augmentation is a crucial technique used to artificially expand the size of a training dataset by applying various transformations to the original data. This helps improve the generalization of CNNs and reduces overfitting. Here are some common data augmentation techniques:\n",
    "\n",
    "**1. Geometric Transformations**\n",
    "- **Rotation**: Rotate images by a certain angle.\n",
    "  - Example: Rotate by 15, 30, or 45 degrees.\n",
    "  \n",
    "- **Translation**: Shift images along the x or y axis.\n",
    "  - Example: Shift images by a few pixels left, right, up, or down.\n",
    "\n",
    "- **Scaling**: Zoom in or out on images.\n",
    "  - Example: Scale images to 90% or 110% of their original size.\n",
    "\n",
    "- **Flipping**: Flip images horizontally or vertically.\n",
    "  - Example: Horizontal flips are common for many tasks.\n",
    "\n",
    "**2. Color Space Transformations**\n",
    "- **Brightness Adjustment**: Change the brightness of images.\n",
    "  - Example: Increase or decrease brightness by a fixed factor.\n",
    "\n",
    "- **Contrast Adjustment**: Modify the contrast of images.\n",
    "  - Example: Enhance or reduce the contrast of images.\n",
    "\n",
    "- **Saturation Adjustment**: Alter the saturation levels of images.\n",
    "  - Example: Make images more or less colorful.\n",
    "\n",
    "- **Hue Adjustment**: Shift the hue of colors in images.\n",
    "  - Example: Change colors to see how the model reacts to different color variations.\n",
    "\n",
    "**3. Noise Injection**\n",
    "- **Gaussian Noise**: Add random noise to images to make them more robust.\n",
    "  - Example: Add small Gaussian noise to pixel values.\n",
    "\n",
    "- **Salt-and-Pepper Noise**: Introduce random white and black pixels.\n",
    "  - Example: Randomly set a percentage of pixels to maximum or minimum values.\n",
    "\n",
    "**4. Random Erasing**\n",
    "- **Random Erasing**: Randomly remove sections of an image to make the model learn to focus on different features.\n",
    "  - Example: Select a random rectangle in the image and set it to a constant value or noise.\n",
    "\n",
    "**5. Elastic Transformations**\n",
    "- **Elastic Deformations**: Apply random elastic deformations to images.\n",
    "  - Example: Distort images to create variations while preserving overall structure.\n",
    "\n",
    "**6. Cutout**\n",
    "- **Cutout**: Randomly mask out square regions in images.\n",
    "  - Example: Set square patches in an image to zero or the mean pixel value.\n",
    "\n",
    "**7. Mixup**\n",
    "- **Mixup**: Create new training examples by mixing two images and their corresponding labels.\n",
    "  - Example: For images A and B with labels \\(y_A\\) and \\(y_B\\), create a new image \n",
    "  $$\n",
    "  \\text{Image}_{new} = \\lambda \\cdot \\text{Image}_A + (1 - \\lambda) \\cdot \\text{Image}_B\n",
    "  $$ \n",
    "  where \\( \\lambda \\) is a random value between 0 and 1.\n",
    "\n",
    "**8. Random Cropping**\n",
    "- **Random Cropping**: Randomly crop images to create variations in scale and aspect ratio.\n",
    "  - Example: Crop a random section of the original image.\n",
    "\n",
    "**Conclusion**\n",
    "Data augmentation helps increase the diversity of the training dataset, making CNNs more robust and improving their performance on unseen data. Many deep learning frameworks (like TensorFlow and PyTorch) provide built-in support for these augmentation techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Shot Learning\n",
    "\n",
    "**One Shot Learning** is a machine learning approach that enables a recognition system to identify or classify objects based on a single example or image. This is particularly challenging in face recognition, where traditionally, deep learning models require large datasets to achieve good performance.\n",
    "\n",
    "**Definition**\n",
    "- **One Shot Learning**: A recognition system can recognize a person by learning from just one image.\n",
    "\n",
    "**Challenges**\n",
    "Historically, deep learning has not performed well when the amount of training data is small. One Shot Learning addresses this challenge by learning a **similarity function** rather than traditional classification.\n",
    "\n",
    "**Similarity Function**\n",
    "To evaluate the similarity between two images, we define a function \\( d \\):\n",
    "$$\n",
    "d(\\text{img1}, \\text{img2}) = \\text{degree of difference between img1 and img2}\n",
    "$$\n",
    "Where:\n",
    "- **img1** and **img2** are the images being compared.\n",
    "- **d** outputs a value representing how similar or different the images are.\n",
    "\n",
    "**Key Points:**\n",
    "- A lower value of \\( d \\) indicates that the images are likely of the same person (i.e., faces are similar).\n",
    "- We introduce a threshold \\( T \\) to make a decision:\n",
    "$$\n",
    "\\text{If } d(\\text{img1}, \\text{img2}) \\leq T \\text{, then the faces are considered the same.}\n",
    "$$\n",
    "\n",
    "**Advantages of One Shot Learning**\n",
    "- **Efficiency**: It allows for effective recognition with minimal training data, which is crucial in scenarios where data collection is limited.\n",
    "- **Robustness**: The similarity function can generalize well to new inputs, making it adaptable to various situations.\n",
    "\n",
    "**Conclusion**\n",
    "One Shot Learning provides a solution to the challenge of recognizing individuals from very limited data. By focusing on learning a similarity function, it allows for effective face recognition even with just a single example image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss\n",
    "\n",
    "Triplet loss is a loss function commonly used in deep learning, particularly in tasks involving similarity learning, such as face recognition and image retrieval. It aims to ensure that the distance between an anchor sample and a positive sample (similar) is smaller than the distance between the anchor sample and a negative sample (dissimilar) by a predefined margin. \n",
    "\n",
    "**Definition**\n",
    "- Given three inputs: an anchor $x_a$, a positive sample $x_p$ (similar to the anchor), and a negative sample $x_n$ (dissimilar to the anchor), the triplet loss can be defined as:\n",
    "\n",
    "$$\n",
    "L(x_a, x_p, x_n) = \\max(0, d(x_a, x_p) - d(x_a, x_n) + \\alpha)\n",
    "$$\n",
    "\n",
    "where:\n",
    "-  $d(x_i, x_j)$ is a distance metric (e.g., Euclidean distance) between samples $x_i$ and $x_j$,\n",
    "-  $\\alpha$ is the margin that is enforced between positive and negative pairs.\n",
    "\n",
    "**Importance for CNNs**\n",
    "- **Learning Discriminative Features**: Triplet loss helps CNNs learn embeddings that are well-separated for different classes while bringing similar classes closer together in the feature space. This is particularly useful in applications where distinguishing between classes is challenging.\n",
    "- **Robustness to Variations**: It provides a robust mechanism for the model to learn invariant features despite variations in pose, lighting, or other conditions, making it suitable for real-world applications.\n",
    "\n",
    "**Applications of Triplet Loss**\n",
    "1. **Face Recognition**: In face recognition systems, triplet loss can be used to ensure that images of the same person are close in the embedding space, while images of different people are far apart.\n",
    "2. **Image Retrieval**: For systems that retrieve images based on similarity, triplet loss helps improve the ranking of images based on user queries.\n",
    "3. **Object Tracking**: In object tracking, triplet loss can help to distinguish the target object from background clutter or other objects.\n",
    "4. **Speaker Verification**: In audio processing, triplet loss can be applied to ensure that recordings of the same speaker are closer together than recordings from different speakers.\n",
    "\n",
    "By applying triplet loss in CNNs, models can achieve higher accuracy and robustness in distinguishing between classes based on learned embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### EDA Questions for CV (Image Data)\n",
    "\n",
    "1. What are the common dimensions of the images (width, height)?\n",
    "2. How does the aspect ratio vary across the dataset?\n",
    "3. What is the color distribution across images?\n",
    "4. Are there differences in brightness or contrast among the images?\n",
    "5. What is the edge distribution in the images (sharp vs. smooth regions)?\n",
    "6. What common textures or patterns are present in different image categories?\n",
    "7. Is there a class imbalance in the number of images per category?\n",
    "8. What are the most common objects detected in the images?\n",
    "9. Are there patterns in metadata, such as capture date, location, or resolution?\n",
    "10. Do images have similarities in background, lighting, or occlusion within classes? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
