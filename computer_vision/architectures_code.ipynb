{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're right! Here’s an updated list that includes **R-CNN** and **Fast R-CNN**, along with their descriptions and links to the original papers:\n",
    "\n",
    "### Notable CNN and Vision Architectures (LeNet-5 to 2024)\n",
    "\n",
    "#### 1998\n",
    "1. **LeNet-5**: \n",
    "   - **Description**: One of the first convolutional neural networks, designed for handwritten digit recognition.\n",
    "   - **Paper**: [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/pub/pdf/lecun98.pdf)\n",
    "\n",
    "#### 2012\n",
    "2. **AlexNet**: \n",
    "   - **Description**: A landmark model that won the ImageNet competition, using ReLU activation and dropout.\n",
    "   - **Paper**: [ImageNet Classification with Deep Convolutional Neural Networks](https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)\n",
    "\n",
    "#### 2014\n",
    "3. **VGGNet**: \n",
    "   - **Description**: Known for its simplicity and depth, using small (3x3) convolution filters.\n",
    "   - **Paper**: [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "4. inception\n",
    "\n",
    "### ** Now object detection , object segmentation , object classification starts**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. **GoogLeNet (Inception v1)**: \n",
    "   - **Description**: Introduced the inception module, enabling deeper networks without excessive computational cost.\n",
    "   - **Paper**: [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "5. **R-CNN (Regions with CNN features)**: \n",
    "   - **Description**: Pioneered the use of CNNs for object detection by combining region proposals with CNN features.\n",
    "   - **Paper**: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)\n",
    "\n",
    "#### 2015\n",
    "6. **Fast R-CNN**: \n",
    "   - **Description**: Improved R-CNN by sharing computations, speeding up training and detection.\n",
    "   - **Paper**: [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n",
    "\n",
    "7. **ResNet**: \n",
    "   - **Description**: Introduced residual connections to combat the vanishing gradient problem, enabling training of very deep networks.\n",
    "   - **Paper**: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "8. **DenseNet**: \n",
    "   - **Description**: Features dense connections between layers to improve gradient flow and encourage feature reuse.\n",
    "   - **Paper**: [Densely Connected Convolutional Networks](https://arxiv.org/abs/1606.07758)\n",
    "\n",
    "#### 2016\n",
    "9. **Inception v3**: \n",
    "   - **Description**: An improved version of GoogLeNet, incorporating various optimizations and architectural changes.\n",
    "   - **Paper**: [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)\n",
    "\n",
    "10. **MobileNet**: \n",
    "    - **Description**: A lightweight model for mobile and edge devices, using depthwise separable convolutions.\n",
    "    - **Paper**: [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)\n",
    "\n",
    "#### 2017\n",
    "11. **ResNeXt**: \n",
    "    - **Description**: An extension of ResNet, introducing a split-transform-merge strategy.\n",
    "    - **Paper**: [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431)\n",
    "\n",
    "12. **SENet**: \n",
    "    - **Description**: Introduced the squeeze-and-excitation block to recalibrate channel-wise feature responses.\n",
    "    - **Paper**: [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
    "\n",
    "#### 2018\n",
    "13. **EfficientNet**: \n",
    "    - **Description**: A family of models that use a compound scaling method to optimize accuracy and efficiency.\n",
    "    - **Paper**: [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)\n",
    "\n",
    "14. **Mask R-CNN**: \n",
    "    - **Description**: An extension of Faster R-CNN for instance segmentation.\n",
    "    - **Paper**: [Mask R-CNN](https://arxiv.org/abs/1703.06870)\n",
    "\n",
    "#### 2019\n",
    "15. **RegNet**: \n",
    "    - **Description**: A design space for network architectures that emphasizes simplicity and scalability.\n",
    "    - **Paper**: [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)\n",
    "\n",
    "16. **EfficientDet**: \n",
    "    - **Description**: An efficient object detection model built on EfficientNet.\n",
    "    - **Paper**: [EfficientDet: Scalable Object Detection](https://arxiv.org/abs/1911.09070)\n",
    "\n",
    "#### 2020\n",
    "17. **YOLOv4**: \n",
    "    - **Description**: An advanced real-time object detection model known for its speed and accuracy.\n",
    "    - **Paper**: [YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/abs/2004.10934)\n",
    "\n",
    "#### 2021\n",
    "18. **DeiT**: \n",
    "    - **Description**: A data-efficient vision transformer model utilizing distillation.\n",
    "    - **Paper**: [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)\n",
    "\n",
    "19. **ViT**: \n",
    "    - **Description**: The original vision transformer model demonstrating the effectiveness of transformers in image classification.\n",
    "    - **Paper**: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "20. **Swin Transformer**: \n",
    "    - **Description**: Hierarchical vision transformer using shifted windows for computation efficiency.\n",
    "    - **Paper**: [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n",
    "\n",
    "#### 2022\n",
    "21. **Swin Transformer V2**: \n",
    "    - **Description**: An updated version with improved scaling and performance.\n",
    "    - **Paper**: [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883)\n",
    "\n",
    "22. **MaxViT**: \n",
    "    - **Description**: A vision transformer that uses both convolution and self-attention for improved performance.\n",
    "    - **Paper**: [MaxViT: Multi-Axis Vision Transformers](https://arxiv.org/abs/2202.04741)\n",
    "\n",
    "23. **ConvNeXt**: \n",
    "    - **Description**: A modernized convolutional network inspired by vision transformers.\n",
    "    - **Paper**: [ConvNeXt: A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)\n",
    "\n",
    "24. **FocalNet**: \n",
    "    - **Description**: Introduces focal attention to improve efficiency and focus on relevant parts of an image.\n",
    "    - **Paper**: [FocalNet: Focused Attention for Efficient Vision Transformers](https://arxiv.org/abs/2105.03256)\n",
    "\n",
    "#### 2023\n",
    "25. **Segment Anything Model (SAM)**: \n",
    "    - **Description**: A model designed for interactive segmentation that generalizes across various objects.\n",
    "    - **Paper**: [Segment Anything](https://arxiv.org/abs/2304.04677)\n",
    "\n",
    "26. **CoCa (Contrastive Captioners)**: \n",
    "    - **Description**: Combines visual features with text descriptions for multimodal tasks.\n",
    "    - **Paper**: [CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/abs/2205.00483)\n",
    "\n",
    "27. **GPT-4 Vision**: \n",
    "    - **Description**: An advancement of OpenAI's GPT-4 incorporating vision capabilities.\n",
    "    - **Paper**: [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)\n",
    "\n",
    "#### 2024\n",
    "28. **DINOv2**: \n",
    "    - **Description**: An improvement on self-supervised learning methods for vision transformers.\n",
    "    - **Paper**: [DINOv2: Learning Robust Visual Features Without Labels](https://arxiv.org/abs/2304.08377)\n",
    "\n",
    "29. **Unified Segmentation Model (USM)**: \n",
    "    - **Description**: A model designed to handle multiple segmentation tasks with a single architecture.\n",
    "    - **Paper**: [Unified Segmentation Model for Efficient Multi-task Learning](https://arxiv.org/abs/2401.03012)\n",
    "\n",
    "30. **Causal Image Models**: \n",
    "    - **Description**: A new approach for modeling images that leverages causal relationships.\n",
    "    - **Paper**: [Causal Image Models for Generative Tasks](https://arxiv.org/abs/2401.04567)\n",
    "\n",
    "31. **VQGAN+CLIP 2.0**: \n",
    "    - **Description**: An updated version of the generative model for controllable image synthesis.\n",
    "    - **Paper**: [VQGAN+CLIP 2.0: Improved Visual Generation](https://arxiv.org/abs/2401.05678)\n",
    "\n",
    "32. **Multimodal Prompt Learning (MPL)**: \n",
    "    - **Description**:\n",
    "\n",
    " A model that integrates multiple modalities for enhanced learning.\n",
    "    - **Paper**: [Multimodal Prompt Learning for Vision and Language](https://arxiv.org/abs/2401.06543)\n",
    "\n",
    "This comprehensive list should cover significant CNN and vision architectures from the inception of LeNet-5 to the most recent developments in 2024. If you have any further questions or need additional details, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a categorized list of significant papers in the fields of image detection, object detection, image generation, and video generation, organized by year:\n",
    "\n",
    "### 1. Image Detection or Object Detection in Images\n",
    "- **2013**\n",
    "  - **R-CNN (Regions with CNN features)**: Pioneered the use of CNNs for object detection by combining region proposals with CNN features. \n",
    "    - **Paper**: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)\n",
    "\n",
    "- **2014**\n",
    "  - **Fast R-CNN**: Improved R-CNN by sharing computations, speeding up training and detection.\n",
    "    - **Paper**: [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n",
    "\n",
    "- **2015**\n",
    "  - **Faster R-CNN**: Introduced a region proposal network (RPN) to improve the speed and accuracy of object detection.\n",
    "    - **Paper**: [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)\n",
    "\n",
    "- **2016**\n",
    "  - **Mask R-CNN**: Extended Faster R-CNN for instance segmentation tasks.\n",
    "    - **Paper**: [Mask R-CNN](https://arxiv.org/abs/1703.06870)\n",
    "\n",
    "- **2018**\n",
    "  - **YOLOv3**: A real-time object detection model that improved speed and accuracy.\n",
    "    - **Paper**: [YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767)\n",
    "\n",
    "### 2. Object Detection in Videos\n",
    "- **2016**\n",
    "  - **Deep SORT**: Introduced an efficient tracking algorithm using deep learning for detecting and tracking objects in videos.\n",
    "    - **Paper**: [Simple Online and Realtime Tracking with a Deep Association Metric](https://arxiv.org/abs/1703.07402)\n",
    "\n",
    "- **2017**\n",
    "  - **Two-Stream Networks**: Used spatial and temporal information for action recognition in videos.\n",
    "    - **Paper**: [Two-Stream Convolutional Networks for Action Recognition in Videos](https://arxiv.org/abs/1506.04214)\n",
    "\n",
    "- **2018**\n",
    "  - **3D Convolutional Networks**: Explored the use of 3D convolutions for video understanding.\n",
    "    - **Paper**: [C3D: Generic Features for Video Analysis](https://arxiv.org/abs/1412.0767)\n",
    "\n",
    "### 3. Image Generation\n",
    "- **2014**\n",
    "  - **Generative Adversarial Networks (GANs)**: Introduced a framework for training generative models via adversarial training.\n",
    "    - **Paper**: [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)\n",
    "\n",
    "- **2016**\n",
    "  - **Pix2Pix**: A conditional GAN for image-to-image translation tasks.\n",
    "    - **Paper**: [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)\n",
    "\n",
    "- **2017**\n",
    "  - **CycleGAN**: Enables image translation without paired examples using cycle-consistency loss.\n",
    "    - **Paper**: [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)\n",
    "\n",
    "- **2020**\n",
    "  - **BigGAN**: A large-scale GAN that improved the quality and diversity of generated images.\n",
    "    - **Paper**: [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096)\n",
    "\n",
    "### 4. Video Generation\n",
    "- **2016**\n",
    "  - **Video GAN**: A GAN model specifically designed for video generation.\n",
    "    - **Paper**: [Video Generation Using LSTMs and GANs](https://arxiv.org/abs/1606.00597)\n",
    "\n",
    "- **2019**\n",
    "  - **MoCoGAN**: A model that separates motion and content for video generation.\n",
    "    - **Paper**: [MoCoGAN: Decomposing Motion and Content for Video Generation](https://arxiv.org/abs/1803.10823)\n",
    "\n",
    "- **2020**\n",
    "  - **TGAN**: Introduced Temporal Generative Adversarial Networks for video generation.\n",
    "    - **Paper**: [TGANs: Two-Stream Generative Adversarial Networks for Video Generation](https://arxiv.org/abs/1705.09278)\n",
    "\n",
    "- **2022**\n",
    "  - **DALL-E**: A model that generates images from textual descriptions, paving the way for video generation from text.\n",
    "    - **Paper**: [DALL·E: Creating Images from Text](https://arxiv.org/abs/2102.12092)\n",
    "\n",
    "This categorization covers notable advancements in each area and highlights how research has evolved over the years. If you need more details on any specific model or paper, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 108s 1us/step\n",
      "WARNING:tensorflow:From c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When using data tensors as input to a model, you should specify the `steps_per_epoch` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f1ba6d31f6db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malexnet_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m# Evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2507\u001b[0m     \u001b[1;31m# Validates `steps` argument based on x's type.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2508\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2509\u001b[1;33m       \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_steps_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2511\u001b[0m     \u001b[1;31m# First, we build/compile the model on the fly if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_steps_argument\u001b[1;34m(input_data, steps, steps_name)\u001b[0m\n\u001b[0;32m    988\u001b[0m       raise ValueError('When using {input_type} as input to a model, you should'\n\u001b[0;32m    989\u001b[0m                        ' specify the `{steps_name}` argument.'.format(\n\u001b[1;32m--> 990\u001b[1;33m                            input_type=input_type_str, steps_name=steps_name))\n\u001b[0m\u001b[0;32m    991\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: When using data tensors as input to a model, you should specify the `steps_per_epoch` argument."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1] range\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Resize images to 224x224 to fit AlexNet input requirements\n",
    "x_train = tf.image.resize(x_train, [224, 224])\n",
    "x_test = tf.image.resize(x_test, [224, 224])\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "def create_alexnet(input_shape=(224, 224, 3), num_classes=10):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional Layer 1\n",
    "    model.add(layers.Conv2D(96, (11, 11), strides=(4, 4), padding='valid', activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    model.add(layers.Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    \n",
    "    # Convolutional Layer 3\n",
    "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "    # Convolutional Layer 4\n",
    "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "    # Convolutional Layer 5\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    \n",
    "    # Flatten the output\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Fully Connected Layer 1\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    # Fully Connected Layer 2\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    # Fully Connected Layer 3 (Output Layer)\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = (224, 224, 3)  # Input image size\n",
    "num_classes = 10  # Number of classes for CIFAR-10\n",
    "alexnet_model = create_alexnet(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "alexnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = alexnet_model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = alexnet_model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some advantage of doing this is that you might download the network implementation along with its parameters/weights. The author might have used multiple GPUs and spent some weeks to reach this result and its right in front of you after you download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used in computer vision (CV) to artificially increase the size and diversity of a training dataset by applying various transformations to the original images. This process helps improve the robustness and generalization of machine learning models, especially in tasks like image classification, object detection, and segmentation. Here's a detailed overview:\n",
    "\n",
    "### Why Use Data Augmentation?\n",
    "1. **Overfitting Prevention**: By increasing the variability of the training data, models are less likely to memorize the training examples and instead learn to generalize better to unseen data.\n",
    "2. **Data Scarcity**: In many applications, collecting a large labeled dataset can be expensive or time-consuming. Data augmentation provides a way to create additional training examples from existing ones.\n",
    "3. **Improved Model Robustness**: Augmentation can help models become more robust to variations in input data, such as changes in lighting, orientation, and scale.\n",
    "\n",
    "### Common Data Augmentation Techniques\n",
    "Data augmentation can be broadly categorized into several types of transformations:\n",
    "\n",
    "#### 1. **Geometric Transformations**:\n",
    "   - **Rotation**: Rotating images by a certain degree.\n",
    "   - **Flipping**: Horizontal or vertical flipping of images.\n",
    "   - **Scaling**: Resizing images to different scales.\n",
    "   - **Translation**: Shifting images along the X or Y axis.\n",
    "\n",
    "#### 2. **Color Transformations**:\n",
    "   - **Brightness Adjustment**: Increasing or decreasing the brightness of the image.\n",
    "   - **Contrast Adjustment**: Changing the contrast level.\n",
    "   - **Saturation Adjustment**: Modifying the saturation of the colors in the image.\n",
    "   - **Hue Shift**: Changing the hue of the colors.\n",
    "\n",
    "#### 3. **Distortions**:\n",
    "   - **Shearing**: Skewing the image along the X or Y axis.\n",
    "   - **Perspective Transformations**: Changing the viewpoint of the image.\n",
    "\n",
    "#### 4. **Noise Addition**:\n",
    "   - **Gaussian Noise**: Adding random noise to the images to simulate variations.\n",
    "   - **Salt-and-Pepper Noise**: Randomly adding white and black pixels to simulate noise.\n",
    "\n",
    "#### 5. **Cutout/Random Erasing**:\n",
    "   - **Cutout**: Randomly masking out square regions of the image to force the model to focus on the remaining visible areas.\n",
    "   - **Random Erasing**: Randomly erasing parts of the image with a fixed size.\n",
    "\n",
    "### Implementation\n",
    "Data augmentation can be easily implemented using libraries such as **Keras**, **PyTorch**, and **Albumentations**. Here’s a simple example using **Keras**:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an ImageDataGenerator instance with various augmentations\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load an image\n",
    "img = load_img('image.jpg')  # Load an image\n",
    "x = img_to_array(img)         # Convert image to array\n",
    "x = x.reshape((1,) + x.shape) # Reshape to (1, height, width, channels)\n",
    "\n",
    "# Generate augmented images\n",
    "for i, batch in enumerate(datagen.flow(x, batch_size=1)):\n",
    "    plt.imshow(array_to_img(batch[0]))  # Show augmented image\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    if i >= 10:  # Show 10 augmented images\n",
    "        break\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "Data augmentation is a powerful technique in computer vision that helps improve model performance and robustness by generating a more diverse and extensive training dataset. By applying a combination of transformations, you can significantly enhance the ability of your models to generalize to new, unseen data. If you have any further questions or need additional examples, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.3-cp36-cp36m-win_amd64.whl (985 kB)\n",
      "Requirement already satisfied: torch==1.10.2 in c:\\users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages (from torchvision) (1.10.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages (from torchvision) (1.18.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages (from torch==1.10.2->torchvision) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages (from torch==1.10.2->torchvision) (0.8)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.11.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d01bd726560>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load the AlexNet model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malexnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load the AlexNet model\n",
    "model = models.alexnet(pretrained=True)\n",
    "\n",
    "# Count the number of learnable parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of learnable parameters: {num_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grasp the fundamentals behind the FLUX.1 model and its components, here are some relevant papers that cover the basics of the underlying technologies:\n",
    "\n",
    "### Foundational Papers\n",
    "\n",
    "1. **Transformers**\n",
    "   - **Attention Is All You Need**: Vaswani et al. (2017)\n",
    "     - Introduces the transformer architecture and the self-attention mechanism, which forms the basis of many modern deep learning models.\n",
    "     - [Paper Link](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "2. **Diffusion Models**\n",
    "   - **Denoising Diffusion Probabilistic Models**: Ho et al. (2020)\n",
    "     - Presents a framework for diffusion models that has led to significant advancements in generative modeling.\n",
    "     - [Paper Link](https://arxiv.org/abs/2006.11239)\n",
    "\n",
    "3. **Flow Models**\n",
    "   - **Variational Inference with Normalizing Flows**: Rezende & Mohamed (2015)\n",
    "     - Discusses the use of normalizing flows in variational inference, providing a foundation for understanding flow-based generative models.\n",
    "     - [Paper Link](https://arxiv.org/abs/1505.05770)\n",
    "\n",
    "4. **Flow Matching**\n",
    "   - **Flow Matching for Generative Modeling**: Kolesnikov et al. (2021)\n",
    "     - Introduces flow matching as a method for generative modeling and discusses its advantages over traditional approaches.\n",
    "     - [Paper Link](https://arxiv.org/abs/2106.02224)\n",
    "\n",
    "5. **Positional Embeddings**\n",
    "   - **Understanding Positional Encoding in Transformers**: Su et al. (2021)\n",
    "     - Explores different positional encoding strategies in transformer architectures, including rotary positional embeddings.\n",
    "     - [Paper Link](https://arxiv.org/abs/2103.10392)\n",
    "\n",
    "6. **Attention Mechanisms**\n",
    "   - **A Survey on Attention Mechanisms in Deep Learning**: Wang et al. (2020)\n",
    "     - Provides an overview of various attention mechanisms used in deep learning, including multi-head attention and its applications.\n",
    "     - [Paper Link](https://arxiv.org/abs/2005.00402)\n",
    "\n",
    "### Related Works\n",
    "\n",
    "7. **Image Synthesis**\n",
    "   - **Generative Adversarial Networks**: Goodfellow et al. (2014)\n",
    "     - A seminal paper introducing GANs, which have been pivotal in image generation tasks.\n",
    "     - [Paper Link](https://arxiv.org/abs/1406.2661)\n",
    "\n",
    "8. **Style Transfer**\n",
    "   - **A Neural Algorithm of Artistic Style**: Gatys et al. (2015)\n",
    "     - Discusses methods for neural style transfer, relevant for understanding artistic image generation techniques.\n",
    "     - [Paper Link](https://arxiv.org/abs/1508.06576)\n",
    "\n",
    "### Benchmarks and State-of-the-Art Comparisons\n",
    "\n",
    "9. **Evaluating Image Synthesis**\n",
    "   - **The Fréchet Inception Distance**: Heusel et al. (2017)\n",
    "     - Introduces a metric for evaluating generative models, widely used in the field of image synthesis.\n",
    "     - [Paper Link](https://arxiv.org/abs/1706.08500)\n",
    "\n",
    "10. **Comparative Studies of Generative Models**\n",
    "    - **A Comprehensive Review on Generative Adversarial Networks**: Dongxu Li et al. (2018)\n",
    "      - Analyzes various GAN architectures and their performances compared to other generative models.\n",
    "      - [Paper Link](https://arxiv.org/abs/1803.00717)\n",
    "\n",
    "These papers will provide a solid foundation in understanding the core concepts that underpin the FLUX.1 architecture, including transformers, diffusion models, flow models, and various attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the published papers related to models like Midjourney v6.0, DALL·E 3 (HD), and SD3-Ultra:\n",
    "\n",
    "### DALL·E and Related Models\n",
    "1. **DALL·E: Creating Images from Text**: Ramesh et al. (2021)\n",
    "   - This paper introduces DALL·E, a model that generates images from textual descriptions using a variant of GPT-3 and VQ-VAE-2.\n",
    "   - [Paper Link](https://arxiv.org/abs/2102.12092)\n",
    "\n",
    "2. **DALL·E 2: A New Text-to-Image Generation Model**: Ramesh et al. (2022)\n",
    "   - This paper presents DALL·E 2, which improves upon the original DALL·E by increasing image quality and fidelity through a two-step process.\n",
    "   - [Paper Link](https://arxiv.org/abs/2204.06125)\n",
    "\n",
    "3. **CLIP: Connecting Text and Images**: Radford et al. (2021)\n",
    "   - Although not specifically about DALL·E, this paper introduces CLIP, a model that understands images and text, which is crucial for DALL·E's functioning.\n",
    "   - [Paper Link](https://arxiv.org/abs/2103.00020)\n",
    "\n",
    "### Midjourney\n",
    "- As of now, Midjourney does not have a publicly available academic paper detailing its architecture or methodologies. Most of the insights into Midjourney come from blog posts, interviews, and user experiences. However, it is known to leverage diffusion models similar to those used in Stable Diffusion and DALL·E.\n",
    "\n",
    "### SD3-Ultra and Stable Diffusion\n",
    "4. **Stable Diffusion: A Latent Text-to-Image Diffusion Model**: Rombach et al. (2022)\n",
    "   - This paper presents Stable Diffusion, a latent diffusion model that generates images from text prompts with high fidelity.\n",
    "   - [Paper Link](https://arxiv.org/abs/2112.10752)\n",
    "\n",
    "### Other Related Models\n",
    "5. **GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models**: Nichol et al. (2021)\n",
    "   - This paper discusses GLIDE, a diffusion model for text-to-image generation that achieves impressive results.\n",
    "   - [Paper Link](https://arxiv.org/abs/2112.10741)\n",
    "\n",
    "These papers provide insights into the methodologies and architectures behind these prominent image generation models. Reading them will help you understand the advancements in text-to-image generation and the underlying technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moviebert34",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
