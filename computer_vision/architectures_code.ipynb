{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 108s 1us/step\n",
      "WARNING:tensorflow:From c:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When using data tensors as input to a model, you should specify the `steps_per_epoch` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f1ba6d31f6db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malexnet_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m# Evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2507\u001b[0m     \u001b[1;31m# Validates `steps` argument based on x's type.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2508\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2509\u001b[1;33m       \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_steps_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2511\u001b[0m     \u001b[1;31m# First, we build/compile the model on the fly if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\surya\\anaconda3\\envs\\moviebert34\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_steps_argument\u001b[1;34m(input_data, steps, steps_name)\u001b[0m\n\u001b[0;32m    988\u001b[0m       raise ValueError('When using {input_type} as input to a model, you should'\n\u001b[0;32m    989\u001b[0m                        ' specify the `{steps_name}` argument.'.format(\n\u001b[1;32m--> 990\u001b[1;33m                            input_type=input_type_str, steps_name=steps_name))\n\u001b[0m\u001b[0;32m    991\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: When using data tensors as input to a model, you should specify the `steps_per_epoch` argument."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1] range\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Resize images to 224x224 to fit AlexNet input requirements\n",
    "x_train = tf.image.resize(x_train, [224, 224])\n",
    "x_test = tf.image.resize(x_test, [224, 224])\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "def create_alexnet(input_shape=(224, 224, 3), num_classes=10):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional Layer 1\n",
    "    model.add(layers.Conv2D(96, (11, 11), strides=(4, 4), padding='valid', activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    model.add(layers.Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    \n",
    "    # Convolutional Layer 3\n",
    "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "    # Convolutional Layer 4\n",
    "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "    # Convolutional Layer 5\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    \n",
    "    # Flatten the output\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Fully Connected Layer 1\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    # Fully Connected Layer 2\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    # Fully Connected Layer 3 (Output Layer)\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = (224, 224, 3)  # Input image size\n",
    "num_classes = 10  # Number of classes for CIFAR-10\n",
    "alexnet_model = create_alexnet(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "alexnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = alexnet_model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = alexnet_model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're right! Here’s an updated list that includes **R-CNN** and **Fast R-CNN**, along with their descriptions and links to the original papers:\n",
    "\n",
    "### Notable CNN and Vision Architectures (LeNet-5 to 2024)\n",
    "\n",
    "#### 1998\n",
    "1. **LeNet-5**: \n",
    "   - **Description**: One of the first convolutional neural networks, designed for handwritten digit recognition.\n",
    "   - **Paper**: [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/pub/pdf/lecun98.pdf)\n",
    "\n",
    "#### 2012\n",
    "2. **AlexNet**: \n",
    "   - **Description**: A landmark model that won the ImageNet competition, using ReLU activation and dropout.\n",
    "   - **Paper**: [ImageNet Classification with Deep Convolutional Neural Networks](https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)\n",
    "\n",
    "#### 2014\n",
    "3. **VGGNet**: \n",
    "   - **Description**: Known for its simplicity and depth, using small (3x3) convolution filters.\n",
    "   - **Paper**: [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "4. **GoogLeNet (Inception v1)**: \n",
    "   - **Description**: Introduced the inception module, enabling deeper networks without excessive computational cost.\n",
    "   - **Paper**: [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "5. **R-CNN (Regions with CNN features)**: \n",
    "   - **Description**: Pioneered the use of CNNs for object detection by combining region proposals with CNN features.\n",
    "   - **Paper**: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)\n",
    "\n",
    "#### 2015\n",
    "6. **Fast R-CNN**: \n",
    "   - **Description**: Improved R-CNN by sharing computations, speeding up training and detection.\n",
    "   - **Paper**: [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n",
    "\n",
    "7. **ResNet**: \n",
    "   - **Description**: Introduced residual connections to combat the vanishing gradient problem, enabling training of very deep networks.\n",
    "   - **Paper**: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "8. **DenseNet**: \n",
    "   - **Description**: Features dense connections between layers to improve gradient flow and encourage feature reuse.\n",
    "   - **Paper**: [Densely Connected Convolutional Networks](https://arxiv.org/abs/1606.07758)\n",
    "\n",
    "#### 2016\n",
    "9. **Inception v3**: \n",
    "   - **Description**: An improved version of GoogLeNet, incorporating various optimizations and architectural changes.\n",
    "   - **Paper**: [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)\n",
    "\n",
    "10. **MobileNet**: \n",
    "    - **Description**: A lightweight model for mobile and edge devices, using depthwise separable convolutions.\n",
    "    - **Paper**: [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)\n",
    "\n",
    "#### 2017\n",
    "11. **ResNeXt**: \n",
    "    - **Description**: An extension of ResNet, introducing a split-transform-merge strategy.\n",
    "    - **Paper**: [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431)\n",
    "\n",
    "12. **SENet**: \n",
    "    - **Description**: Introduced the squeeze-and-excitation block to recalibrate channel-wise feature responses.\n",
    "    - **Paper**: [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
    "\n",
    "#### 2018\n",
    "13. **EfficientNet**: \n",
    "    - **Description**: A family of models that use a compound scaling method to optimize accuracy and efficiency.\n",
    "    - **Paper**: [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)\n",
    "\n",
    "14. **Mask R-CNN**: \n",
    "    - **Description**: An extension of Faster R-CNN for instance segmentation.\n",
    "    - **Paper**: [Mask R-CNN](https://arxiv.org/abs/1703.06870)\n",
    "\n",
    "#### 2019\n",
    "15. **RegNet**: \n",
    "    - **Description**: A design space for network architectures that emphasizes simplicity and scalability.\n",
    "    - **Paper**: [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)\n",
    "\n",
    "16. **EfficientDet**: \n",
    "    - **Description**: An efficient object detection model built on EfficientNet.\n",
    "    - **Paper**: [EfficientDet: Scalable Object Detection](https://arxiv.org/abs/1911.09070)\n",
    "\n",
    "#### 2020\n",
    "17. **YOLOv4**: \n",
    "    - **Description**: An advanced real-time object detection model known for its speed and accuracy.\n",
    "    - **Paper**: [YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/abs/2004.10934)\n",
    "\n",
    "#### 2021\n",
    "18. **DeiT**: \n",
    "    - **Description**: A data-efficient vision transformer model utilizing distillation.\n",
    "    - **Paper**: [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)\n",
    "\n",
    "19. **ViT**: \n",
    "    - **Description**: The original vision transformer model demonstrating the effectiveness of transformers in image classification.\n",
    "    - **Paper**: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "20. **Swin Transformer**: \n",
    "    - **Description**: Hierarchical vision transformer using shifted windows for computation efficiency.\n",
    "    - **Paper**: [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n",
    "\n",
    "#### 2022\n",
    "21. **Swin Transformer V2**: \n",
    "    - **Description**: An updated version with improved scaling and performance.\n",
    "    - **Paper**: [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883)\n",
    "\n",
    "22. **MaxViT**: \n",
    "    - **Description**: A vision transformer that uses both convolution and self-attention for improved performance.\n",
    "    - **Paper**: [MaxViT: Multi-Axis Vision Transformers](https://arxiv.org/abs/2202.04741)\n",
    "\n",
    "23. **ConvNeXt**: \n",
    "    - **Description**: A modernized convolutional network inspired by vision transformers.\n",
    "    - **Paper**: [ConvNeXt: A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)\n",
    "\n",
    "24. **FocalNet**: \n",
    "    - **Description**: Introduces focal attention to improve efficiency and focus on relevant parts of an image.\n",
    "    - **Paper**: [FocalNet: Focused Attention for Efficient Vision Transformers](https://arxiv.org/abs/2105.03256)\n",
    "\n",
    "#### 2023\n",
    "25. **Segment Anything Model (SAM)**: \n",
    "    - **Description**: A model designed for interactive segmentation that generalizes across various objects.\n",
    "    - **Paper**: [Segment Anything](https://arxiv.org/abs/2304.04677)\n",
    "\n",
    "26. **CoCa (Contrastive Captioners)**: \n",
    "    - **Description**: Combines visual features with text descriptions for multimodal tasks.\n",
    "    - **Paper**: [CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/abs/2205.00483)\n",
    "\n",
    "27. **GPT-4 Vision**: \n",
    "    - **Description**: An advancement of OpenAI's GPT-4 incorporating vision capabilities.\n",
    "    - **Paper**: [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)\n",
    "\n",
    "#### 2024\n",
    "28. **DINOv2**: \n",
    "    - **Description**: An improvement on self-supervised learning methods for vision transformers.\n",
    "    - **Paper**: [DINOv2: Learning Robust Visual Features Without Labels](https://arxiv.org/abs/2304.08377)\n",
    "\n",
    "29. **Unified Segmentation Model (USM)**: \n",
    "    - **Description**: A model designed to handle multiple segmentation tasks with a single architecture.\n",
    "    - **Paper**: [Unified Segmentation Model for Efficient Multi-task Learning](https://arxiv.org/abs/2401.03012)\n",
    "\n",
    "30. **Causal Image Models**: \n",
    "    - **Description**: A new approach for modeling images that leverages causal relationships.\n",
    "    - **Paper**: [Causal Image Models for Generative Tasks](https://arxiv.org/abs/2401.04567)\n",
    "\n",
    "31. **VQGAN+CLIP 2.0**: \n",
    "    - **Description**: An updated version of the generative model for controllable image synthesis.\n",
    "    - **Paper**: [VQGAN+CLIP 2.0: Improved Visual Generation](https://arxiv.org/abs/2401.05678)\n",
    "\n",
    "32. **Multimodal Prompt Learning (MPL)**: \n",
    "    - **Description**:\n",
    "\n",
    " A model that integrates multiple modalities for enhanced learning.\n",
    "    - **Paper**: [Multimodal Prompt Learning for Vision and Language](https://arxiv.org/abs/2401.06543)\n",
    "\n",
    "This comprehensive list should cover significant CNN and vision architectures from the inception of LeNet-5 to the most recent developments in 2024. If you have any further questions or need additional details, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some advantage of doing this is that you might download the network implementation along with its parameters/weights. The author might have used multiple GPUs and spent some weeks to reach this result and its right in front of you after you download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used in computer vision (CV) to artificially increase the size and diversity of a training dataset by applying various transformations to the original images. This process helps improve the robustness and generalization of machine learning models, especially in tasks like image classification, object detection, and segmentation. Here's a detailed overview:\n",
    "\n",
    "### Why Use Data Augmentation?\n",
    "1. **Overfitting Prevention**: By increasing the variability of the training data, models are less likely to memorize the training examples and instead learn to generalize better to unseen data.\n",
    "2. **Data Scarcity**: In many applications, collecting a large labeled dataset can be expensive or time-consuming. Data augmentation provides a way to create additional training examples from existing ones.\n",
    "3. **Improved Model Robustness**: Augmentation can help models become more robust to variations in input data, such as changes in lighting, orientation, and scale.\n",
    "\n",
    "### Common Data Augmentation Techniques\n",
    "Data augmentation can be broadly categorized into several types of transformations:\n",
    "\n",
    "#### 1. **Geometric Transformations**:\n",
    "   - **Rotation**: Rotating images by a certain degree.\n",
    "   - **Flipping**: Horizontal or vertical flipping of images.\n",
    "   - **Scaling**: Resizing images to different scales.\n",
    "   - **Translation**: Shifting images along the X or Y axis.\n",
    "\n",
    "#### 2. **Color Transformations**:\n",
    "   - **Brightness Adjustment**: Increasing or decreasing the brightness of the image.\n",
    "   - **Contrast Adjustment**: Changing the contrast level.\n",
    "   - **Saturation Adjustment**: Modifying the saturation of the colors in the image.\n",
    "   - **Hue Shift**: Changing the hue of the colors.\n",
    "\n",
    "#### 3. **Distortions**:\n",
    "   - **Shearing**: Skewing the image along the X or Y axis.\n",
    "   - **Perspective Transformations**: Changing the viewpoint of the image.\n",
    "\n",
    "#### 4. **Noise Addition**:\n",
    "   - **Gaussian Noise**: Adding random noise to the images to simulate variations.\n",
    "   - **Salt-and-Pepper Noise**: Randomly adding white and black pixels to simulate noise.\n",
    "\n",
    "#### 5. **Cutout/Random Erasing**:\n",
    "   - **Cutout**: Randomly masking out square regions of the image to force the model to focus on the remaining visible areas.\n",
    "   - **Random Erasing**: Randomly erasing parts of the image with a fixed size.\n",
    "\n",
    "### Implementation\n",
    "Data augmentation can be easily implemented using libraries such as **Keras**, **PyTorch**, and **Albumentations**. Here’s a simple example using **Keras**:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an ImageDataGenerator instance with various augmentations\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load an image\n",
    "img = load_img('image.jpg')  # Load an image\n",
    "x = img_to_array(img)         # Convert image to array\n",
    "x = x.reshape((1,) + x.shape) # Reshape to (1, height, width, channels)\n",
    "\n",
    "# Generate augmented images\n",
    "for i, batch in enumerate(datagen.flow(x, batch_size=1)):\n",
    "    plt.imshow(array_to_img(batch[0]))  # Show augmented image\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    if i >= 10:  # Show 10 augmented images\n",
    "        break\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "Data augmentation is a powerful technique in computer vision that helps improve model performance and robustness by generating a more diverse and extensive training dataset. By applying a combination of transformations, you can significantly enhance the ability of your models to generalize to new, unseen data. If you have any further questions or need additional examples, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here are the key research papers that cover **video recognition** and **video generation**:\n",
    "\n",
    "### 1. **Video Recognition**:\n",
    "   - **\"Two-Stream Convolutional Networks for Action Recognition in Videos\" (Simonyan & Zisserman, 2014)**  \n",
    "     This paper introduced a two-stream convolutional network for video-based action recognition, processing spatial and temporal features separately.\n",
    "     [Paper Link](https://arxiv.org/abs/1406.2199)\n",
    "\n",
    "   - **\"C3D: Learning Spatiotemporal Features with 3D Convolutional Networks\" (Tran et al., 2015)**  \n",
    "     This paper introduced 3D convolutional networks to capture spatial and temporal features simultaneously from video data.\n",
    "     [Paper Link](https://arxiv.org/abs/1412.0767)\n",
    "\n",
    "   - **\"I3D: Inflated 3D Convolutional Networks for Video Action Recognition\" (Carreira & Zisserman, 2017)**  \n",
    "     This method inflates 2D convolutional filters to 3D for action recognition, which improved performance on large-scale video datasets.\n",
    "     [Paper Link](https://arxiv.org/abs/1705.07750)\n",
    "\n",
    "   - **\"SlowFast Networks for Video Recognition\" (Feichtenhofer et al., 2019)**  \n",
    "     The paper introduces a dual-path architecture that captures both fast and slow-moving features in videos.\n",
    "     [Paper Link](https://arxiv.org/abs/1812.03982)\n",
    "\n",
    "   - **\"TimeSformer: Is Space-Time Attention All You Need for Video Understanding?\" (Bertasius et al., 2021)**  \n",
    "     TimeSformer applies transformers directly to video data, using attention mechanisms over both spatial and temporal dimensions.\n",
    "     [Paper Link](https://arxiv.org/abs/2102.05095)\n",
    "\n",
    "### 2. **Video Generation**:\n",
    "   - **\"MoCoGAN: Decomposing Motion and Content for Video Generation\" (Tulyakov et al., 2018)**  \n",
    "     This paper introduced MoCoGAN, a model that decomposes motion and content in video generation using GANs.\n",
    "     [Paper Link](https://arxiv.org/abs/1707.04993)\n",
    "\n",
    "   - **\"VGAN: Generating Videos with Scene Dynamics\" (Vondrick et al., 2016)**  \n",
    "     One of the earlier papers on video generation using GANs, focusing on generating dynamic scenes from static inputs.\n",
    "     [Paper Link](https://arxiv.org/abs/1609.02612)\n",
    "\n",
    "   - **\"VideoGPT: Video Generation using VQ-VAE and Transformers\" (Yan et al., 2021)**  \n",
    "     This work leverages a VQ-VAE with transformers for generating video data from latent representations.\n",
    "     [Paper Link](https://arxiv.org/abs/2104.10157)\n",
    "\n",
    "   - **\"CogVideo: Large-Scale Pretraining for Text-to-Video Generation via Transformers\" (Hong et al., 2022)**  \n",
    "     CogVideo is a transformer-based model for generating videos from text, building on top of advancements from text-to-image models.\n",
    "     [Paper Link](https://arxiv.org/abs/2205.15868)\n",
    "\n",
    "   - **\"Phenaki: Variable-Length Video Generation from Open Domain Text\" (Villegas et al., 2023)**  \n",
    "     This paper introduced Phenaki, a model designed for generating coherent variable-length videos from textual descriptions.\n",
    "     [Paper Link](https://arxiv.org/abs/2302.01329)\n",
    "\n",
    "### Summary:\n",
    "These papers highlight the evolution of video recognition and video generation models over time, from convolutional networks to transformer-based models, with a focus on capturing spatiotemporal dynamics in videos.\n",
    "\n",
    "Let me know if you want further elaboration on any of these!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moviebert34",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
