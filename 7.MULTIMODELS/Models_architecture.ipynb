{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "### Multimodal AI Models by Year\n",
    "\n",
    "1. **DALL-E** (2021) - OpenAI's model that generates images from textual descriptions, marking an early significant step in multimodal AI.\n",
    "\n",
    "2. **CLIP (Contrastive Language-Image Pre-training)** (2021) - Also from OpenAI, CLIP aligns text and images for tasks like image classification.\n",
    "\n",
    "3. **Flamingo** (2022) - Developed by DeepMind, this model integrates visual and textual features for tasks such as image captioning and visual question answering.\n",
    "\n",
    "4. **ChatGPT** (November 2022) - Initially a unimodal model focused on text, it laid the groundwork for future multimodal capabilities.\n",
    "\n",
    "5. **GPT-4** (March 2023) - Enhanced capabilities in text generation, serving as a precursor to multimodal functionalities.\n",
    "\n",
    "6. **Gemini** (December 2023) - Googleâ€™s natively multimodal model capable of processing text, images, audio, video, and code.\n",
    "\n",
    "7. **GPT-4o** (February 2024) - OpenAI's advanced multimodal model that processes and generates multiple data types in real-time.\n",
    "\n",
    "8. **Claude 3** (March 2024) - Anthropic's model with enhanced vision capabilities, integrating text and image processing.\n",
    "\n",
    "9. **Meta ImageBind** (April 2024) - Supports six modalities: text, audio, visuals, movement, thermal, and depth data.\n",
    "\n",
    "10. **GPT-4o Mini** (July 2024) - A smaller variant of GPT-4o designed for efficiency while maintaining multimodal functionality.\n",
    "\n",
    "11. **Claude 3.5 Sonnet** (July 2024) - An upgraded version of Claude with improved vision and reasoning capabilities.\n",
    "\n",
    "This timeline highlights the evolution of multimodal AI models from their inception to the present day, showcasing advancements in integrating various data types for more complex interactions and outputs.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences between ai agents vs multimodal ai :\n",
    "**Functionality** :\n",
    "\n",
    "AI Agents are designed primarily for task execution, decision-making, and interaction with environments (e.g., customer service bots, autonomous vehicles).\n",
    "\n",
    "Multimodal AI Models, such as CLIP or DALL-E, are specifically focused on integrating multiple types of data (text, images, audio) for tasks like image generation or understanding context from various inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
