{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras \n",
    "has a high level of API as it is capable on running on top of the other frameworks. So it is easier to use and less code are needed. PyTorch has a low level API and hence it becomes little difficult to use and more code is required for the similar task but it gives better control to the programmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfer learning is a powerful technique in deep learning that allows you to leverage existing neural network (NN) architectures that have already been trained on large datasets. Here’s a simple breakdown:\n",
    "\n",
    "**What is Transfer Learning?**\n",
    "- Instead of starting from scratch with random weights for your NN, you can use pretrained parameters or weights from a network that has already learned to recognize various features in data.\n",
    "- This approach can significantly boost the performance of your NN for a specific task.\n",
    "\n",
    "**Benefits of Transfer Learning**\n",
    "- **Time-Saving**: Pretrained models have typically been trained on extensive datasets like ImageNet, MS COCO, or Pascal. They have optimized hyperparameters and took a long time to learn those parameters, saving you both time and computational resources.\n",
    "- **Performance Improvement**: By using a model trained on a large dataset, your NN can achieve better performance, especially when you have limited data for your specific problem.\n",
    "\n",
    "**Example Scenario: Cat Classification**\n",
    "Imagine you want to classify images of cats into three classes: Tigger, Misty, and neither. You don’t have a lot of data to train a NN from scratch.\n",
    "\n",
    "1. **Using Pretrained Models**: You can download a good pretrained NN with its weights.\n",
    "2. **Modifying the Network**: Remove the softmax activation layer from the pretrained model and replace it with your own softmax layer tailored to your classes.\n",
    "3. **Freezing Layers**: Fix the weights of the layers you don't want to train, while allowing the new layer to learn. This is often done by setting `trainable = 0` or `freeze = 0` in your framework.\n",
    "\n",
    "**Speeding Up Training**\n",
    "- A helpful trick is to run the pretrained NN without the final softmax layer to get an intermediate representation of your images. \n",
    "- Save these representations to disk and use them as input features for a shallow NN. This approach converts your images into vectors, which speeds up the training process since you skip running the images through all the layers.\n",
    "\n",
    "**Additional Examples**\n",
    "1. **Using More Data**: If you have a lot of pictures of your cats, you can freeze some layers from the beginning of the pretrained network and let the other layers learn.\n",
    "2. **Custom Layers**: You can discard the unfrozen layers and add your own layers tailored to your specific task.\n",
    "3. **Fine-Tuning**: If you have sufficient data, you can fine-tune all layers in your pretrained network. Instead of random initialization, you retain the learned parameters and continue training from there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning vs. Transfer Learning\n",
    "\n",
    "Yes, fine-tuning is a specific approach within the broader concept of transfer learning. Here’s a simple comparison:\n",
    "\n",
    "## Transfer Learning\n",
    "- **Definition**: Transfer learning involves taking a pretrained model that has been trained on a large dataset and applying it to a new but related task.\n",
    "- **Usage**: In transfer learning, you may freeze some layers of the pretrained model and only train the new layers you added for your specific task.\n",
    "\n",
    "## Fine-Tuning\n",
    "- **Definition**: Fine-tuning is the process of taking a pretrained model and allowing some or all of its layers to be retrained on a new dataset.\n",
    "- **Usage**: During fine-tuning, you usually start with the weights learned from the pretrained model and adjust them slightly to better fit your new task. This often involves unfreezing some of the earlier layers and training the model for a few additional epochs with a lower learning rate.\n",
    "\n",
    "## Key Differences\n",
    "- **Layer Freezing**: In transfer learning, you often keep most layers frozen and train only the new layers. In fine-tuning, you gradually unfreeze layers and adjust their weights based on the new data.\n",
    "- **Goal**: Transfer learning aims to leverage the knowledge from one task to improve another, while fine-tuning focuses on refining the pretrained model to adapt it specifically to the new task.\n",
    "\n",
    "In summary, all fine-tuning is transfer learning, but not all transfer learning involves fine-tuning. Fine-tuning is a specific strategy used to adapt pretrained models to new tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# State of Computer Vision\n",
    "\n",
    "**Data Availability in Computer Vision**:\n",
    "- **Data Amounts**: \n",
    "  - **Speech Recognition**: Large amount of data available.\n",
    "  - **Image Recognition**: Medium amount of data.\n",
    "  - **Object Detection**: Currently has a small amount of data.\n",
    "\n",
    "**Approaches Based on Data Amount**:\n",
    "- **When Large Data is Available**:\n",
    "  - Use **simpler algorithms**.\n",
    "  - Less focus on **hand-engineering**.\n",
    "\n",
    "- **When Limited Data is Available**:\n",
    "  - More emphasis on **hand-engineering** techniques (\"hacks\").\n",
    "  - Consider using **more complex neural network architectures**.\n",
    "\n",
    "**Important enginnering**:\n",
    "- Many computer vision problems lack sufficient data, leading to reliance on **hand-engineering** techniques to improve model performance.\n",
    "\n",
    "**Strategies for Success in Benchmarks and Competitions**:\n",
    "1. **Ensembling**:\n",
    "   - Train several networks independently and average their outputs.\n",
    "   - Merge multiple classifiers to improve performance.\n",
    "   - Initialize some weights randomly and train them independently to gain an additional performance boost (around 2%).\n",
    "   - Note: Ensembling can slow down production and increase memory usage, making it less common in real-world applications.\n",
    "\n",
    "2. **Multi-Crop at Test Time**:\n",
    "   - Run classifiers on multiple versions of the test images and average the results.\n",
    "   - Utilize techniques like **10 crops** to improve results during production.\n",
    "\n",
    "3. **Utilizing Open Source Resources**:\n",
    "   - Leverage architectures from published literature.\n",
    "   - Implement open source code whenever possible.\n",
    "   - Use pretrained models and fine-tune them on your specific dataset for better performance.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
